const data = 
[{"model": "mathematics.question", "pk": 2, "fields": {"code": "3.1.1", "category": 4, "problem": "Bunny rabbit has three dens A, B and C. It likes A better than B and\r\nC. If it's in B or C on any night, it will always take chance 0.9 to go to A\r\n and chance $0.1$ to go to the other den for the following night. Once it reaches\r\n A, it will stay there for two nights and the third night will be in B or C\r\n with equal chance 1/2. Let $X_n$ be the den Bunny stays for night $n$.\r\n What is the state space of the $\\{X_n\\}$?\r\n Is $\\{X_n\\}$ a $MC$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "The state space of $\\{X_n\\}$ is $\\{\\hbox{Den A, Den B, Den C}\\}$.\r\n$\\{X_n\\}$ is a $MC$.", "choicesb": "The state space of $\\{X_n\\}$ is $\\{\\hbox{Den A, Den B, Den C}\\}$.\r\n$\\{X_n\\}$ is not a $MC$.", "choicesc": "None of the above is correct.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The state space of $\\{X_n\\}$ is $\\{\\hbox{Den A, Den B, Den C}\\}$.\r\n$\\{X_n\\}$ is not a $MC$,\r\nbecause, for example,\r\n $$P(X_{n+1}=A|X_{n}=A, X_{n-1}=B)=1 \\not=0=P(X_{n+1}=A|X_n=A, X_{n-1}=A).$$\r\n$$\\qquad {\\textcolor[rgb]{1,0,0}{\\hbox{  (MC definition)}} }$$", "linkability1": 3.0, "linkability2": 0.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Alas! Please check the definition of Markov chain.", "messagesuccess": "Congratulations! You have mastered the definition of Markov chains.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [3], "rightproblems": [32], "wrongproblems": [31, 124], "twinproblems": [124]}}, {"model": "mathematics.question", "pk": 3, "fields": {"code": "3.1.3", "category": 4, "problem": "Suppose $X_n, n= 0, 1, 2, ...$ is\r\na discrete time $MC$. Let\r\n$0 \\leq n_0< n_1< n_2<   ...$ be a subsequence of the nonnegative integers\r\nand $Y_k = X_{n_k}$. Is $\\{Y_k: k =0,1, 2,...,\\}$\r\na $MC$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\{Y_k: k =0,1, 2,...,\\}$ is a $MC$.", "choicesb": "$\\{Y_k: k =0,1, 2,...,\\}$ is not a $MC$.", "choicesc": "None of the above is correct.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The definition of MC $\\{X_n\\}$, namely the past and future are\r\n independent given any fixed state of the present,   can be expressed as\r\n \\begin{eqnarray*}\r\n && P(X_{n+1}\\in A_1, X_{n+2} \\in A_2, ..., X_{n-1} \\in B_{n-1},   ..., X_0\r\n \\in B_0 | X_n = i)\r\n \\\\\r\n &=&  P(X_{n+1}\\in A_1, X_{n+2} \\in A_2, ...,|X_n=i)\r\n   \\times\r\n P(X_{n-1} \\in B_{n-1},  ..., X_0\r\n \\in B_0 | X_n = i)   \\qquad  (1)\r\n \\end{eqnarray*}\r\n$$\\qquad {\\textcolor[rgb]{1,0,0}{\\hbox{(MC definition and Conditional independence)}} }$$\r\n \r\n\r\n for any time $n$,\r\n any state $i$ and $any$ subsets of the state space, $A_1, A_2, ....,B_0, B_1, B_2, ...$; see the remark.\r\n Then,\r\n \\begin{eqnarray*}\r\n && P(Y_{k+1}\\in A_1, Y_{k+2} \\in A_2, ..., \\,\\,\\, Y_{k-1} \\in B_{k-1},  ..., Y_0\r\n \\in B_0 | Y_k=i  )\r\n \\\\\r\n &=&\r\n P(X_{n_{k+1}}\\in A_1, X_{n_{k+2}} \\in A_2, ...,\\,\\,\\, X_{n_{k-1}} \\in B_{k-1},   ..., X_{n_0}\r\n \\in B_0 | X_{n_k}=i  )\r\n \\\\\r\n && \\qquad \\hbox{(by definition of $Y_n$)}\r\n \\\\\r\n &=&\r\n P(X_{n_{k+1}}\\in A_1, X_{n_{k+2}} \\in A_2, ..., |X_{n_k}=i)\r\n  \\times P(X_{n_{k-1}} \\in B_{k-1},   ..., X_{n_0}\r\n \\in B_0 | X_{n_k}=i  )\r\n\\\\\r\n&& \\quad \\hbox{(by applying (1))}\r\n \\\\\r\n &=&\r\n P(Y_{k+1}\\in A_1, Y_{k+2} \\in A_2, ...,|Y_k=i)\r\n P(Y_{k-1} \\in B_{n-1}, Y_{k-2} \\in B_{n-2}, ..., Y_0\r\n \\in B_0 | Y_k = i).\r\n\\\\\r\n && \\quad \\hbox{(by definition of $Y_n$)}\r\n \\end{eqnarray*}\r\n \r\n<br>\r\n Remark. \r\n<br>\r\nThe mathematical expression that we often use as definition of $MC$ is\r\n $$P(X_{n+1}=j_1|X_n=i, X_{n-1}=i_{n-1}, ..., X_0=i_0)=P(X_{n+1}=j_1|X_n=i),\\qquad(2)$$\r\n for all time $n$, states $j_1,i, i_0,i_1,...,$\r\n which is a relatively simple version, but is actually equivalent to\r\n the seemingly more\r\n general version in (1). The following is to show the equivalency.\r\n Observe that (2) implies\r\n \\begin{eqnarray*}\r\n &&\r\n P(X_{n+k}=j_k, ..., X_{n+1}=j_1 | X_n=i,  X_{n-1}=i_{n-1}, ..., X_0=i_0)\r\n \\\\\r\n &=& P_{i, j_1} P_{j_1, j_2} \\cdots P_{j_{k-1}, j_k}\r\n \\\\\r\n &=&\r\n P(X_{n+k}=j_k, ..., X_{n+1}=j_1 | X_n=i),\r\n \\end{eqnarray*}\r\n which, by $DIY 3.1.2$, is equivalent to\r\n \\begin{eqnarray*}\r\n && P(X_{n+k}=j_k, ..., X_{n+1}=j_1,\r\n \\,\\,   X_{n-1}=i_{n-1}, ..., X_0=i_0 |X_n=i)\r\n \\\\\r\n &=&\r\n P(\\{X_{n+k}=j_k, ..., X_{n+1}=j_1\\} \\cap \\{   X_{n-1}=i_{n-1}, ..., X_0=i_0\\} |X_n=i)\r\n \\\\\r\n &=&\r\n P(X_{n+k}=j_k, ..., X_{n+1}=j_1 | X_n=i)\r\n P(X_{n-1}=i_{n-1}, ..., X_0=i_0 |X_n=i).\r\n \\end{eqnarray*}\r\n Then,\r\n \\begin{eqnarray*}\r\n &&\r\n P(X_{n+k}\\in A_k, ..., X_{n+1}\\in A_1,\r\n \\,\\,   X_{n-1}\\in B_{n-1}, ..., X_0\\in B_0 |X_n=i)\r\n \\\\\r\n &=&\r\n \\sum_{j_k \\in A_k} \\cdots \\sum_{j_1 \\in A_1} \\sum_{i_{n-1}\\in B_{n-1}} \\cdots\r\n \\sum_{i_0 \\in B_0}\r\n P(X_{n+k}=j_k, ..., X_{n+1}=j_1, \\,\\,\\,   X_{n-1}=i_{n-1}, ..., X_0=i_0|X_n=i)\r\n \\\\\r\n &=&\r\n \\sum_{j_k \\in A_k} \\cdots \\sum_{j_1 \\in A_1} \\sum_{i_{n-1}\\in B_{n-1}} \\cdots\r\n \\sum_{i_0 \\in B_0}\r\n \\\\\r\n && \\qquad\r\n P(X_{n+k}=j_k, ..., X_{n+1}=j_1 | X_n=i)\r\n P(X_{n-1}=i_{n-1}, ..., X_0=i_0 |X_n=i)\r\n \\\\\r\n &=&\r\n P(X_{n+k}\\in A_k, ..., X_{n+1}\\in A_1 |X_n=i)\r\n P(  X_{n-1}=B_{n-1}, ..., X_0=B_0 |X_n=i)\r\n .\r\n \\end{eqnarray*}\r\n This equality fits precisely the statement of the conditional independence of\r\n the future and the past given a fixed state of the present.", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Alas! Please check the definition of Markov chain and conditional independence.", "messagesuccess": "Congratulations! You have mastered the definition of Markov chains\r\nand conditional independence.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [3, 7], "rightproblems": [49], "wrongproblems": [48], "twinproblems": [49]}}, {"model": "mathematics.question", "pk": 7, "fields": {"code": "3.4.1", "category": 4, "problem": "For coin tossing, is there a similar phenomenon for pattern of length 2 to\r\nthat for pattern of length 3 shown Example 3.7?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes.", "choicesb": "No.", "choicesc": "I am not sure.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No. Only four patterns of length 2: HH, HT, TH and TT.\r\nTH occurs before HH with chance 3/4, before HT and TT with 1/2 each.\r\n(Also by symmetry),  HT occurs before TT with chance 3/4, before\r\nTH and HH with 1/2 each. There is no loop of three or four patterns\r\nin which one occurs before another with larger\r\nthan 1/2 chance, unlike patterns of length 3.", "linkability1": 3.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Or you may review Example 3.7.", "messagesuccess": "Great! You know the application of first step analysis.", "sensitivity": 1.0, "gussingparameter": 0.333, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [2, 3, 5, 9, 12], "rightproblems": [50], "wrongproblems": [36], "twinproblems": [50]}}, {"model": "mathematics.question", "pk": 8, "fields": {"code": "3.4.3", "category": 4, "problem": "Based on the fact that the mean number of tosses till\r\n first HH is 6, can you calculate the mean number of tosses till the first\r\n HHH occur (by using one equation)?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "14.", "choicesb": "12.", "choicesc": "10.", "choicesd": "9.", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "(This is a little tricky but still based only on the idea of first step analysis.)\r\nLet $w$ be the mean number till first  HHH  occur. Suppose  HH  occurs the first time\r\n at $n$-th toss. With 1/2 chance the $n+1$-th toss is H and it takes one addtional\r\n toss to have the first\r\n time HHH at $n+1$-th toss (remember, with 1/2 chance). The other 1/2 chance\r\n is for $n+1$-th toss being T. In this case, counting from $n+2$-th toss, it will take averagely\r\n w additional tosses to reach HHH. Hence,\r\n $$ w= 6 + 1/2 \\times 1 + 1/2 \\times (1+ w).$$\r\n Solve the equation, we have $w=14$.", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Here is a hint: apply first step analysis.", "messagesuccess": "Perfect! You have mastered the concept of first step analysis!", "sensitivity": 1.0, "gussingparameter": 0.2, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [3, 9, 13], "rightproblems": [51, 128], "wrongproblems": [4], "twinproblems": [4]}}, {"model": "mathematics.question", "pk": 11, "fields": {"code": "3.1.5", "category": 4, "problem": "$\\{X_n, n= 1, 2, ...\\}$ is\r\n a Markov chain with state space $ \\{-1,0,1\\} $.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\{sin(X_n):n=1,2,\\ldots\\} $ is a Markov chain.", "choicesb": "$\\{cos(X_n):n=1,2,\\ldots\\} $ is a Markov chain.", "choicesc": "$\\{|X_n|:n=1,2,\\ldots\\} $ is a Markov chain.", "choicesd": "$\\{X_n^2:n=1,2,\\ldots\\} $ is a Markov chain.", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The definition of MC $\\{X_n\\}$, namely the past and future are\r\n independent given any fixed state of the present,   can be expressed as\r\n\r\n \t $$P(X_{n+1}=j|X_n=i, X_{n-1}=i_{n-1}, ..., X_0=i_0)=P(X_{n+1}=j_1|X_n=i),$$\r\n \r\n for all time $n$, states $j_1,i, i_0,i_1,...$\r\n \r\n With the defintion of Markov chain, we check the (a) to (d) one by one:\r\n \r\n (a). It is easy to see under the state space ${-1,0,1} $, $ sin(X_n) $ is an one-to-one map on the state space, so \r\n \\begin{eqnarray*}\r\n \t&& P(sin(X_{n+1})=j|sin(X_n)=i,sin(X_{n-1})=i_{n-1}, ..., sin(X_0)=i_0)\r\n \t\\\\\r\n \t&=&\r\n \tP(X_{n+1}=arcsin(j)|X_n=arcsin(i), X_{n-1}=arcsin(i_{n-1}), ..., X_0=arcsin(i_0)\r\n \t\\\\\r\n \t&=&\r\n \t\tP(X_{n+1}=arcsin(j)|X_n=arcsin(i))\r\n \t\\\\\r\n \t&& \\qquad \\hbox{(by applying (1))}\r\n \t\\\\\r\n \t&=&P(sin(X_{n+1})=j|sin(X_n)=i)\r\n\\end{eqnarray*}  \r\n\r\n(b). Under the state space $ \\{-1,0,1\\} $, $ cos(X_n) $ is not an one-to-one map on the state space.\r\n<br>\r\n(c). Under the state space $\\{-1,0,1\\} $, $ |X_n| $ is not an one-to-one map on the state space.\r\n<br>\r\n(d). Under the state space $\\{-1,0,1\\} $, $ X_n^2 $ is not an one-to-one map on the state space.\r\nAnd the map in the (b),(c),(d) are somehow equivalent under the state space ${-1,0,1} $. So they should be both right or both wrong. So they are not correct answer.\r\n<br>\r\nRemark: The question is a type of question: given the $\\{X_n\\}$ is a Markov chain, and ask whether the transformation of $\\{X_n\\}$, such as $\\{cos(X_n)\\}$ is still a Markov chain. What we should do is to check whether the transformation is a one-to-one map.", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "e", "alternativesolutions": "None", "messagefailure": "Oops! Please think about the definition of markov chain and conditional independence.", "messagesuccess": "Congratulations! You have mastered the definition of markov chains\r\nand known how to use it.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [3, 7], "rightproblems": [], "wrongproblems": [12, 13, 14], "twinproblems": [12, 14]}}, {"model": "mathematics.question", "pk": 12, "fields": {"code": "3.1.6", "category": 4, "problem": "$\\{X_n, n= 1, 2, ...\\}$ is\r\n a Markov chain with state space $ \\{1,2,\\ldots,10\\} $.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$ \\{X_n+X_{n-1}: n=1,2,\\ldots\\} $ is a Markov chain", "choicesb": "$ \\{X_n-X_{n-1}: n=1,2,\\ldots\\} $ is a Markov chain", "choicesc": "$ \\{X_nX_{n-1}: n=1,2,\\ldots\\} $ is a Markov chain", "choicesd": "$ \\{X_n/X_{n-1}: n=1,2,\\ldots\\} $ is a Markov chain", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "E", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "In the answer of  DIY 3.1.5, the transformation in the state space should be one-to-one.\r\n \r\n With the answer in  DIY 3.1.5, we check the (a) to (d).\r\n In (a), the transformation is $ f(x,y) = x+y $ which is not one-to-one. So (a) is not a correct answer. The same is as transformations $ f(x,y) = x-y $, $ f(x,y) = xy $, $ f(x,y) = x/y $ which are in the (b), (c) and (d) . So the right answer is (e).", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "A", "alternativesolutions": "", "messagefailure": "Oops! Please think about the definition of markov chain and conditional independence.", "messagesuccess": "Congratulations! You have mastered the definition of markov chains\r\nand known how to use it.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [3, 7], "rightproblems": [14], "wrongproblems": [11], "twinproblems": [11]}}, {"model": "mathematics.question", "pk": 13, "fields": {"code": "3.1.7", "category": 4, "problem": "Suppose we have three random variables $ X_0 $, $ X_1 $ and $ X_2 $ forming a Markov process with time domain $ \\{0,1,2\\} $. Then which following is right?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$ X_1, X_2,X_0 $ also form a Markov process.", "choicesb": "$ X_2, X_0,X_1 $ also form a Markov process.", "choicesc": "$ X_0, X_2,X_1 $ also form a Markov process.", "choicesd": "$ X_2, X_1,X_0 $ also form a Markov process.", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The definition of MC $\\{X_n\\}$, namely the past and future are\r\n independent given any fixed state of the present,   can be expressed as\r\n \r\n $$P(X_{n+1}=j|X_n=i, X_{n-1}=i_{n-1}, ..., X_0=i_0)=P(X_{n+1}=j_1|X_n=i),$$\r\n \r\n for all time $n$, states $j_1,i, i_0,i_1,...$\r\n \r\n With the defintion of Markov chain, we have \r\n \\begin{eqnarray*}\r\n P(X_2 = j|X_1 = i_1, X_0 = i_0)&=& P(X_2=j|X_1 = i_1)\r\n \\\\\r\nthen ~~\\frac{P(X_2 = j,X_1 = i_1, X_0 = i_0)}{P(X_1 = i_1, X_0 = i_0)} &=& \\frac{P(X_2=j, X_1 = i_1)}{P(X_1 = i_1)}\r\n \\\\\r\n then~~\\frac{P(X_2 = j,X_1 = i_1, X_0 = i_0)}{P(X_2=j, X_1 = i_1)}&=&\\frac{P(X_1 = i_1, X_0 = i_0)}{P(X_1 = i_1)} \r\n \\\\\r\n namely~~P(X_0 = i_0|X_2=j, X_1 = i_1)&=&P(X_0=i_0|X_1 = i_1)\r\n  \\end{eqnarray*}\r\n which means $ X_2, X_1,X_0 $ is a Markov chain.", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "A", "alternativesolutions": "", "messagefailure": "Oops! Please think about the definition of markov chain and conditional independence.", "messagesuccess": "Congratulations! You have mastered the definition of markov chains\r\nand known how to use it.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [3, 7], "rightproblems": [], "wrongproblems": [11], "twinproblems": []}}, {"model": "mathematics.question", "pk": 14, "fields": {"code": "3.1.8", "category": 4, "problem": "Mickey mouse and travels blindly independently in the maze in Example 3.2. And Donald duck follows Mickey's footstep.Let $ M_n $ be cell number of Mickey after n step transition and $ D_n $ be the cell number of Donald after n step transition. Then $ D_n = M_{n-1} $ for $ n\\geq1 $, with $ M_0 = D_1 = 4 $. Counting $ n = 1, 2, \\ldots $\r\n<br>\r\nThe maze is below:", "problempicture1": "theall/image/WX20170316-2301342x.png", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$ D_n^2 $ is a Markov chain.", "choicesb": "$ 2M_n +D_n $ is a Markov chain.", "choicesc": "$ M_nD_n $ is a Markov chain.", "choicesd": "$ M_n - D_n $ is a Markov chain.", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "In the remark of  DIY 3.1.5, the transformation in the state space should be one-to-one. The transformation as follow where we regard $ D_n = M_{n-1} $:\r\n <br>\r\n (a). $ f(x) = x^2 $\r\n <br>\r\n (b). $ f(x,y) = 2x + y $\r\n <br>\r\n (c). $ f(x,y) = xy $\r\n<br>\r\n (d). $ f(x,y) = x - y $\r\n <br>\r\n With the state space $ 0, 1, \\ldots, 8 $, $ f(x) $ in (a) is one-to-one. So the right answer is (a).", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "E", "alternativesolutions": "", "messagefailure": "Oops! Please think about the definition of markov chain and conditional independence.", "messagesuccess": "Congratulations! You have mastered the knowledge in the 3.1.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [3, 7], "rightproblems": [12], "wrongproblems": [11], "twinproblems": [11]}}, {"model": "mathematics.question", "pk": 15, "fields": {"code": "3.2.1", "category": 4, "problem": "The transition probability matrix is \r\n  \r\n  $$ \\qquad  \\,\\, \\,\\,\\, 0\\qquad 1\\qquad 2  $$\r\n  $${ P} =\\matrix{ 0 \\cr 1 \\cr 2    }\r\n  \\pmatrix{\r\n  \t0 &  2/3  &  1/3              \\cr\r\n  \t1/2\t&   0  &  1/2      \\cr\r\n  \t1/2\t &   1/2  &   0          \\cr\r\n  }\r\n  $$\r\n Then, $ P_{00}^{(3)} $ is", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1/2", "choicesb": "1/3", "choicesc": "1/4", "choicesd": "1/5", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "$ P_{00}^{(3)} $ means with initial point 0, the probability that it still at point 0 after 3 steps.\r\nWhen we are trying to know such kind probability, The easiest way is to compute $ P^{(3)} $. $ P^{(3)} = P\\times P \\times P$ .\r\n$$ \\qquad  \\,\\, \\,\\,\\, 0\\qquad 1\\qquad 2  $$\r\n$${ P^2} =\\matrix{ 0 \\cr 1 \\cr 2    }\r\n\\pmatrix{\r\n\t1/2 &  1/6  &  1/3              \\cr\r\n\t1/4\t&   7/12  &  1/6      \\cr\r\n\t1/4\t &   1/3  &   5/12          \\cr\r\n}\r\n$$\r\n$$ \\qquad  \\,\\, \\,\\,\\, 0\\quad 1 \\,\\quad 2  $$\r\n$${ P^3} =\\matrix{ 0 \\cr 1 \\cr 2    }\r\n\\pmatrix{\r\n\t1/4 &  *  &  *              \\cr\r\n\t*\t&   *  &  *      \\cr\r\n\t*\t &   *  &   *       \\cr\r\n}\r\n$$\r\nWe only need the first component of $ P^3 $ which is $ P_{00}^{(3)} $ . So the right answer is c.", "linkability1": 1.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Please check how to calculate transition matrix.", "messagesuccess": "Congratulations! You have known how to calculate the transition matrix.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [5], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 16, "fields": {"code": "3.4.6", "category": 4, "problem": "The transition probability matrix is \r\n   $$ \\qquad  \\,\\, \\,\\,\\, 0\\qquad 1\\qquad 2  $$\r\n  $${ P} =\\matrix{ 0 \\cr 1 \\cr 2    }\r\n  \\pmatrix{\r\n  \t0 &  2/3  &  1/3              \\cr\r\n  \t1/2\t&   0  &  1/2      \\cr\r\n  \t1/2\t &   1/2  &   0          \\cr\r\n  }\r\n  $$\r\n \r\n Then, starting from state 0, the mean number of visits of state 2 before coming back to state 0 is", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "5/9", "choicesb": "6/9", "choicesc": "7/9", "choicesd": "8/9", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The key to solve this question is use first step analysis. We use $ w_i $ to stand for starting from state i, the mean number of visits of state 2 before coming back to state 0. So we have\r\n \\begin{eqnarray*}\r\n w_0 &=& 2/3w_1 + 1/3w_2\\\\\r\n w_1 &=& 1/2 w_2\\\\\r\n w_2 &=& 1 + 1/2w_1\r\n \\end{eqnarray*}\r\nSo$ w_0 = 8/9, w_1 = 2/3, w_2 = 4/3$. So the answer is d.\r\n<br>\r\n First step analysis is a very useful way to sovle this question. If we do not use this analysis, the question will be very difficult.", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Try to use first step analysis to deal with the question.", "messagesuccess": "Congratulations! You have a right answer on this question about first step analysis.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [], "wrongproblems": [17, 18], "twinproblems": [17, 18, 19, 20]}}, {"model": "mathematics.question", "pk": 17, "fields": {"code": "3.4.7", "category": 4, "problem": "A Markov chain has one-step transition probability  matrix\r\n  \r\n  $$ \\qquad  \\,\\, ~~0 \\qquad 1 \\qquad 2\\qquad 3  $$\r\n $${\\bf P} =\\matrix{0 \\cr 1 \\cr 2 \\cr 3    }\r\n \\pmatrix{\r\n \t0 &  1/3  &  1/3   &   1/3           \\cr\r\n \t1/3\t&   1/3  &  1/3  &    0          \\cr\r\n \t1/2\t &   1/2  &   0  &   0          \\cr\r\n \t0 \t&   1/2   &  1/2   &   0\r\n }\r\n $$\r\n Then, starting from state 0, the chance of visits of state 3 before visiting state 2 is", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0", "choicesb": "1/5", "choicesc": "2/5", "choicesd": "3/5", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The key to solve this question is use first step analysis. We use $ w_i $ to stand for starting from state i, the chance of visiting state 3 before visitng state 2. So we have\r\n \\begin{eqnarray*}\r\n w_0 &=& 1/3w_1 + 1/3w_2 + 1/3w_3\\\\\r\n w_1 &=& 1/3 w_0 + 1/3 w_1 +1/3 w_2\\\\\r\n w_2 &=& 0\\\\\r\n w_3 &=& 1\r\n \\end{eqnarray*}\r\nSo $ w_0 = 2/5, w_1 = 1/5, w_2 = 0, w_3 = 1$. So the answer is c.", "linkability1": 3.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Try to use first step analysis to deal with the question.", "messagesuccess": "Congratulations! You have a right answer on this question about first step analysis.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [], "wrongproblems": [16], "twinproblems": [16]}}, {"model": "mathematics.question", "pk": 18, "fields": {"code": "3.4.8", "category": 4, "problem": "A Markov chain has one-step transition probability  matrix\r\n  \r\n $$ \\qquad  \\,\\, 0 \\qquad 1 \\qquad 2\\qquad 3  $$\r\n$${\\bf P} =\\matrix{0 \\cr 1 \\cr 2 \\cr 3    }\r\n\\pmatrix{\r\n\t0 &  1/3  &  1/3   &   1/3           \\cr\r\n\t1/3\t&   1/3  &  1/3  &    0          \\cr\r\n\t1/2\t &   1/2  &   0  &   0          \\cr\r\n\t0 \t&   1/2   &  1/2   &   0\r\n}\r\n$$\r\n Then, starting from state 0, the mean number of visits of state 2 from state 1 before visiting state 3, is", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1/4", "choicesb": "1/3", "choicesc": "1/2", "choicesd": "1", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The key to solve this question is use first step analysis. We use $ w_i $ to stand for starting from state i,the mean number of visits of state 2 from state 1 before visiting state 3. So we have\r\n \\begin{eqnarray*}\r\n w_0 &=& 1/3w_1 + 1/3w_2 + 1/3w_3\\\\\r\n w_1 &=& 1/3 w_0 + 1/3 w_1 +1/3 (w_2+1)\\\\\r\n w_2 &=& 1/2w_0 + 1/2w_1\\\\\r\n w_3 &=& 0\r\n \\end{eqnarray*}\r\nSo $ w_0 = 1, w_1 = 4/3, w_2 = 5/3, w_3 = 0$. So the answer is d.", "linkability1": 3.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Try to use first step analysis to deal with the question.", "messagesuccess": "Congratulations! You have a right answer on this question about first step analysis.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [], "wrongproblems": [16, 19], "twinproblems": [16]}}, {"model": "mathematics.question", "pk": 19, "fields": {"code": "3.4.9", "category": 4, "problem": "A Markov chain has one-step transition probability  matrix\r\n  $$ \\qquad  \\,\\,  0\\qquad 1\\qquad 2\\qquad 3  $$\r\n  $${\\bf P} =\\matrix{ 0 \\cr 1 \\cr 2 \\cr 3    }\r\n  \\pmatrix{\r\n  \t1/4 &  1/4  &  1/4  &   1/4            \\cr\r\n  1/4 & 1/4  &  1/2  &    0     \\cr\r\n  0\t &   0  &   1/2  &   1/2         \\cr\r\n 0 \t&   0   &  1/3   &   2/3    \r\n   }\r\n  $$\r\n  \r\n Starting from state 0, the mean number of visits of state 1 is", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1/2", "choicesb": "1/3", "choicesc": "1/4", "choicesd": "1/5", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "E", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The key to solve this question is use first step analysis. We use $ w_i $ to stand for starting from state i,the mean number of visits of state 1. So we have\r\n \\begin{eqnarray*}\r\n w_0 &=& 1/4w_0 + 1/4w_1 + 1/4w_2 + 1/4w_3\\\\\r\n w_1 &=& 1 + 1/4 w_0 + 1/4 w_1 +1/2 w_2\\\\\r\n w_2 &=& 1/2w_2 + 1/2w_3\\\\\r\n w_3 &=& 1/3w_2 + 2/3w_3\r\n \\end{eqnarray*}\r\nFirst we know $ w_2 = w_3 $. Then we can see that $ w_2 = w_3 = 0 $, since from stating 2 or 3, we can never back to state 1.\r\nSo $ w_0 = 1/8, w_1 = 3/8, w_2 = 0, w_3 = 0$. So the answer is e.", "linkability1": 3.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Try to use first step analysis to deal with the question.", "messagesuccess": "Congratulations! You have a right answer on this question about first step analysis.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [19], "wrongproblems": [18], "twinproblems": [16]}}, {"model": "mathematics.question", "pk": 20, "fields": {"code": "3.4.10", "category": 4, "problem": "Toss a fair coin a number of times. You always bet on the head to make one dollor if it is a head and to lose one dollar if it is a tail. You are stopped whenever you make 3 dollar or you lose 2 dollar. Then,", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "you end up winning with 2/3 probability", "choicesb": "you end up winning with 1/3 probability", "choicesc": "you end up winning with 3/4 probability", "choicesd": "you end up winning with 3/5 probability", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "E", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "First we need to find the markov process in the question. We have 6 states. In order not to make you confused, we use state $ i $ to stand for the number of your money you win. Negative means you lose.\r\n<br>\r\n\t\tSo the one-step transition probability  matrix in this question is:\r\n\t\t   $$ \\qquad \\,\\,\\quad-2 ~\\,\\quad -1 \\,~\\quad 0\\,\\,~\\quad 1 \\,\\,~\\quad 2\\,\\,~\\quad3  $$\r\n\t\t$${\\bf P} =\\matrix{-2 \\cr -1 \\cr 0 \\cr 1 \\cr 2 \\cr 3    }\r\n\t\t\\pmatrix{\r\n\t\t\t1 &  0  &  0  &   0 & 0 &0           \\cr\r\n\t\t\t1/2\t&   0  &  1/2  &    0  & 0 & 0    \\cr\r\n\t\t\t0\t &   1/2  &   0  &   1/2&0&  0        \\cr\r\n\t\t\t0 \t&   0   &  1/2   &   0&1/2&0       \\cr\r\n\t\t\t0 & 0 & 0 & 1/2 &0 &1/2             \\cr\r\n\t\t\t0 &0&0&0&0&1\r\n\t\t}\r\n\t\t$$\r\n\t\tThe key to solve this question is use first step analysis. We use $ w_i $ to stand for starting from state i,the chance we make 3 dollar before we lose 2 dollar. So we have\r\n\t\t\\begin{eqnarray*}\r\n\t\t\tw_{-2} &=& 0\\\\\r\n\t\t\tw_{-1} &=& 1/2w_{-2} + 1/2w_0 \\\\\r\n\t\t\tw_0 &=& 1/2w_{-1} + 1/2w_1\\\\\r\n\t\t\tw_1 &=& 1/2w_0 + 1/2w_2\\\\\r\n\t\t\tw_2 &=& 1/2w_1 + 1/2w_3\\\\\r\n\t\t\tw_3 &=& 1\r\n\t\t\\end{eqnarray*}\r\n\t\r\n\t\tSo $ w_{-2} =0, w_{-1} = 1/5 , w_0 = 2/5, w_1 = 3/5, w_2 = 4/5, w_3 = 1$. So the answer is e.", "linkability1": 3.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Try to use first step analysis to deal with the question.", "messagesuccess": "Congratulations! You have a right answer on this question about first step analysis.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [6, 9], "rightproblems": [], "wrongproblems": [], "twinproblems": [16]}}, {"model": "mathematics.question", "pk": 48, "fields": {"code": "3.1.2", "category": 4, "problem": "For three events A, B and C,  we have the following three statements\r\n: (i) $P(A\\cap B|C)= P(A|C)P(B|C)$; (ii). $P(A|C \\cap B) = P(A|C)$;\r\nand (iii) $P(B|A\\cap C) = P(B|C)$. (Assume all quantities here are well defined.) Which of them are equivalent?\r\nNotice that statement (i) says that A and B are conditionally independent given C.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "(i) and (ii)", "choicesb": "(ii) and (iii)", "choicesc": "(i) and (iii)", "choicesd": "(i) and (ii) and (iii)", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "(i) $\\Longrightarrow$ (ii). Write\r\n$$P(A|C\\cap B)={ P(A \\cap C \\cap B) \\over P(C \\cap B)}=\r\n{ P(A \\cap B|C)P(C)  \\over P(B|C)P(C)  }= {P(A|C)P(B|C) \\over P(B|C)}= P(A|C).$$\r\n (ii) $\\Longrightarrow$ (iii). Write\r\n$$P(B|A\\cap C)={ P(A \\cap C \\cap B) \\over P(A \\cap C)}=\r\n{ P(A | B \\cap C)P(B \\cap C)  \\over P(A|C)P(C)  }= {P(A|C)P(B|C)P(C) \\over P(A|C)P(C)}= P(B|C).$$\r\n(iii) $\\Longrightarrow$ (i). Write\r\n$$P(A\\cap B | C)={ P(A \\cap B \\cap C) \\over P( C)}=\r\n{ P(B| A\\cap C)P(A\\cap C)  \\over  P(C)  }= {P(B|C)P(A|C) P(C) \\over P( C)}= P(A|C)P(B|C).$$\r\n\r\n\r\nRemark. In (i), $A$ and $B$ are exchangeable. Therefore, if (i) is equivalent to (ii), it must\r\nbe equivalent to (iii).", "linkability1": 3.0, "linkability2": 0.0, "linkability3": 2.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Alas! Please check the definition of conditional independence.", "messagesuccess": "Congratulations! You have mastered the definition of conditional independence.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [7], "rightproblems": [], "wrongproblems": [3], "twinproblems": []}}, {"model": "mathematics.question", "pk": 49, "fields": {"code": "3.1.4", "category": 4, "problem": "Suppose $\\{X_n: n= \\cdots -2, -1, 0, 1, 2, \\cdots\\}$\r\nis a discrete time $MC$ with the time being  all integers from\r\n$-\\infty$ to $\\infty$. Let $Y_n = X_{-n}$ for all integers $n$.\r\nIs $\\{Y_n: n= \\cdots -2, -1, 0, 1, 2, \\cdots\\}$ a $MC$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes", "choicesb": "No", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Yes. Write\r\n\\begin{eqnarray*}\r\n && P(\\{Y_{n+1}\\in A_1, Y_{n+2}\\in A_2, ... \\} \\cap \\{ Y_{n-1} \\in B_1 , Y_{n-2} \\in B_2 ,...\\}\r\n|Y_n=j)\r\n\\\\\r\n&=& P(\\{X_{-n-1}\\in A_1, X_{-n-2}\\in A_2, ... \\} \\cap \\{ X_{-n+1} \\in B_1 , X_{-n+2} \\in B_2 ,...\\}\r\n|X_{-n}=j)\r\n\\\\\r\n&=& P(\\{X_{-n-1}\\in A_1, X_{-n-2}\\in A_2, ... \\} |  X_{-n}=j)\r\nP(\\{ X_{-n+1} \\in B_1 , X_{-n+2} \\in B_2 ,...\\}\r\n|X_{-n}=j)\r\n \\\\\r\n&=& P(\\{Y_{ n+1}\\in A_1, Y_{ n+2}\\in A_2, ... \\} |  Y_{n}=j)\r\nP(\\{ Y_{n-1} \\in B_1 , Y_{n-2} \\in B_2 ,...\\}\r\n|Y_{n}=j)\r\n\\end{eqnarray*}\r\nProving the conditional independence of the past and future given a fixed state of the present.", "linkability1": 3.0, "linkability2": 0.0, "linkability3": 2.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Alas! Please check the definition of markov process.", "messagesuccess": "Congratulations! You have mastered the definition of markov process.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [2, 3, 4], "rightproblems": [3], "wrongproblems": [], "twinproblems": [3]}}, {"model": "mathematics.question", "pk": 50, "fields": {"code": "3.4.2", "category": 4, "problem": "In Example 3.7,   a\r\n$MC$ based on patterns of length 2 with 4 states is constructed\r\nto solve the problem. If Mr Y picks\r\nthe pattern HHH, what is the chance of Mr Z to win\r\n by picking THH? You may construct a $MC$ based on patterns of\r\nlength 3 (rather than 2) with 8 states to\r\ncalculate.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "5/8", "choicesb": "6/8", "choicesc": "7/8", "choicesd": "1", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $\\{X_n: n\\geq 3\\}$ be the MC with state space being\r\n the eight patterns of length 3:\r\n $$\\hbox{\\{ HHH (0), THH (1),  HTH (2), TTH (3),\r\n HHT (4) , THT (5), HTT (6), TTT (7) \\}.}$$\r\n and transition probability matrix:\r\n \\begin{eqnarray*}\r\n&& \\qquad  \\qquad \\qquad  0\\quad \\, 1 \\quad \\, 2 \\quad\\,\r\n 3\\quad \\,\\,  4 \\quad \\,\\, 5 \\quad \\, 6 \\quad \\,\\, 7 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{HHH (0)}  \\cr\r\n\\hbox{THH (1)} \\cr  \\hbox{HTH (2)} \\cr \\hbox{TTH (3)}\\cr\r\n\\hbox{HHT (4)} \\cr  \\hbox{THT (5)} \\cr\r\n\\hbox{HTT (6)}\\cr \\hbox{TTT (7)} }\r\n\\pmatrix{\r\n.5   & 0   & 0 & 0 & .5 & 0 & 0 & 0   \\cr\r\n.5 & 0 & 0 & 0 & .5 & 0 & 0 &0  \\cr\r\n0 & .5 & 0 & 0 & 0 & .5 & 0 & 0   \\cr\r\n0 & .5 & 0 & 0 & 0 & .5 & 0 & 0  \\cr\r\n0 & 0 & .5&  0 & 0 & 0 & .5 & 0   \\cr\r\n0 & 0 & .5&  0 & 0 & 0 & .5 & 0   \\cr\r\n0 & 0 & 0 & .5 & 0 & 0 &0 & .5   \\cr\r\n0 & 0 & 0 & .5 & 0 & 0 &0 & .5\r\n}\r\n\\end{eqnarray*}\r\n\r\nLet $p_i$ be the chance that THH occur before HHH, beginning with state\r\n$i$, ($X_3=i$).\r\nThen,\r\n$$p_0=0, \\quad p_1=1, \\quad p_i =  \\sum_{j=0}^7 P_{ik}p_k, \\quad \\hbox{for $i\\not=0, 1$}.$$\r\nSolving the equations, we have\r\n$$p_2= p_3= p_4= p_5 =  p_6=  p_7=1.$$\r\nSince $P(X_3=i)=1/8$ for $i=0,...,7$, the chance that Mr Z wins is\r\n$$1/8\\times (p_0+p_1 + \\cdots+p_7)= 7/8.$$\r\n\r\n\r\nRemark. Directly solving the above 6 equations appears to be too much of\r\ncomputation. But a quick look shall find that $p_2=p_3$, $p_4=p_5$\r\nand $p_6=p_7$. Thus the number of equations is actually only three.\r\n(More astute observation further leads to $p_4=p_5=p_6=p_7$, which reduces the\r\nnumber of equations to two.)", "linkability1": 3.0, "linkability2": 2.0, "linkability3": 3.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Reread Example 3.7 might be helpful!", "messagesuccess": "Congratulations! You have solved a difficult problem of first step analysis!", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [7, 130], "wrongproblems": [], "twinproblems": [7]}}, {"model": "mathematics.question", "pk": 51, "fields": {"code": "3.4.4", "category": 4, "problem": "A rabbit has three dens A, B and C.  If it stays in A for a night,\r\nthe following night it's always in B. If it stays in B or C for a night,\r\nthe following day,  it takes equal chance 1/3 to\r\neither continue to stay or go to one of the other two dens for the night.\r\nA wolf, trying to hunt down the rabbit, has a different pattern.\r\nEvery day, it takes chance .8 to go clockwise to the next den\r\nand  chance .2 to go anti-clockwise the other den for the night-stay.\r\nThe rabbit would be eaten by the wolf the night they are in the same den.\r\nSuppose at the night of day 0, the rabbit is in Den A and the wolf\r\nin Den B.   What is the mean life time in days of the rabbit?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "3.78", "choicesb": "2.54", "choicesc": "3.96", "choicesd": "4.21", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $X_n $ represent the den the rabbit stays for $n$-th night\r\nfollowed by the den the wolf stays for the $n$-th night, assuming the wolf and rabbit\r\nalways stay in the den forever once they are in the same den.\r\nThen, $\\{X_n: n\\geq 0\\}$ is a {\\it MC} with nine states\r\n$$ \\{ \\hbox{ AA (0) , AB (1), AC (2), BA (3), BB (4), BC (5), CA (6), CB (7), CC (8)}, \\}$$\r\nand\r\nwith transition probability matrix\r\n\\begin{eqnarray*}\r\n&& \\qquad\\quad AA \\quad \\,\\, AB\\quad \\,\\, AC\\quad \\,\\, BA\\quad \\,\\, BB\\quad \\,\\, BC\\quad \\,\r\n CA\\quad \\, CB\\quad \\,\\, CC \\\\\r\n{\\bf P} &=& \\matrix{AA \\cr AB \\cr AC \\cr BA \\cr BB \\cr BC\\cr CA \\cr CB \\cr CC}\r\n\\pmatrix{\r\n1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\cr\r\n0 & 0 & 0 & .8 & 0& .2 & 0 &0 & 0 \\cr\r\n0 & 0 & 0 & .2 & .8 & 0& 0 & 0 & 0 \\cr\r\n0 & .067 & .266 & 0 & .067 & .266 &0 & .067 & .266 \\cr\r\n0 & 0 & 0&  0 & 1 & 0 & 0 & 0 &0 \\cr\r\n.067&.266 & 0 & .067&.266 & 0  & .067&.266 & 0 \\cr\r\n0 & .067 & .266 & 0 & .067 & .266 &0 & .067 & .266 \\cr\r\n.266&0 &.067 & .266&0 & .067   & .266&0 & .067  \\cr\r\n0 & 0 & 0&  0 & 0 & 0 & 0 & 0 &1\r\n}\r\n\\end{eqnarray*}\r\nLet $w_i $ be the mean number of steps to reach AA, BB or CC, starting from\r\nstate $i$.\r\nThen\r\n$$w_0=w_4=w_8=0, \\qquad w_i = 1 + \\sum_{k=0}^8 P_{ik}w_i \\quad \\hbox{for $i\\not=0, 4, 8.$}$$\r\nSolve the equations,\r\n$$w_1= 3.78, \\,\\,\r\nw_2= 1.53, \\,\\,\r\nw_3= 2.69, \\,\\,\r\nw_5= 3.14, \\,\\,\r\nw_6= 2.69, \\,\\,\r\nw_7= 2.89 $$\r\nThe mean life time of rabbit is $w_1=3.78$ days ($X_0=1=$AB).", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "You may have a look at example 3.7?", "messagesuccess": "Congratulations! You mastered first step analysis!", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [9, 13], "rightproblems": [8, 129], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 52, "fields": {"code": "3.9.1", "category": 4, "problem": "Is it possible that $E(X_n) \\uparrow \\infty$ geometrically\r\nfast, and the chance of extinction is still positive?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes, it is possible.", "choicesb": "No, it is not possible.", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "$E(X_n) = \\mu^n$. Choose $\\xi $ such that\r\n$P(\\xi=2)=3/4$ and $P(\\xi=0)=1/4$, so that $\\mu = 3/2 > 1$ and\r\n$E(X_n)   \\uparrow \\infty$ geometrically. But, on the other hand,\r\n$\\phi(s) = 1/4 + 3/4s^2$ and $s=\\phi(s)$ has a solution $1/3$.\r\nTherefore $\\mu_\\infty = 1/3$ which is positive.", "linkability1": 3.0, "linkability2": 0.0, "linkability3": 2.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! You may check the definition of extinction probability.", "messagesuccess": "Congratulations! You mastered it!", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [16, 21], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 53, "fields": {"code": "3.9.3", "category": 4, "problem": "Suppose $P(\\xi\\geq 4)=0$, and  $P(\\xi=i)=p_i$ for\r\n$i=0,..., 3$. If we express $u_\\infty$ in terms of $p_0,...,p_3$, how many situations are there in total?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1", "choicesb": "3", "choicesc": "4", "choicesd": "5", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Consider equation $s=\\phi(s)$, which can be written as\r\n$$ p_3s^3+p_2s^2 + (p_1-1)s +p_0=0.$$\r\nAs $1$ is always a solution. The equation is\r\n$$   (s-1)(p_3 s^2 +(p_2+p_3)s  -p_0)=0$$\r\nSolving the equations, we have\r\n$$u_\\infty = \\cases { 0 &  if $p_0=0$;\r\n\\cr\r\n1 & if $p_0>0, p_2=p_3=0$;\r\n\\cr\r\n \\min(1, p_0/p_2) & if $p_0>0, p_2>0, p_3=0$;\r\n \\cr\r\n(1/2p_3) \\{ -p_2-p_3 + \\sqrt{ (p_2+p_3)^2 + 4p_0p_3} \\}&\r\nif $p_0>0, p_3>0 $ and $p_0 \\leq 2p_3+p_2$ \\cr\r\n1 & if $p_0>0, p_3>0 $ and $p_0 > 2p_3+p_2$.}\r\n$$", "linkability1": 3.0, "linkability2": 0.0, "linkability3": 2.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "You may check the definition of extinction problem.", "messagesuccess": "Great! You have considered all possibilities!", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [20, 21, 22], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 54, "fields": {"code": "3.4.5", "category": 4, "problem": "{Mickey in Maze with an exit.}\r\nSuppose now there is an exit in cell 2 (See the graph below). Assume Mickey moves around in the same way\r\nas before, except that, once in cell 2, he has chance 0.5 to get out and never return\r\nand chance equal chance 1/4 to go to cell 1 or 5.\r\nSuppose Mickey begins in cell 6, what's the mean number of visits of cell 1?", "problempicture1": "theall/image/DIY-3.4.5-1.PNG", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "6.", "choicesb": "5.", "choicesc": "4.", "choicesd": "3.", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $s_i$ be the mean number of visits of cell 1 beginning from\r\ncell $i$, $including$ the beginning state if it is 1.\r\nThen,\r\n\\begin{eqnarray*}\r\n&&s_0 = 1/2(s_1+s_3) \\qquad  \\qquad\r\ns_1= 1+ 1/3(s_0+s_2+s_4) \\qquad \\quad\r\ns_2= 1/4(s_1+s_5) \\\\\r\n&&s_3=1/3(s_0+s_4+s_6) \\qquad s_4=1/4(s_1+s_3+s_5+s_7) \\qquad  \\,\\,  s_5=1/3(s_2+s_4+s_8) \\\\\r\n&&s_6=1/2(s_3+s_7) \\qquad \\qquad s_7=1/3(s_4+s_6+s_8 ) \\qquad \\qquad \\,\\, \\,  s_8=1/2(s_5+s_7)\r\n\\end{eqnarray*}\r\nSolve this equation, we have\r\n$$s_0=3.375,\\,\\, s_1= 3.625,\\,\\, s_2=1.5,\\,\\, s_3=3.125,\\,\\, s_4=3,\\,\\, s_5=2.375,\\,\\,s_6=3,\\,\\,\r\n s_7=2.876,\\,\\, s_8=2.625.$$\r\n The mean number of visits of state 1 beginning from state 6 is 3.\r\n<br><br>\r\nRemark. \r\n<br>\r\nThe above exercise is a little tedious in solving the nine linear equations.\r\nIn the exam, the number equations would be much smaller. For this problem, no symmetry\r\ncan be used to shorten number of equations. If, instead, the problem is to\r\n calculate the mean number of times the MC visits state 4, beginning from\r\nstate 6, symmetry can be used to eliminate three of the nine equations.", "linkability1": 3.0, "linkability2": 3.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "Let $a_i$ be the mean number of visits of cell 1 beginning from\r\ncell $i$, $excluding$ the beginning state if it is 1.\r\n Then,\r\n\\begin{eqnarray*}\r\n&&a_0 = 1/2(a_1+1 +a_3) \\qquad\r\n\\,\\, a_1=   1/3(a_0+a_2+a_4) \\qquad \\qquad \\qquad\r\na_2= 1/4(a_1+1 +a_5) \\\\\r\n&&a_3=1/3(a_0+a_4+a_6) \\qquad a_4=1/4(a_1+1+a_3+a_5+a_7) \\qquad a_5=1/3(a_2+a_4+a_8) \\\\\r\n&&a_6=1/2(a_3+a_7) \\qquad\\qquad  a_7=1/3(a_4+a_6+a_8 ) \\qquad\\qquad \\qquad a_8=1/2(a_5+a_7)\r\n\\end{eqnarray*}\r\nThe equations are the same as those of Method 1 with $a_1+1=s_1$ and $a_i=s_i$ for $i\\not=1$.\r\nWe get the same solutions, (except for $a_1=s_1-1=2.625$).", "messagefailure": "Oops, try again. Get yourself familiar with the application of first step analysis!", "messagesuccess": "Well done! You have mastered the concept of first step analysis!", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [3, 5, 9, 15], "rightproblems": [5], "wrongproblems": [4], "twinproblems": [4]}}, {"model": "mathematics.question", "pk": 55, "fields": {"code": "3.9.2", "category": 4, "problem": "How is $u_\\infty$ related to $s=\\phi(s)$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "The larger solution of $s=\\phi(s)$ on $[0, \\infty]$ is $u_\\infty$.", "choicesb": "The smaller solution of $s=\\phi(s)$ on $[0, \\infty]$ is $u_\\infty$.", "choicesc": "There is no certain answers.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $a$ be a solution of\r\n$s=\\phi(s)$ on\r\n$[0, 1]$. Since $1$ is always a solution, $a$ is well defined.\r\nObserve that $u_1=p_0=\\phi(0)$. Since $\\phi(\\cdot)$ is\r\nnondecreasing, we have\r\n$a =\\phi(a) \\geq \\phi(0) = u_1$. For any $n \\geq 1$, assume now\r\n$a \\geq u_n$. Then\r\n$a =\\phi(a) \\geq \\phi(u_n) = u_{n+1}$.\r\nBy induction, it follows that\r\n$a \\geq u_n$ for all $n \\geq 1$.\r\nTaking limit, we know\r\n$a \\geq u_\\infty$.\r\nConsequently, $u_\\infty$ must be the smallest solution\r\non $[0, 1]$ of\r\nthe equation $s=\\phi(s)$.", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 3.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Get yourself familiar with probability generating function!", "messagesuccess": "Well done! You have mastered the concepts of probability generating function and branching process!", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 5, "calculateddifficulty": null, "linkneuron": [19, 20], "rightproblems": [131, 132], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 123, "fields": {"code": "4.1.1", "category": 4, "problem": "Following Example 4.4,\r\nWhat is the average number of times per day that Mr C goes from Gym to\r\nHome?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.343", "choicesb": "0.274", "choicesc": "0.835", "choicesd": "0.8", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Recall that in Example 4.4, $X_n$ represent the states (whereabouts)\r\nof Mr. C. after $n$ steps of transitions. Over a large number, $n$, of steps,\r\nthe fraction of  transitions from G to H is\r\n$${ \\sum_{i=1}^n 1_{\\{X_{i-1}=G, X_i=H\\}} \\over n }\r\n\\approx {\\pi_G } \\times P(X_{n+1}=H|X_n=G)= 0.343 \\times 0.8= 0.274$$\r\nFor a large $n$ transitions, the time in hours are\r\n$$ (12 \\pi_H + 10 \\pi_O + 2 \\pi_G )n = 7.864 n,$$\r\nwhich is $ 7.864 n/24= 0.328n$ days.\r\nIn other words, there are about $ 0.274n$ transitions from G to H\r\nover $0.328n$ days. On average, it is about $ 0.274/.328= 0.835$ times\r\nfrom G to H daily.", "linkability1": 3.0, "linkability2": 2.0, "linkability3": 3.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. You may revise Chapter 4.1 - 4.2.", "messagesuccess": "You get it!", "sensitivity": 1.0, "gussingparameter": 0.25, "difficulty": 5, "calculateddifficulty": null, "linkneuron": [37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 124, "fields": {"code": "4.2.2", "category": 4, "problem": "In Example 4.5, is the process $\\{(X_{n-1}, X_{n}): n \\geq 2\\}$   a $MC$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes.", "choicesb": "No.", "choicesc": "It is uncertain.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No. It is not a MC, because,\r\n\\begin{eqnarray*} && P(X_{n+1}=2| X_n=1, X_{n-1}=1, X_{n-2}=2) =1-p  \\\\\r\n&\\not=& 0\r\n=P(X_{n+1}=2| X_n=1, X_{n-1}=1, X_{n-2}=1, X_{n-3}=2).\r\n\\end{eqnarray*}\r\n(Please contemplate why?)", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 3.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Sorry, it is not correct. Time to revise the concept of Markov chain!", "messagesuccess": "Great! You got the concept of Markov chain!", "sensitivity": 1.0, "gussingparameter": 0.333, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [3], "rightproblems": [], "wrongproblems": [2], "twinproblems": [2]}}, {"model": "mathematics.question", "pk": 125, "fields": {"code": "4.2.4", "category": 4, "problem": "Let a random variable $\\xi$\r\nfollow   exponential distribution\r\nwith density\r\n$ f(x)= e^{-x}$ for $x>0$ and $0$ for $x < 0$.\r\nIn Example 4.6, suppose every up (down) day, the rise (fall), as multiple\r\nof the previous day's closing price, has the same distribution\r\nas that of $\\xi+1$ $(1/(a \\xi+1))$, independent of everything else, where\r\n$a>0$ is a constant.\r\nIs the information above enough to calculate the long term return of the stock, particularly when\r\n$a=1$ or $a=7/4$ or $a=4/7$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes", "choicesb": "No.", "choicesc": "It is uncertain.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $\\eta_i$ be the return of day $n$, as multiple of\r\nthe closing price of day\r\n$n-1$. Then, 1 dollar will grow to\r\n$$\\prod_{i=1}^n \\eta_i\r\n= e^{n \\times (1/n) \\sum_{i=1}^n \\log \\eta_i } $$\r\nafter\r\n$n$ days of trading. Recall that the long run fraction of up (down)\r\ndays is $4/11$ ($7/11$).\r\nObserve that\r\n\\begin{eqnarray*}\r\n&& (1/n)\\sum_{i=1}^n \\log \\eta_i   \\approx 4/11 \\times E\\{\\log(\\xi+1)\\}\r\n+ 7/11 \\times E\\{-\\log(a\\xi + 1)\\}\r\n\\\\\r\n&=&\r\n {4\\over 11}  \\int_0^\\infty \\log(x+1) e^{-x} dx\r\n- {7\\over 11}  \\int_0^\\infty \\log(ax+1) e^{-x} dx\r\n\\end{eqnarray*}\r\nIf $a \\geq 1$, obviously the above quantity is negative.\r\nTherefore, in the long run the stock decreases exponentially fast.\r\nIf $a=4/7$, since $\\log(ax+1) > a \\log(x+1)$, the above quantity is\r\nstill negative, and the stock still decreases exponentially fast.\r\nThe dividing value of $a$ is 0.4635. And the stock increases\r\n(decreases) exponentially fast\r\nwhen $a<0.4635$ ($a>0.4635$).", "linkability1": 3.0, "linkability2": 3.0, "linkability3": 3.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again.", "messagesuccess": "Well done! You have mastered the concept of long run behavior.", "sensitivity": 1.0, "gussingparameter": 0.33, "difficulty": 5, "calculateddifficulty": null, "linkneuron": [37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 126, "fields": {"code": "4.3.2", "category": 4, "problem": "Can you prove Proposition 4.1 (a), (b) and (c)?\r\n<br>\r\n((b) and (c) are beyond requirement)", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes, I can!", "choicesb": "I have no idea...", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "(a). By the definition of $d(i)$, all $n$ with $P_{ii}^{(n)} >0$\r\ncontains $d(i)$ as a factor. Whence, if $k$ is not a multiple of $d(i)$,\r\n$P_{ii}^{(k)}=0$.\r\n\r\n(b). Citing a result of algebra regarding gcd: there exists\r\n$n_1, n_2, ..., n_k$ with $P_{ii}^{(n_j)}>0$ and non-zero integers $a_1,\r\n..., a_k$ such that\r\n$d(i)=a_1 n_1 + \\cdots + a_k n_k$. (Proof of this result is also elmentary;\r\nsee the following remark.)\r\nFor notational simplicity, assume $a_1,..., a_l$ are positive and $a_{l+1},..., a_k$ are\r\nnegative, for some $1 \\leq l \\leq k$.\r\nLet $m=-(a_{l+1}n_{l+1} +\\cdots + a_k n_k) >0$.\r\nFor any  $n\\geq m^2+m$, let $n^*$ be such that\r\n$n  = n^* m + r$, where $ 0 \\leq r < m$. Then,\r\n$n^* > m $ and\r\n\\begin{eqnarray*}\r\nnd(i) &=& n^*m d(i) + r d(i) = n^*m d(i) + r a_1 n_1 + \\cdots + ra_k a_k\r\n\\\\\r\n&=&\r\n -(n^*d(i) - r)(a_{l+1} n_{l+1} + \\cdots + a_k n_k)\r\n+ r a_1 n_1 \\cdots + ra_l a_l.\r\n\\end{eqnarray*}\r\nNotice that $n^*d(i) - r>0$.\r\nThen the claim of part (b) follows by\r\nobserving that, for any nonnegative integers $s_1,...,s_k$,\r\n$$P_{ii}^{(s_1 n_1+ \\cdots s_k n_k)} \\geq\r\nP_{ii}^{(s_1 n_1)} \\times \\cdots \\times P_{ii}^{(s_k n_k)}\r\n\\geq \\{ P_{ii}^{( n_1)} \\}^{s_1}\\times \\cdots \\times\r\n\\{P_{ii}^{(n_k)}\\}^{s_k} > 0.$$\r\n\r\n\r\n(c). If $i \\leftrightarrow j$, there exist $k>0$ and $l>0$ such that\r\n$P_{ij}^{(k)}>0$ and $P_{ji}^{(l)}>0$.\r\nSince, by part (b), $P_{jj}^{(n d(j))}>0$ for all large $n$, it follows\r\nthat\r\n$$ P_{ii}^{ (k+l + n d(j))} \\geq P_{ij}^{(k)} P_{jj}^{ (n d(j))} P_{ji}^{(l)}\r\n>0,$$\r\nfor all large $n$. Hence $k+l + n d(j)$ contains $d(i)$ as a factor\r\nfor all large $n$. Then the difference of\r\n$k+l + (n+1) d(j)$ and $k+l+n d(j)$ for a large $n$, which is\r\n$d(j)$ must contain $d(i)$ as a factor.\r\nAs a result, $d(j) \\geq d(i)$. Likewise, one can show\r\n$d(i) \\geq d(j)$. It then holds that\r\n$d(i)=d(j)$.\r\n\r\n<br><br>\r\nRemark. \r\n<br>\r\nSuppose ${\\cal N}$ is a set of positive integers and $J$ is\r\nthe gcd of ${\\cal N}$. Then,\r\n$$ J= \\min\\{ a_1 n_1+ \\cdots + a_k n_k > 0: \\hbox{ for\r\nall $k\\geq 1$, } n_i \\in {\\cal N} \\hbox{ and } a_i\r\n\\hbox{ are nonzero integers} \\}.$$\r\n In other words, $J$ is the smallest positive integer as a\r\n  combination of elements of ${\\cal N}$ with the combination coefficients\r\n  being integers.\r\n  Proof is a little technical but not too difficult: Suppose $J^* \\equiv a_1 n_1 + \\cdots +a_k n_k$\r\n  is indeed the smallest positive number among such combinations.\r\n  Obviously, $J^* \\leq n$ for all $n \\in {\\cal A}$.\r\n  For any $n \\in {\\cal A}$, write $n= n^* J^* + r$ where\r\n  $ 0 \\leq r < J^*$. Then, $r=0$, for otherwise\r\n  $$n-n^*(a_1 n_1+\\cdots a_k n_k)= n-n^*J^* = r$$\r\n  produces a positive number smaller than $J^*$.\r\n  Hence $J^*$ is a common divisor of ${\\cal N}$.\r\n  Suppose now all $n \\in {\\cal N}$ contain, in addition to\r\n  $J^*$, another common factor $j\\geq 1$.\r\n  Set $b_i = n_i/(J^* j)$ which are positive integers.\r\n  Then,\r\n  $$J^* = a_1 n_1+ \\cdots a_k n_k = J^* j (a_1b_1 + \\cdots + a_kb_k)$$\r\n  As a result, $1/j= (a_1 b_1 + \\cdots +a_k b_k)$, implying that\r\n  $j$ must be $1$. This proves $J^*$ is the gcd $J$.\r\n  As mentioned before, part (b) and (c) in the above Exercise\r\n  is too technical and probabilistically uninteresting and thus not required.", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 3.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Don't worry, it is not that easy. You may go through the answer and see whether you understand it!", "messagesuccess": "Excellent! You make it! I am sure now you have a better understanding of Proposition 4.1!", "sensitivity": 1.0, "gussingparameter": 0.5, "difficulty": 5, "calculateddifficulty": null, "linkneuron": [37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 127, "fields": {"code": "3.1.9", "category": 4, "problem": "A Markov chain $X_n$ ($X_0=0$) has two states 0 and 1 \r\nand transition probability matrix \r\n$$ \\hskip .41in 0 \\quad \\,\\,\r\n\\, 1 $$\r\n$$ \r\n{\\bf P}= \\matrix{0 \\cr 1} \\pmatrix{.80  \\quad .20 \\cr \r\n\t.30\\quad .70}\r\n$$\r\nThen what is $P(X_3=0, X_1 \\not=0, X_2 \\not=0 \\,\\,|\\,\\, X_0=0)$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.7", "choicesb": "0.512", "choicesc": "0.042", "choicesd": "0.8", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "\\begin{eqnarray*}\r\n&&P(X_{3}=0, X_{1} \\not=0, X_{2} \\not=0 \\,\\,|\\,\\, X_{0}=0)\r\n\\\\\r\n&=&\r\nP(X_{3}=0\\,\\,|\\,\\, X_{1}\\not=0, X_{2}\\not=0, X_{0}\\not=0)P(X_{1}\\not=0, X_{2}\\not=0\\,\\,|\\,\\,X_{0}=0)\r\n\\\\\r\n&=&\r\nP(X_{3}=0\\,\\,|\\,\\, X_{2}\\not=0)P(X_{2}\\not=0\\,\\,|\\,\\,X_{1}\\not=0, X_{0}=0)P(X_{1}\\not=0\\,\\,|\\,\\,X_{0}=0)\r\n\\\\\r\n&=&\r\nP(X_{3}=0\\,\\,|\\,\\, X_{2}\\not=0)P(X_{2}\\not=0\\,\\,|\\,\\,X_{1}\\not=0)P(X_{1}\\not=0\\,\\,|\\,\\,X_{0}=0)\r\n\\\\\r\n&=&\r\nP_{10}P_{11}P_{01}\r\n\\\\\r\n&=&\r\n0.3\\times 0.7\\times 0.2\r\n\\\\\r\n&=&\r\n0.042\r\n\\end{eqnarray*}", "linkability1": 1.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, you may try it again. Or revise the concept of Markov chain.", "messagesuccess": "Great! You have mastered the concept of Markov chain!", "sensitivity": 1.0, "gussingparameter": 0.25, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [3, 5, 6, 7], "rightproblems": [41], "wrongproblems": [40], "twinproblems": [40]}}, {"model": "mathematics.question", "pk": 128, "fields": {"code": "3.4.12", "category": 4, "problem": "A Markov chain has transition probability matrix\r\n$$ \\hskip .4in 0 \\quad \\, 1 \\quad \\,\\,2 $$\r\n$${\\bf P}= \\matrix{0 \\cr 1 \\cr 2} \\pmatrix{\r\n\t0 \\quad .5 \\quad .5 \\cr\r\n\t0 \\quad .5 \\quad .5 \\cr\r\n\t0 \\quad .5 \\quad .5  }\r\n$$\r\nWhat is the mean time to reach state 2 starting from state 0?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.", "choicesb": "1.", "choicesc": "2.", "choicesd": "3.", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $s_i$ be the mean time to reach state 2 starting from\r\nstate $i$, $including$ the beginning state if it is 2.\r\nThen,\r\n\\begin{eqnarray*}\r\n&&s_0 = 1 + 0.5\\times(s_1+s_2)\r\n\\\\\r\n&&s_1 = 1 + 0.5\\times(s_1+s_2)\r\n\\\\\r\n&&s_2 = 0\r\n\\end{eqnarray*}\r\nSolve this equation, we have\r\n$$s_0=s_1=2,\\,\\, s_2= 0$$\r\n The mean time to reach state 2 starting from state 0 is 2.", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. It is all about calculating the mean step to a state!", "messagesuccess": "Good! Now you know how to calculate the mean step to a state!", "sensitivity": 1.0, "gussingparameter": 0.2, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [9, 13], "rightproblems": [8], "wrongproblems": [95], "twinproblems": [95]}}, {"model": "mathematics.question", "pk": 129, "fields": {"code": "3.4.14", "category": 4, "problem": "A Markov chain $\\{X_n\\}$ $(X_0=0)$ has transition probability matrix\r\n $$ \\hskip .4in 0 \\quad \\, 1 \\quad \\,\\,2 $$\r\n $${\\bf P}= \\matrix{0 \\cr 1 \\cr 2} \\pmatrix{\r\n \t0  \\quad 0.6 \\quad 0.4 \\cr\r\n \t0   \\quad 0.5 \\quad 0.5 \\cr\r\n \t0   \\quad 0.5 \\quad 0.5  }\r\n $$\r\nLet $T=\\min\\{ n\\geq 0: X_n=2, X_{n+1}=2\\}$, then what is $E(T)$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1.2", "choicesb": "2.4", "choicesc": "3.6", "choicesd": "4.8", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $s_i$ be the mean time to reach 2 such that $X_{s_i} = X_{s_i + 1} = 2$, beginning from state $i$. Then,\r\n\\begin{eqnarray*}\r\n&&s_0 = 0.6s_1 + 0.4s_2 + 1\r\n\\\\\r\n&&s_1 = 0.5s_1 + 0.5s_2 + 1\r\n\\\\\r\n&&s_2 = 0.5(s_1+1) + 0.5\\times0\r\n\\end{eqnarray*}\r\nThe solutions are $s_0 = 3.6, \\,\\, s_1 = 3, \\,\\, s_2 = 2$.\r\n\r\nSince $X_0 = 0$, we have $E(T) = s_0 = 3.6$.", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. It is about calculating the mean step to a state!", "messagesuccess": "Great! Now you should be familiar with calculating the mean step to a state.", "sensitivity": 1.0, "gussingparameter": 0.2, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [13], "rightproblems": [51], "wrongproblems": [95], "twinproblems": [95]}}, {"model": "mathematics.question", "pk": 130, "fields": {"code": "3.4.16", "category": 4, "problem": "A Markov chain $\\{X_n\\}$ $(X_0=0)$ has transition probability matrix\r\n$$ \\hskip .4in 0 \\quad \\, 1 \\quad \\,\\,2 $$\r\n$${\\bf P}= \\matrix{0 \\cr 1 \\cr 2} \\pmatrix{\r\n\t.2  \\quad 0.4 \\quad 0.4 \\cr\r\n\t0   \\quad 0.5 \\quad 0.5 \\cr\r\n\t0   \\quad 0.5 \\quad 0.5  }\r\n$$\r\n\r\nWhat is the mean number of visits of state 1 before reaching state 2?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1", "choicesb": "2", "choicesc": "3", "choicesd": "4", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $w_i$ be the mean number of visits of state 1 before reaching state 2, starting from state $i$ (including the starting state).\r\nThen,\r\n\\begin{eqnarray*}\r\n&&w_0 = 0.2w_0 + 0.4w_1 + 0.4w_2\r\n\\\\\r\n&&w_1 = 1 + 0.5w_1 + 0.5w_2\r\n\\\\\r\n&&w_2 = 0\r\n\\end{eqnarray*}\r\nSolving the equations, \r\n$$w_0 = 1, \\,\\, w_1 = 2, \\,\\, w_2 = 0$$\r\nSince $X_0 = 0$, the mean number of visits of state 1 before reaching state 2 is 1.", "linkability1": 1.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. It is about calculating the mean number of visits to a state!", "messagesuccess": "Great! Now you should be familiar with calculating the mean number of visits to a state.", "sensitivity": 1.0, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [15], "rightproblems": [50], "wrongproblems": [4], "twinproblems": [4]}}, {"model": "mathematics.question", "pk": 131, "fields": {"code": "3.9.4", "category": 4, "problem": "Recall that the $n$-th generation of a branching\r\nprocess can be expressed as\r\n$$X_{n+1}= \\sum_{j=0}^{X_n} \\xi^{(n)}_j.$$\r\nwith $\\xi, \\xi^{(n)}_j, j \\geq 1$ are iid. And\r\n$X_0=1$ and $\\xi^{(n)}_0=0.$\r\nLet $\\phi(s)$ be the probability generating function\r\nof $\\xi$.\r\nSuppose $\\phi(s)=0.7+0.1s+0.1s^2+0.1s^3.$\r\nWhat is the probability of eventual extinction?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.2", "choicesb": "0.4", "choicesc": "0.6", "choicesd": "0.8", "choicese": "1", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "E", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Since\r\n$\\phi(s)=0.7+0.1s+0.1s^2+0.1s^3,$\r\n\\begin{eqnarray*}\r\n\\phi(s)-s=0 &\\Rightarrow& 0.7+0.1s+0.1s^2+0.1s^3-s=0 \\\\\r\n&\\Rightarrow&(s-1)(s^2 + 2s - 7)=0\\\\\r\n&\\Rightarrow& s=1 \\,\\, {\\rm or }\\,\\, s = -1+2\\sqrt{2} \\,\\, {\\rm or }\\,\\, s = -1-2\\sqrt{2}.\r\n\\end{eqnarray*}\r\nSince only $s=1\\in [0,1]$, we have $u_{\\infty}=1$.", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Get yourself familiar with Branching process.", "messagesuccess": "Great! You have mastered a typical type of questions about Branching process.", "sensitivity": 1.0, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [14, 20, 21, 22], "rightproblems": [55], "wrongproblems": [83, 132], "twinproblems": [83, 132]}}, {"model": "mathematics.question", "pk": 132, "fields": {"code": "3.9.6", "category": 4, "problem": "Let $\\phi(\\cdot)$ be  the probability generating\r\nfunction of $\\xi$, the number of direct next generation of any\r\nindividual of a population that evolves as a branching process.\r\nSuppose\r\n$$\\phi(s) =\r\n.5 s^2 + bs +c, \\qquad 0 \\leq s \\leq 1.$$\r\nAssume the population begins with one\r\nindividual ($X_0=1$). What is the probability of eventual\r\nextinction?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.4", "choicesb": "0.6", "choicesc": "0.8", "choicesd": "1.0", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "E", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "\\begin{eqnarray*}\r\n{\\rm Since}\\,\\,\\phi(1) = 1 &\\Rightarrow&\\phi(1) = 0.5 + b + c = 1\\\\\r\n&\\Rightarrow&c = .5 - b\\\\\r\n&\\Rightarrow&\\phi(s) = .5 s^2 + bs +.5-b \\\\\r\n\\end{eqnarray*}\r\n<br>\r\n\r\n\\begin{eqnarray*}\r\n\\phi(s)-s=0 &\\Rightarrow& .5s^2+(b-1)s+.5-b=0 \\\\\r\n&\\Rightarrow&(s-1)(.5s+b-.5)=0\\\\\r\n&\\Rightarrow&u_{\\infty} = s = \\left\\{ \\begin{array}{rl}\r\n 1-2b &\\mbox{ if $0<b<.5$} \\\\\r\n  1 &\\mbox{ otherwise.}\r\n       \\end{array} \\right.\r\n\\end{eqnarray*}", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Get yourself familiar with Branching process.", "messagesuccess": "Great! You have mastered a typical type of questions about Branching process.", "sensitivity": 1.0, "gussingparameter": 0.2, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [14, 20, 21, 22], "rightproblems": [55], "wrongproblems": [131], "twinproblems": [131]}}, {"model": "mathematics.question", "pk": 134, "fields": {"code": "4.1.2", "category": 4, "problem": "A mouse initially stays in cell 0\r\n in the following cage of four cells.\r\n At each step, it moves to one of\r\n the two connected cells with equal chance.\r\n Let $X_n=i$ if the mouse\r\n stays at cell $i$ at step $n$.\r\n Over a very long time,\r\n what  fraction of\r\n the transitions are the transitions from cell 0 to cell 1?", "problempicture1": "theall/image/DIY-4.1.2-1.PNG", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1/8", "choicesb": "1/6", "choicesc": "1/4", "choicesd": "1/2", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The transition matrix for $X_n$ is\r\n \\begin{eqnarray*}\r\n&&  \\,\\,\\qquad  0\\quad \\,\\, 1 \\quad \\,\\, 2 \\quad\\,\r\n 3\\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0}  \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3}}\r\n\\pmatrix{\r\n0 & .5 & 0 & .5  \\cr\r\n.5   & 0   & .5 & 0  \\cr\r\n0 & .5 & 0 & .5  \\cr\r\n.5   & 0   & .5 & 0\r\n}\r\n\\end{eqnarray*}\r\n\r\nNotice that\r\n$${\\bf P}^k ={\\bf P} \\quad \\hbox {for all odd $k$}, \\qquad\r\n{\\bf P}^k= \\pmatrix{.5 & 0 & .5 & 0 \\cr 0  & .5 & 0 & .5 \\cr .5 & 0 & .5 & 0 \\cr 0  & .5 & 0 & .5 } \\qquad \\hbox{for all even $k$}.$$\r\nLet $\\pi_j$ be the long run fraction of time staying in state $j$ after an odd-number times of transitions. Solve the equations\r\n$$(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2\\,\\,\\pi_3)=(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2\\,\\,\\pi_3){\\bf P}^2$$\r\n$$\\pi_0 + \\pi_1 + \\pi_2 + \\pi_3=1$$\r\nThe solutions are $\\pi_0 = \\pi_1 = \\pi_2 = \\pi_3 = 1/4$.\r\n\r\nSimilarly, if the number of transitions is even in a long run, we also have\r\n$$(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2\\,\\,\\pi_3)=(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2\\,\\,\\pi_3){\\bf P}^2$$\r\n$$\\pi_0 + \\pi_1 + \\pi_2 + \\pi_3=1$$\r\nThus in the long run, $\\pi_0 = \\pi_1 = \\pi_2 = \\pi_3 = 1/4$.\r\n\r\nIn the long run, the fraction of the transitions from cell 0 to cell 1 is\r\n\\begin{eqnarray*}\r\n&&P(X_{n+1}=1,\\,X_n=0)\r\n\\\\\r\n&=&\r\nP(X_{n+1}=1,\\,X_n=0\\,|\\,X_{n-1}=0)+P(X_{n+1}=1,\\,X_n=0\\,|\\,X_{n-1}=1)\r\n\\\\\r\n&&\r\n+P(X_{n+1}=1,\\,X_n=0\\,|\\,X_{n-1}=2)+P(X_{n+1}=1,\\,X_n=0\\,|\\,X_{n-1}=3)\r\n\\\\\r\n&=& \r\nP_{01}P_{00}\\pi_0+P_{01}P_{10}\\pi_1+P_{01}P_{20}\\pi_2+P_{01}P_{30}\\pi_3\r\n\\\\\r\n&=&\r\n{1\\over 4} P_{01}(P_{00}+P_{10}+P_{20}+P_{30})\r\n\\\\\r\n&=&\r\n1\\over 8\r\n\\end{eqnarray*}", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Get yourself familiar with Long run behavior.", "messagesuccess": "Great! You have mastered a typical type of questions about long run behavior.", "sensitivity": 1.0, "gussingparameter": 0.2, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [35, 37, 39], "rightproblems": [103, 135], "wrongproblems": [102], "twinproblems": [102]}}, {"model": "mathematics.question", "pk": 135, "fields": {"code": "4.1.4", "category": 4, "problem": "A Markov chain $X_n, n\\geq 0$, has transition probability\r\nmatrix\r\n$$ \\hskip .4in 0 \\quad \\, 1 \\quad \\,\\,2 $$\r\n$${\\bf P}= \\matrix{0 \\cr 1 \\cr 2} \\pmatrix{\r\n\t.2 \\quad .5 \\quad .3 \\cr 0 \\quad .5 \\quad .5 \\cr .4 \\quad .5 \\quad\r\n\t.1  }\r\n$$\r\nWhat is the limit of $P(X_n=0, X_{n+1}=1, X_{n+2}=2)$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1/24", "choicesb": "1/8", "choicesc": "1/6", "choicesd": "1/4", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $\\pi_j$ be the long run fraction of time staying in state $j$. Notice that ${\\bf P}$ is regular. Solve the equations\r\n$$(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2)=(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2){\\bf P}^2$$\r\n$$\\pi_0 + \\pi_1 + \\pi_2=1$$\r\nThe solutions are $\\pi_0 = 1/6$, $\\pi_1 =1/2$, $\\pi_2 =1/3$.\r\n\\begin{eqnarray*}\r\n&&P(X_n=0, X_{n+1}=1, X_{n+2}=2)\r\n\\\\\r\n&=& \r\nP(X_{n+2}=2\\,|\\,X_{n+1}=1,\\,X_n=0)P(X_{n+1}=1\\,|\\,X_n=0)P(X_n=0)\r\n\\\\\r\n&=&\r\nP(X_{n+2}=2\\,|\\,X_{n+1}=1)P(X_{n+1}=1\\,|\\,X_n=0)P(X_n=0)\r\n\\\\\r\n&=&\r\nP_{12}P_{01}\\pi_0\r\n\\\\\r\n&=&\r\n1/2 \\times 1/2 \\times 1/6\r\n\\\\\r\n&=&\r\n1/24.\r\n\\end{eqnarray*}", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Get yourself familiar with Long run behavior.", "messagesuccess": "Great! You have mastered a typical type of questions about long run behavior.", "sensitivity": 1.0, "gussingparameter": 0.2, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [35, 37, 39], "rightproblems": [134], "wrongproblems": [102], "twinproblems": [102]}}, {"model": "mathematics.question", "pk": 136, "fields": {"code": "4.1.6", "category": 4, "problem": "The following is the transition probability matrix\r\nof a Markov chain with states 0, 1 and 2:\r\n$$ \\hskip .4in 0 \\quad \\,\\,\\,\\,\\, 1 \\quad \\,\\,\\,\\,\\, 2 $$\r\n$${\\bf P} = \\matrix{0 \\cr 1 \\cr 2}\r\n\\pmatrix{.50 \\quad .50 \\quad \\,\\, 0 \\cr\r\n\t\\,\\,.25 \\quad .50 \\quad .25 \\cr\r\n\t\\,\\,\\, 0 \\quad \\, \\,\\,\\, .50 \\quad \\, .50 }\r\n$$\r\nSomeone is paid 2 dollar for each one step transit from\r\nstate 1 to state 2 and  $-1$ dollar each one step transit from\r\nstate 2 to state 1. After 1000,000 steps, approximately how much\r\nis gained?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "50,000", "choicesb": "75,000", "choicesc": "100,000", "choicesd": "125,000", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $\\pi_j$ be the long run fraction of time staying in state $j$. Notice that ${\\bf P}$ is regular. Solve the equations\r\n$$(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2)=(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2){\\bf P}$$\r\n$$\\pi_0 + \\pi_1 + \\pi_2=1$$\r\nThe solutions are $\\pi_0 = 1/4$, \\, $\\pi_1 =1/2$,\\, $\\pi_2 =1/4$.\r\n\\begin{eqnarray*}\r\n&&P(X_{n+1}=1, X_{n+2}=2)\r\n\\\\\r\n&=& \r\nP(X_{n+1}=1, X_{n+2}=2\\,|\\,X_n=0)+P(X_{n+1}=1, X_{n+2}=2\\,|\\,X_n=1)+P(X_{n+1}=1, X_{n+2}=2\\,|\\,X_n=2)\r\n\\\\\r\n&=&\r\nP(X_{n+2}=2\\,|\\,X_{n+1}=1)P(X_{n+1}=1\\,|\\,X_n=0)P(X_n=0)\r\n\\\\\r\n&&\r\n+P(X_{n+2}=2\\,|\\,X_{n+1}=1)P(X_{n+1}=1\\,|\\,X_n=1)P(X_n=1)\r\n\\\\\r\n&&\r\n+P(X_{n+2}=2\\,|\\,X_{n+1}=1)P(X_{n+1}=1\\,|\\,X_n=2)P(X_n=2)\r\n\\\\\r\n&=&\r\nP_{12}P_{01}\\pi_0+P_{12}P_{11}\\pi_1+P_{12}P_{21}\\pi_2\r\n\\\\\r\n&=&\r\n1/8\r\n\\end{eqnarray*}\r\n\\begin{eqnarray*}\r\n&&P(X_{n+1}=2, X_{n+2}=1)\r\n\\\\\r\n&=& \r\nP(X_{n+1}=2, X_{n+2}=1\\,|\\,X_n=0)+P(X_{n+1}=2, X_{n+2}=1\\,|\\,X_n=1)+P(X_{n+1}=2, X_{n+2}=1\\,|\\,X_n=2)\r\n\\\\\r\n&=&\r\nP(X_{n+2}=1\\,|\\,X_{n+1}=2)P(X_{n+1}=2\\,|\\,X_n=0)P(X_n=0)\r\n\\\\\r\n&&\r\n+P(X_{n+2}=1\\,|\\,X_{n+1}=2)P(X_{n+1}=2\\,|\\,X_n=1)P(X_n=1)\r\n\\\\\r\n&&\r\n+P(X_{n+2}=1\\,|\\,X_{n+1}=2)P(X_{n+1}=2\\,|\\,X_n=2)P(X_n=2)\r\n\\\\\r\n&=&\r\nP_{21}P_{02}\\pi_0+P_{21}P_{12}\\pi_1+P_{21}P_{22}\\pi_2\r\n\\\\\r\n&=&\r\n1/8\r\n\\end{eqnarray*}\r\nThe approximate gain after 1000,000 steps would be\r\n$$1000,000 \\times {{1\\over 8} \\times {2} + {1\\over 8} \\times (-1)} = 125,000.$$", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Get yourself familiar with Long run behavior.", "messagesuccess": "Great! You have mastered a typical type of questions about long run behavior.", "sensitivity": 1.0, "gussingparameter": 0.2, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [35, 37, 39], "rightproblems": [], "wrongproblems": [102], "twinproblems": []}}, {"model": "mathematics.question", "pk": 137, "fields": {"code": "4.2.6", "category": 4, "problem": "A stock has only two outcomes of performance\r\n on each trading day: up (U) or down (D). Let $X_n$ be\r\n its performance outcome on trading day $n$. The stock performance\r\n on any day depends only on those of the preceding two days. Assume\r\n for all $n\\geq 1$,\r\n $P(X_{n+1}=U| X_n = U, X_{n-1}=U) = 0.9$,\r\n $P(X_{n+1}=U| X_n = U, X_{n-1}=D) = 0.7$, $P(X_{n+1}=U| X_n = D,\r\n X_{n-1}=U) = 0.4$, $P(X_{n+1}=U| X_n = D, X_{n-1}=D) = 0.2$. What\r\n is the approximate chance that the stock is up on the trading day ten years from\r\n now?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1/3", "choicesb": "1/4", "choicesc": "2/3", "choicesd": "3/4", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Set $Y_n=(X_{n-1},X_n)$. Then $Y_n$ is a $MC$\r\nwith state space $\\{({\\rm U, U}), ({\\rm D, U}),\r\n({\\rm U, D}),   ({\\rm D, D})\\}$ and transition probability\r\nmatrix\r\n$$ \\hskip .8 in 0 ({\\rm U, U}) \\quad \\,\\,\\,\\,\\, 1 ({\\rm D, U}) \\quad \\,\\,\\,\\,\\, 2 ({\\rm U, D})\\quad \\,\\,\\,\\,\\, 3 ({\\rm D, D}) $$\r\n$${\\bf P} = \\matrix{0 ({\\rm U, U}) \\cr 1 ({\\rm D, U}) \\cr 2 ({\\rm U, D})\\cr 3 ({\\rm D, D})}\r\n\\pmatrix{.9\\quad\\quad \\quad\\quad\\quad 0 \\quad\\quad\\quad\\quad\\quad .1 \\quad\\quad\\quad\\quad\\quad 0\\cr\r\n\t.7\\quad\\quad \\quad\\quad\\quad 0 \\quad\\quad\\quad\\quad\\quad .3 \\quad\\quad\\quad\\quad\\quad 0\\cr\r\n\t0\\quad\\quad \\quad\\quad\\quad .4 \\quad\\quad\\quad\\quad\\quad 0 \\quad\\quad\\quad\\quad\\quad .6\\cr\r\n      0\\quad\\quad \\quad\\quad\\quad .2 \\quad\\quad\\quad\\quad\\quad 0 \\quad\\quad\\quad\\quad\\quad .8}\r\n$$\r\nSolve the equations\r\n$$(\\pi_0\\,\\, \\pi_1 \\,\\,\\pi_2\\,\\,\\pi_3)=(\\pi_0\\,\\, \\pi_1 \\,\\,\\pi_2\\,\\,\\pi_3){\\bf P}$$\r\n$$\\pi_0+\\pi_1+\\pi_2+\\pi_3=1$$\r\nThe solutions are $\\pi_0=7/12,\\,\\, \\pi_1=\\pi_2=1/12, \\,\\,\\pi_3=1/4$.\r\n<br>\r\nIn the long run, we have\r\n\\begin{eqnarray*}\r\n&&P(X_n=U)\r\n\\\\\r\n&=&\r\nP(X_n=U, X_{n-1}=D)+P(X_n=U, X_{n-1}=U)\r\n\\\\\r\n&=&\r\nP(Y_n=(D,U))+P(Y_n=(U,U))\r\n\\\\\r\n&=&\r\n\\pi_1 + \\pi_0\r\n\\\\\r\n&=&\r\n1/2+7/12\r\n\\\\\r\n&=&\r\n2/3.\r\n\\end{eqnarray*}", "linkability1": 3.0, "linkability2": 2.0, "linkability3": 3.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Get yourself familiar with Long run behavior.", "messagesuccess": "Great! You have mastered a typical type of questions about long run behavior.", "sensitivity": 1.0, "gussingparameter": 0.2, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [35, 37, 39], "rightproblems": [108], "wrongproblems": [109], "twinproblems": [109]}}, {"model": "mathematics.question", "pk": 138, "fields": {"code": "4.3.4", "category": 4, "problem": "A Markov chain has transition probability matrix\r\n $$ \\hskip .4in \\, 0 \\quad \\, 1 \\quad \\,\\,2 $$\r\n $${\\bf P}= \\matrix{0 \\cr 1 \\cr 2} \\pmatrix{\r\n \t0 \\quad .6 \\quad .4 \\cr\r\n \t0 \\quad .7 \\quad .3 \\cr\r\n \t0 \\quad .5 \\quad .5  }\r\n $$\r\nWhich states are transient states?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "State 0.", "choicesb": "State 1.", "choicesc": "State 2.", "choicesd": "Both State 1 and State 2.", "choicese": "Both State 2 and State 3.", "choicesf": "Both State 1 and State 3.", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "For $n>0$, starting from state 0, the $MC$ will never return to state 0 at any step $n$.\r\n$f_{00}^{(0)}=0$, $f_{00}=0$. Therefore, state 0 is transient.", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Get yourself familiar with Long run behavior.", "messagesuccess": "Great! You have mastered a typical type of questions about long run behavior.", "sensitivity": 1.0, "gussingparameter": 0.167, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 139, "fields": {"code": "4.2.1", "category": 4, "problem": "For a nonnegative random variable $Y$,\r\n$E(Y) = \\int_0^\\infty P(Y> x) dx$. \r\nIf $Y$ is nonnegative integer-valued,\r\n$E(Y) =\\sum_{k=0}^\\infty P(Y>k)$ and \r\n$E(Y) = \\sum_{k=1}^\\infty P(Y \\geq k)$\r\nHow many of the above equations are correct?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0", "choicesb": "1", "choicesc": "2", "choicesd": "3", "choicese": "none of the above", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "(This is purely a problem of basic probability.)\r\nBy definition,\r\n\\begin{eqnarray*}\r\nE(Y) &=&  \\int_0^\\infty y dP(Y \\leq y) = - \\lim_{a \\uparrow \\infty}\r\n\\int_0^a  y d P(Y > y) =\r\n  \\lim_{a \\uparrow \\infty} \\{ -aP(Y>a)+\r\n\\int_0^a    P(Y > y) dy \\}\r\n\\\\\r\n&=&\r\n\\lim_{a \\uparrow \\infty}\r\n\\int_0^a    P(a\\geq  Y > y) dy\r\n=\\int_0^\\infty   P(  Y > y) dy .\r\n\\end{eqnarray*}\r\n If $Y$ is integer valued,\r\n$$E(Y)= \\int_0^\\infty   P(  Y > y) dy\r\n= \\sum_{k=0}^\\infty \\int_k^{k+1}P(Y>y) dy\r\n= \\sum_{k=0}^\\infty P(Y> k)=\\sum_{k=1}^\\infty P(Y\\geq k).$$", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [23], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 140, "fields": {"code": "4.2.3", "category": 4, "problem": "In Example 4.6 of lecture notes, change the conditional probability of\r\nU given (D, D) to 0.2 and D given (D,D) to 0.8. Find out\r\nthe long run fraction of up days.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.25", "choicesb": "0.5", "choicesc": "0.75", "choicesd": "1", "choicese": "0", "choicesf": "0.33", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "With this change, the transition probabilities\r\nare symmetric with D and U. Therefore, our intuition is that\r\nthe long run fractions of up days and down days should be\r\nthe same, which must be 1/2.\r\nIn fact, solving the equations, you will find that\r\n$\\pi_{DD}=\\pi_{UU}$ and $\\pi_{DH}=\\pi_{HD}$\r\n(Please check.)\r\nAnd you find that the long run fraction of up days is\r\n$\\pi_{DD} +\\pi_{DH} = 1/2$.", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [46], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 141, "fields": {"code": "4.3.1", "category": 4, "problem": "A regular {\\it MC} is  ?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "irreducible", "choicesb": "aperiodic", "choicesc": "irreducible and aperiodic", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "If regular, there exits a $k>0$ such that\r\nthe $k$-step transition matrix has all positive entries.\r\ni.e, $P_{ij}^{(k)}>0$ for all states $i, j$.\r\nAs $\\sum_{l=0}^\\infty P_{il} = 1$, there must be an $l$ such\r\nthat $P_{il} > 0$. Hence,\r\n $P_{ij}^{(k+1)}= \\sum_{l=0}^\\infty P_{il} P_{lj}^{(k)} >0$,\r\n for all states $i, j$.\r\n By induction, we know, for all $n \\geq k$,  $n$-step transition matrix has\r\n all positive entries. Therefore, the {\\it MC} is\r\n irreducible and aperiodic.", "linkability1": 1.0, "linkability2": 0.0, "linkability3": 1.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [35, 52, 53], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 142, "fields": {"code": "4.3.3", "category": 4, "problem": "Consider the transition matrix  in Example 4.8. Suppose $X_0=1$.\r\n(a). Compute $f_{11}^{(n)}$ for all $n \\geq  0$.\r\n(b). Compute $E(M)$, the mean number of visits of state 1 using\r\nTry to use first step analysis to verify whether your calculation in\r\n(b) is correct.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$(1/2)^n$,  1", "choicesb": "$(1/2)^n$,  1/2", "choicesc": "$(1/2)^(n-1)$,  1", "choicesd": "$(1/2)^(n-1)$,   1/2", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "(a). $f_{11}^{(0)}=0$, $f_{11}^{(1)}=0$, and for $n\\geq 2$, the only path\r\nto come back at step $n$ for the first time is going to state 1 at step 1, staying there\r\nall the time and then coming back to state 1 at step n. Therefore, for $n\\geq 2$,\r\n\\begin{eqnarray*}\r\n f_{11}^n &=& P(X_n=1, X_{n-1}\\not=1, ..., X_1 \\not=1 |X_0=1)\r\n\\\\\r\n&=& P(X_n=1, X_{n-1}=0, ..., X_1 =0 |X_0=1)\r\n=\r\n P_{10}P_{00} \\cdots P_{00}P_{01}\r\n \\\\&=& (1/2)^n.\r\n \\end{eqnarray*}\r\n(b). Since\r\n$$f_{11} = \\sum_{n=1}^\\infty f_{11}^{(n)}= \\sum_{n=2}^\\infty 2^{-n} = 1/2.$$\r\nIt follows from (4.4) that $E(M) = f_{11}/(1-f_{11}) = 1$.\r\n\r\n Let $s_i$ be mean number of visits of state 1, beginning from state $i$ and\r\nexcluding the beginning state if it is 1. Then,\r\n$$ s_1 = 1/2 s_0 \\quad \\hbox{and} \\quad s_0 = 1/2 s_0 + 1/2 (1+s_1)$$\r\nSolving the equation, we have $s_1 = 1 $, which is the same as $E(M)$.", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [56, 57], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 143, "fields": {"code": "3.4.11", "category": 4, "problem": "A mouse initially stays in cell 0\r\nin the following cage of four cells.\r\nAt each step, it moves to one of\r\nthe two connected cells with equal chance.\r\nWhat is the\r\nthe mean number of steps the mouse passes by\r\ncell 1 before it reaches cell 3?", "problempicture1": "theall/image/diy3.4.11.png", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0", "choicesb": "1", "choicesc": "2", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Set $w_{i}$ the mean number of steps the mouse passes by cell 1 before it reaches cell 3 starting from cell $i$. Then obviously $w_{3} = 0$. By symmetry, $w_{0} = w_{2}$.\r\nMoreover,\r\n\\begin{align*}\r\nw_{0} &= \\frac{1}{2}(w_{1}+w_{3})\\\\\r\nw_{1} &= 1 + \\frac{1}{2}(w_{0} + w_{2})\\\\\r\nw_{2} &= w_{0}\\\\\r\nw_{3} &= 0\r\n\\end{align*}\r\nSolving the above equations,\r\n$w_{0} = w_{2} = 1, w_{1} = 2, w_{3} = 0$.\\\\\r\nStarting from cell 0, the mean number of steps the mouse passes by cell 1 before 3 is 1.", "linkability1": 1.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [15], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 144, "fields": {"code": "3.4.13", "category": 4, "problem": "A mouse travels randomly in between the 4 points in the\r\n  following figure. It starts from point A. From any point,  it\r\n  moves to each of the surrounding connected points with equal\r\n  probability. (For example, for point C, it moves B or D with 1/2\r\n  probability; from point B,  it moves to A, C or D with 1/3\r\n  probability; etc.) What is the probability that it reaches point D\r\n  before reaching point C?", "problempicture1": "theall/image/diy3.4.13.png", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0", "choicesb": "0.6", "choicesc": "0.8", "choicesd": "1", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "First, number A, B, C, D as 1, 2, 3, 4.\r\nSet $p_{i}$ = P(the mouse reaches point 4 before reaching 3 $|$ starting from point $i$).\r\nThen,\r\n\\begin{align*}\r\np_{1} &= P_{12}p_{2} + P_{14}p_{4} = \\frac{1}{2}(p_{2}+p_{4})\\\\\r\np_{2} &= \\frac{1}{3}(p_{1} + p_{3}+p_{4})\\\\\r\np_{3} &= 0\\\\\r\np_{4} &= 1\r\n\\end{align*}\r\nSolving the above equations,\r\n$p_{1} =\\frac{4}{5}, p_{2} = \\frac{3}{5}, p_{3} = 0, p_{4}=1$.\r\nStarting from point A, the probability that the mouse reaches point D before C is $\\frac{4}{5}$.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [9, 12], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 145, "fields": {"code": "3.4.15", "category": 4, "problem": "The following is the transition probability matrix\r\nof a Markov chain with states 0, 1 and 2:\r\n$$ \\hskip .4in 0 \\quad \\,\\,\\,\\,\\, 1 \\quad \\,\\,\\,\\,\\, 2 $$\r\n$${\\bf P} = \\matrix{0 \\cr 1 \\cr 2}\r\n\\pmatrix{.50 \\quad .50 \\quad \\,\\, 0 \\cr\r\n\t\\,\\,.30 \\quad .50 \\quad .20 \\cr\r\n\t\\,\\,\\, 0 \\quad \\, \\,\\,\\, .50 \\quad \\, .50 }\r\n$$\r\nWhat is the approximate probability of\r\n$P(X_{n-1}=X_n = X_{n+1}=0)$ for a large $n$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.3", "choicesb": "0.15", "choicesc": "0.075", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "t is easily verified that $P^{2}$ has all entries positive and therefore $P$ is regular.\r\nThen we have,\r\n\\begin{align*}\r\n\\pi_{0} &= 0.5\\pi_{0} + 0.3\\pi_{1} \\\\\r\n\\pi_{1} &= 0.5\\pi_{0} +0.5\\pi_{1} + 0.5\\pi_{2}\\\\\r\n\\pi_{2} &= 0.2\\pi_{1}+0.5\\pi_{2}\\\\\r\n\\pi_{0} &+\\pi_{1}+\\pi_{2} = 1\r\n\\end{align*}\r\nSolving the above equations, $\\pi_{0} = 0.3, \\pi_{1} = 0.5, \\pi_{2} = 0.2$\\\\\r\nTherefore, $P(X_{n-1} = 0) = 0.3$ for large n.\r\n\\begin{align*}\r\nP(X_{n-1} = X_{n} = X_{n+1} = 0) &= P(X_{n+1} = 0 | X_{n} = X_{n-1} = 0) P(X_{n} = 0 | X_{n-1} = 0)P(X_{n-1} = 0) \\\\\r\n&= P(X_{n+1} = 0 | X_{n} = 0) P(X_{n} = 0 |X_{n-1} = 0) P(X_{n-1} = 0)\\\\\r\n&= P_{00} \\times  P_{00}  \\times \\pi_{0}\\\\\r\n&= 0.075 \r\n\\end{align*}", "linkability1": 1.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [25, 37, 39], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 147, "fields": {"code": "3.4.17", "category": 4, "problem": "A Markov chain $\\{X_n\\}$ $(X_0=0)$ has transition probability matrix\r\n$$ \\hskip .4in 0 \\quad \\, 1 \\quad \\,\\,2 $$\r\n$${\\bf P}= \\matrix{0 \\cr 1 \\cr 2} \\pmatrix{\r\n\t.2  \\quad 0.4 \\quad 0.4 \\cr\r\n\t0   \\quad 0.5 \\quad 0.5 \\cr\r\n\t0   \\quad 0.5 \\quad 0.5  }\r\n$$\r\nCompute $E(T)$ where $T=\\min\\{ n\\geq 0: X_n=2, X_{n+1}=2\\}$.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "3", "choicesb": "5", "choicesc": "5.25", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Set $w_{i}$ the mean number of steps before the first two consecutive state 2 occur (including the step firstly reaches state 2 of the consecutive states starting from state 0).\r\nThen,\r\n\\begin{align*}\r\nw_{0} &= (0.2w_{0} + 0.4w_{1} + 0.4w_{2}) + 1\\\\\r\nw_{1} &= (0.5w_{1} + 0.5w_{2}) + 1\\\\\r\nw_{2} &= 0.5(1+w_{1}) + 0.5\\times 0\r\n\\end{align*}\r\nSolving the equations, $w_{0} = 5.25, w_{1}=5, w_{2} =  3$.\r\nTherefore, $E(T) = w_{0} = 5.25$", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [9, 12], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 148, "fields": {"code": "3.9.5", "category": 4, "problem": "Let $\\phi(\\cdot)$ be  the probability generating\r\nfunction of $\\xi$, the number of direct next generation of any\r\nindividual of a population that evolves as a branching process.\r\nSuppose\r\n$$\\phi(s) =\r\n.5 s^2 + bs +c, \\qquad 0 \\leq s \\leq 1.$$\r\n\r\nSpecify the conditions on the constants\r\n$b$ and $c$.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "b+c=1", "choicesb": "b+c=0.5", "choicesc": "$0\\leq b \\leq 0.5, 0 \\leq c \\leq 0.5$", "choicesd": "b and c", "choicese": "a and c", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "E", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Note that,\r\n\\begin{align*}\r\n\\phi(s) = E(s^{\\xi}) = \\Sigma_{k=0}^{\\infty}s^{k}p_{k} &= p_{0} + p_{1}s + p_{2}s^{2} + p_{3}s^{3} \\dots\\\\\r\n&= \\phi(s) = 0.5s^{2} + bs + c\r\n\\end{align*}\r\nAnd $\\Sigma_{i=0}^{\\infty} p_{i} = 1, p_{i} \\geq 0$ for all $i$.\r\nThen,  {$0.5 + b + c = 1, b\\geq 0, c\\geq 0$}\r\ni.e. {$b + c = 0.5, 0\\leq b \\leq 0.5, 0 \\leq c \\leq 0.5$}", "linkability1": 1.0, "linkability2": 1.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [20], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 149, "fields": {"code": "3.9.7", "category": 4, "problem": "Consider a branching process $\\{X_n: n \\geq 0\\}$ with $X_0={\\bf 2}$. \r\nAssume the distribution of $\\xi$, the size of next generation fathered by one individual in \r\nthe population, is \r\n$$P(\\xi=0)= 0.2, \\quad  P(\\xi=1)=0.5, \\quad P(\\xi=2)=0.2,\\quad P(\\xi=3)=0.1.$$   \r\nCompute the probability of eventual extinction of the population.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1", "choicesb": "0", "choicesc": "$\\frac{-3+\\sqrt{17}}{2}$", "choicesd": "$\\frac{-3-\\sqrt{17}}{2}$", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "For $n \\geq 1, X_{0} = 1$,\r\n\\begin{equation*}\r\nu_{n} = \\Sigma_{k=0}^{3}u_{n-1}^{k}p_{k} = 0.2 + 0.5u_{n-1} + 0.2u_{n-1}^{2} + 0.1u_{n-1}^{3}\r\n\\end{equation*}\r\nLet $n \\rightarrow \\infty$, $u_{n} = 1$ or $\\frac{-3+\\sqrt{17}}{2}$ or $\\frac{-3-\\sqrt{17}}{2}$.\r\nThen the probability of eventual extinction is $\\frac{-3+\\sqrt{17}}{2}$.", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [21], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 150, "fields": {"code": "4.1.3", "category": 4, "problem": "The following is the transition probability matrix\r\n of a Markov chain with states 0, 1 and 2:\r\n $$ \\hskip .4in 0 \\quad \\,\\,\\,\\,\\, 1 \\quad \\,\\,\\,\\,\\, 2 $$\r\n $${\\bf P} = \\matrix{0 \\cr 1 \\cr 2} \r\n \\pmatrix{.50 \\quad .50 \\quad \\,\\, 0 \\cr\r\n \t\\,\\,.25 \\quad .50 \\quad .25 \\cr\r\n \t\\,\\,\\, 0 \\quad \\, \\,\\,\\, .50 \\quad \\, .50 }\r\n $$\r\n In the long run, what is the mean fraction of\r\n time the Markov chain stays at state 0?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.25", "choicesb": "0.5", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "It is easily verified that $P^{2}$ has all entries positive and therefore $P$ is regular.\r\nThen we have, ($\\pi_{0}$ $\\pi_{1}$ $\\pi_{2}$) = ($\\pi_{0}$ $\\pi_{1}$ $\\pi_{2}$)$P$, $\\pi_{0}+ \\pi_{1}+\\pi_{2} = 1$.\r\ni.e. \r\n\\begin{align*}\r\n\\pi_{0} &= 0.5\\pi_{0} + 0.25\\pi_{1}\\\\\r\n\\pi_{1} &= 0.5 \\pi_{0} + 0.5\\pi_{1} + 0.5\\pi_{2}\\\\\r\n\\pi_{2} &= 0.25\\pi_{1} + 0.5\\pi_{2}\\\\\r\n\\pi_{0}&+ \\pi_{1}+\\pi_{2} = 1\r\n\\end{align*}\r\nSolving the equations, $\\pi_{0} = 0.25, \\pi_{1} = 0.5, \\pi_{2} = 0.25$.\r\nIn the long run, the mean fraction of time the Markov Chain stays at state 0 is 0.25.", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [39], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 151, "fields": {"code": "4.1.5", "category": 4, "problem": "A Markov chain $\\{X_n\\}$ has transition probability matrix\r\n$$ \\hskip .4in 0 \\quad \\, 1 \\quad \\,\\,2 $$\r\n$${\\bf P}= \\matrix{0 \\cr 1 \\cr 2} \\pmatrix{\r\n\t0 \\quad .6 \\quad .4 \\cr\r\n\t0 \\quad .6 \\quad .4 \\cr\r\n\t0 \\quad .3 \\quad .7  }\r\n$$\r\nIf $n$ is very large, what is the approximate distribution of $X_n$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$P(X_{n} = 1) = \\frac{3}{7}, P(X_{n} = 2) = \\frac{4}{7}$", "choicesb": "$P(X_{n} = 1) = \\frac{4}{7}, P(X_{n} = 2) = \\frac{3}{7}$", "choicesc": "$P(X_{n} = 1) = \\frac{1}{2}, P(X_{n} = 2) = \\frac{1}{2}$", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Note that from the matrix, state 0 can only be the initial state and cannot be reached from any state. Therefore $X_{n} \\neq 0$ for any $n > 0$. In the long run, state 0 is redundant and we can discard it and the new matrix is $P'$.\r\nThen we have, ($\\pi_{1}$ $\\pi_{2}$) = ($\\pi_{1}$ $\\pi_{2}$)$P'$, i.e. $\\pi_{1} + \\pi_{2} = 1$.\r\n\\begin{align*}\r\n\\pi_{1} &= 0.6\\pi_{1} + 0.3\\pi_{2}\\\\\r\n\\pi_{2} &= 0.4\\pi_{1} + 0.7\\pi_{2}\\\\\r\n\\pi_{1} &+ \\pi_{2} = 1\r\n\\end{align*}\r\nSolving the equations, {$\\pi_{1} = \\frac{3}{7}, \\pi_{2} = \\frac{4}{7}$.}\r\nIf n is very large, the approximate distribution of $X_{n}$ is, {$P(X_{n} = 1) = \\frac{3}{7}, P(X_{n} = 2) = \\frac{4}{7}$.}", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [37, 39], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 152, "fields": {"code": "4.2.5", "category": 4, "problem": "Let $X_n$ denote the whether of day $n$, which\r\n could only be sunny (S) or cloudy (C). Suppose whether of any day\r\n depends only on the previous two days' whether in the following\r\n way:    $P(X_n=S| X_{n-1}=S, X_{n-2}=S ) =0.9 $, $P(X_n=S|\r\n X_{n-1}=S, X_{n-2}=C ) = P(X_n=S| X_{n-1}=C, X_{n-2}=S ) =0.5 $\r\n and $P(X_n=S| X_{n-1}=C, X_{n-2}=C ) =0.2 $.\r\n \r\n What is an approximate value of $P(X_{n+1}=S, X_n= C,\r\n X_{n-1}=S)$ for a very large $n$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$1/19$", "choicesb": "$2/19$", "choicesc": "$10/19$", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "theall/image/diy4.2.5.png", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Set $Y_{n} = (X_{n-1}, X_{n})$. Then $\\{Y_{n}\\}$ is a MC with state space $\\{ (S,S),(S,C),(C,S),(C,C)\\}$ and the transition probability matrix is displayed as in the picture.\r\nLet 0, 1, 2 and 3 denote $(S,S),(S,C),(C,S),(C,C)$ respectively. Then,\r\n ($\\pi_{0}$ $\\pi_{1}$ $\\pi_{2}$ $\\pi_{3}$) = ($\\pi_{0}$ $\\pi_{1}$ $\\pi_{2}$ $\\pi_{3}$)$P$, $\\pi_{0}+ \\pi_{1}+\\pi_{2} +\\pi_{3} = 1$.\r\n i.e.\r\n \\begin{align*}\r\n \\pi_{0} &= 0.9\\pi_{0} + 0.5\\pi_{2}\\\\\r\n\\pi_{1} &= 0.1 \\pi_{0} + 0.5\\pi_{2}\\\\\r\n\\pi_{2} &= 0.5\\pi_{1} + 0.2\\pi_{3}\\\\\r\n\\pi_{3} &= 0.5\\pi_{1} + 0.8\\pi_{3}\\\\\r\n\\pi_{0}&+ \\pi_{1}+\\pi_{2} + \\pi_{3}= 1\r\n \\end{align*}\r\n Solving the equations, {$\\pi_{0} = \\frac{10}{19}, \\pi_{1} = \\pi_{2} = \\frac{2}{19}, \\pi_{3} = \\frac{5}{19}$.}\r\nFor large $n$,\r\n\\begin{align*}\r\nP(X_{n+1} = S, X_{n} = C, X_{n-1} = S) &= P(X_{n+1} = S | X_{n}= C, X_{n-1} = S) P(X_{n} = C, X_{n-1} = S)\\\\\r\n&= P_{12} \\times \\pi_{1} = \\frac{1}{19}\r\n\\end{align*}", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [37, 39, 42], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 153, "fields": {"code": "4.2.7", "category": 4, "problem": "A Markov chain $\\{X_n: n \\geq 0\\}$  has two states 0 and\r\n 1 and transition probability matrix\r\n $$ \\hskip .41in 0 \\quad \\,\\,\r\n \\, 1 $$\r\n $$\r\n {\\bf P}= \\matrix{0 \\cr 1} \\pmatrix{0.2  \\quad 0.8 \\cr 0.7\\quad\r\n \t0.3}\r\n $$\r\n What is the  long run fraction of transitions from\r\n state 0 to state 1?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$28/75$", "choicesb": "$7/15$", "choicesc": "$32/75$", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "We have, ($\\pi_{0}$ $\\pi_{1}$) = ($\\pi_{0}$ $\\pi_{1}$)$P$, $\\pi_{0} + \\pi_{1} = 1$.\r\ni.e.\r\n\\begin{align*}\r\n\\pi_{0} &= 0.2\\pi_{0} + 0.7\\pi_{1}\\\\\r\n\\pi_{1} &= 0.8\\pi_{0} + 0.3\\pi_{1}\\\\\r\n\\pi_{0} &+ \\pi_{1} = 1\r\n\\end{align*}\r\n Solving the equations, {$\\pi_{0} = \\frac{7}{15}, \\pi_{1}= \\frac{8}{15}, \\pi_{0} \\times P_{01} = \\frac{7}{15} \\times 0.8 = \\frac{28}{75}$}\r\n The long run fraction of transitions from state 0 to 1 is $\\frac{28}{75}$.", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 154, "fields": {"code": "4.3.5", "category": 4, "problem": "A Markov chain has transition probability matrix\r\n $$ \\hskip .4in 0 \\quad \\, 1 \\quad \\,\\,2 $$\r\n $${\\bf P}= \\matrix{0 \\cr 1 \\cr 2} \\pmatrix{\r\n \t.5 \\quad .2 \\quad .3 \\cr 0 \\quad .5 \\quad .5 \\cr 0 \\quad .5 \\quad\r\n \t.5  }\r\n $$\r\n Point out which of the three states are recurrent and\r\n which are transient.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "All are recurrent.", "choicesb": "Only state 1 and 2 are recurrent and state 0 is transient.", "choicesc": "Only state 0 is recurrent and state 1 and 2 are transient.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Note that $\\Sigma_{n=1}^{\\infty} P_{00}^{(n)} < 1$, thus state 0 is transient.\r\nNote that state 1 communicates with state 2 and the MC will return to visit state 1 or 2 infinite number of times.\r\nThus State 1 and 2 are recurrent.", "linkability1": 2.0, "linkability2": 0.0, "linkability3": 1.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [56, 58], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 187, "fields": {"code": "5.1.1", "category": 4, "problem": "Can we use the moment generating function to derive the mean and variance of\r\na Poisson random variable?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes.", "choicesb": "No.", "choicesc": "Perhaps.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Suppose $X\\sim {\\cal P}(\\lambda)$. Its moment generating function\r\nis\r\n$$\\psi(t)\\equiv E(e^{tX})= \\sum_{n=0}^\\infty e^{tn} \\lambda^n e^{-\\lambda}/n! = e^{-\\lambda}\r\n\\sum_{n=0}^\\infty (e^{t} \\lambda)^n /n!\r\n=e^{-\\lambda + \\lambda e^t}.$$\r\nThen,\r\n$$ E(X)= {\\partial \\over \\partial t} \\psi(t)\\Bigl|_{t=0}\r\n=\\lambda, \\quad \\hbox{and} \\quad\r\nE(X^2)= {\\partial^2 \\over \\partial t^2}\\psi(t)\\Bigl|_{t=0}\r\n=\\lambda + \\lambda^2$$\r\nTherefore, $$var(X)= E(X^2) - (E(X))^2 = \\lambda + \\lambda^2 - \\lambda^2 = \\lambda.$$", "linkability1": 1.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Get yourself familiar with the probability of Poisson random variable and moment generating function!", "messagesuccess": "Great! You have mastered the probability of Poisson random variable and an application of moment generating function!", "sensitivity": 1.0, "gussingparameter": 0.333, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [62, 63, 64], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 188, "fields": {"code": "5.2.1", "category": 4, "problem": "Can we use binomial approximation to interpret the fact that the mean and\r\nvariance are equal for a Poisson random variable?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes.", "choicesb": "No.", "choicesc": "Perhaps.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "A Poisson distribution ${\\cal P}(\\lambda)$ is the limit\r\nof binomial distribution $Bin(n, \\lambda/n)$ as $n \\to \\infty$.\r\n$$\\qquad {\\textcolor[rgb]{1,0,0}{\\hbox{  (Binomial approximation)}} }$$\r\nFor binomial distribution, the mean is $n \\lambda/n = \\lambda$ and\r\nthe variance is $n (\\lambda/n)(1-\\lambda/n) \\to \\lambda$ as\r\n$n \\to \\infty$. It is therefore understandable that\r\n the Poisson distribution has equal mean and variance.\r\n (This is not a rigorous mathematical proof though.)", "linkability1": 1.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. It is related to Binomial approximation.", "messagesuccess": "Great! You have mastered the concept of Binomial approximation!", "sensitivity": 1.0, "gussingparameter": 0.333, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [64, 67], "rightproblems": [], "wrongproblems": [], "twinproblems": [189]}}, {"model": "mathematics.question", "pk": 189, "fields": {"code": "5.3.2", "category": 4, "problem": "Can you prove Proposition 5.2 (as shown in the following)?\r\n<br>\r\n<br>\r\nProposition 5.2 [Extended binomial approximation] \r\n<br>\r\nSuppose (a). $\\xi_{n, i}$ takes\r\nvalue $1 $ and $0$ with probability $p_{n,i} $ and $1-p_{n,i}$; (b).\r\n$\\xi_{n,1} , ...,\\xi_{n, n}$ are independent; (c). $\\max\\{p_{n, i}: 1 \\leq i \\leq n\\}\r\n\\to 0$ and $\\sum_{i=1}^n p_{n,i} \\to \\lambda$\r\nas $n \\to \\infty$ where $0< \\lambda < \\infty$.\r\nThen, as $n \\to \\infty$,\r\n$$\\sum_{i=1}^n \\xi_{n,i}  \\to {\\cal P}(\\lambda) \\qquad \\hbox{in distribution.}$$", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes.", "choicesb": "No.", "choicesc": "Perhaps.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "As with the famous central limit theorem,\r\nthe commonly adopted proof is via $\\textcolor[rgb]{1,0,0}{\\hbox{characteristic function}}$, which\r\nis the same as moment generating function except now $t$ is replaced\r\nby $\\textcolor[rgb]{1,0,0}{\\hbox{$s=\\sqrt{-1}t$}}$, where $\\sqrt{-1}$ is complex but $t$ is real.\r\nThe characteristic function of\r\n${\\cal P}(\\lambda)$ is $ e^{-\\lambda + e^s \\lambda}$.\r\nObserve that\r\n$$ E( e^{s \\sum_{j=1}^n \\xi_{n,j} }) =\r\n\\prod_{j=1}^n E(e^{s \\xi_{n,j}})= \\prod_{j=1}^n (1-p_{n, j} +p_{n,j} e^s)\r\n= e^{\\sum_{j=1}^n \\log(1+ p_{n, j}(e^s-1))}.$$\r\nFor small $x$,  $|\\log(1+x)- x | \\leq |x|^2$.\r\nHence, as $n \\to \\infty$, \r\n$$ E( e^{s \\sum_{j=1}^n \\xi_{n,j} }) =  e^{ (e^s-1) \\sum_{j=1}^n p_{n, j} }\r\n+o(1) = e^{ (e^s-1)\\lambda} +o(1).$$\r\nIn summary, the characteristic function of $\\sum_{j=1}^n \\xi_{n, j}$\r\ntends to that of ${\\cal P}(\\lambda)$. Then the distribution\r\nof $\\sum_{j=1}^n \\xi_{n, j}$ also converges to that\r\n${\\cal P} (\\lambda)$. \r\n<br>\r\n(This is citing a theorem in probability theory: a sequence\r\nof distributions converges to a limit distribution if and only if\r\ntheir characteristic functions converge to that of the limit distribution.)", "linkability1": 3.0, "linkability2": 3.0, "linkability3": 3.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. It is related to Binomial approximation and characteristic function.", "messagesuccess": "Great! You have mastered Binomial approximation and the application of characteristic function!", "sensitivity": 1.0, "gussingparameter": 0.333, "difficulty": 5, "calculateddifficulty": null, "linkneuron": [65, 67], "rightproblems": [], "wrongproblems": [], "twinproblems": [188]}}, {"model": "mathematics.question", "pk": 190, "fields": {"code": "6.1.1", "category": 4, "problem": "Does Postulate 1 (as shown in the following) for Poisson Process imply independent increment?\r\n<br>\r\n<br>\r\n$Postulate 1$. (via infinitesimal probabilities)\r\n<br>\r\n(i) $P(X(t+h)-X(t) =1 | X(t) = k) = \\lambda h + o(h)$\r\n<br>\r\n(ii) $P(X(t+h)-X(t) =0 | X(t) = k) = 1- \\lambda h + o(h)$\r\n<br>\r\n(iii) $X(0)=0$.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes.", "choicesb": "No.", "choicesc": "I have no idea.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Yes, but it must be coupled with $\\textcolor[rgb]{1,0,0}{\\hbox{the requirement that it be a $MC$}}$.\r\nNote that, with the Markovian property,  $Postulate 1$ implies\r\n\\begin{eqnarray*}\r\n&& P(X(t+\\Delta t) -X(t)=1|X(s) , 0 \\leq s \\leq t)=\\lambda h + o(h),\\\\\r\n\\quad \\hbox{and}\r\n\\quad && P(X(t+\\Delta t) -X(t)=0|X(s) , 0 \\leq s \\leq t)=1-\\lambda h +o(h).\r\n\\end{eqnarray*}\r\nDivide the interval $(s, t]$ into\r\nsubintervals $(s, s+h], (s+h, s+2h], ..., (s+(n-1)h, s+nh]$ where\r\n$h=(t-s)/n$ and $n$ is a {\\it large } integer.\r\nFor notational simplicity, let $Y_l= X(s+lh)-X(s+(l-1)h)$.\r\nBy Postulate 1,\r\nwe have, for any $0 \\leq s< t <\\infty$ and any nonnegative integers $i$ and $ j $,\r\n\\begin{eqnarray*}\r\n&& P(X(t)-X(s)=j| X(a), 0\\leq  a \\leq t) \\\\\r\n&=&\r\n \\sum_{0 \\leq k_1 <\\cdots < k_j \\leq n} k_1\r\n P( Y_1=0,...,Y_{k_1-1}=0, Y_{k_1}=1, Y_{k_1+1}=0, ...,\r\n Y_{k_2-1}=0, Y_{k_2}=1, Y_{k_2+1}=0, \\\\\r\n && \\qquad ..., Y_{k_j-1}=0, Y_{k_j=1},\r\n Y_{k_j+1}=0,.., Y_n=0|X(a), 0\\leq  a \\leq t) +o(h)\r\n \\\\\r\n &=& \\sum_{0 \\leq k_1 <\\cdots < k_j \\leq n} (1-\\lambda h)\\cdots (1-\\lambda h) \\lambda h\r\n (1-\\lambda h) \\cdots\r\n \\\\\r\n && \\qquad (1-\\lambda h) \\lambda h\r\n (1-\\lambda h) \\cdots (1-\\lambda h) \\lambda h (1-\\lambda h) \\cdots (1-\\lambda h) +o(h)\r\n \\\\\r\n &=& {n \\choose j}\r\n (1-\\lambda h)^{n-j} (\\lambda h)^j +o(h)\r\n \\\\\r\n &\\to & (\\lambda (t-s))^j e^{- \\lambda(t-s)}/j! \\qquad \\hbox{as $n \\to \\infty$}\r\n .\r\n\\end{eqnarray*}\r\nThe conditional distribution of $X(t)-X(s)$ is independent of\r\n$X(a), 0 \\leq a \\leq s$, and, therefore, is independent of any\r\nincrements on interval $[0, s]$.", "linkability1": 3.0, "linkability2": 3.0, "linkability3": 3.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, you may try it again.", "messagesuccess": "Great! You have mastered Postulate 1 of Poisson Process!", "sensitivity": 1.0, "gussingparameter": 0.333, "difficulty": 5, "calculateddifficulty": null, "linkneuron": [95], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 191, "fields": {"code": "6.1.3", "category": 4, "problem": "Does $P_n(t), n \\geq 1$ satisfying (6.1) also satisfy the\r\ndifferential equation (6.3)?\r\n\r\n$$ P_n(t) = \\lambda_{n-1} e^{-\\lambda_n t} \\int_0^t e^{\\lambda_n\r\ns} P_{n-1}(s) ds  \\qquad\\qquad \\qquad  (6.1)$$\r\n$$\\cases{ P_0'(t) = -\\lambda_0 P_0(t) \\cr\r\nP_n'(t) = - \\lambda_n P_n(t) + \\lambda_{n-1} P_{n-1}(t)  \\qquad n\r\n\\geq 1 } \\qquad (6.3)$$", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes.", "choicesb": "No.", "choicesc": "I have no idea.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "$\\textcolor[rgb]{1,0,0}{\\hbox{Rewrite (6.1)}}$ as\r\n$$    e^{\\lambda_n t} P_n (t)  = \\lambda_{n-1} \\int_0^t\r\ne^{\\lambda_n s} P_{n-1}(s) d s , \\qquad n \\geq 1, $$\r\nand $\\textcolor[rgb]{1,0,0}{\\hbox{differentiate  both sides}}$. It follows that\r\n$$\\lambda_{n }   e^{ \\lambda_n t}P_n(t) +  e^{ \\lambda_n t}P'_n(t)\r\n= \\lambda_{n-1} e^{\\lambda_n t} P_{n-1}(t)\r\n$$\r\nwhich is the same as   (6.3) for $n \\geq 1$.", "linkability1": 1.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Here's a hint: you may first rewrite (6.1).", "messagesuccess": "Great! Now you may have a better understanding of Postulate 2 of Pure birth process!", "sensitivity": 1.0, "gussingparameter": 0.333, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [89, 110], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 192, "fields": {"code": "6.1.5", "category": 4, "problem": "In Example 6.1, can you show $\\lambda_k = (N-k)k \\alpha/(N-1)$ for $k=1,..., N$?\r\n<br>\r\n(Please refer to $\\backslash$Lecture notes$\\backslash$Chapter6 Part1)", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes.", "choicesb": "No.", "choicesc": "I have no idea.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "$X(t)=k$ means there are $k$ PPs and    $N-k$ normal people\r\non earth at time $t$.\r\nAny one ``lethal\" contact by a PP is to a normal people with chance\r\n$(N-k)/(N-1)$ and to PP with chance\r\n$(k-1)/(N-1)$. Here $N-1$ refers to the size of the entire earth population except\r\nfor the PP making the contact.\r\nTherefore an increment of the size of PPs by one due to a\r\nparticular PP within time $(t, t+\\Delta t)$ has probability\r\n$(N-k)/(N-1) \\alpha \\Delta t + o(\\Delta t)$. Write\r\n\\begin{eqnarray*}\r\n&& P(X(t+\\Delta t)-X(t)=1 | X(t)=k)\r\n\\\\\r\n&=& \\sum_{j=1}^k P(\\hbox{ One additional PP within\r\n$(t, t+\\Delta t]$  due to\r\nthe j-th existing PP } |X(t)=k) \\qquad \\textcolor[rgb]{1,0,0}{\\hbox{Slicely universe}}\r\n\\\\\r\n&=&  k (N-k)/(N-1) \\alpha \\Delta t  + o(\\Delta t).\r\n\\end{eqnarray*}\r\nHence, $\\lambda_k = k (N-k)\\alpha/(N-1)$.", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try it again.", "messagesuccess": "Great! You should have a better understanding of Pure birth process!", "sensitivity": 1.0, "gussingparameter": 0.333, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [86, 88], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 193, "fields": {"code": "6.2.2", "category": 4, "problem": "Suppose\r\n$\\{X(t): t \\in [0,\\infty)\\}$ is pure birth process with\r\n$X(0)=0$ and birth rates $\\lambda_0, \\lambda_1, ....$. $T$ is an\r\nexponentially distributed random variable with parameter\r\n$\\theta$, independent of the process $X(\\cdot)$. For any integer $K \\geq 1$,\r\ndescribe $P(X(T) \\geq K)$ by $K, i, \\lambda_i$, and $\\theta$.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\prod_{j=0}^{K}{ \\lambda_j \\over \\lambda_j + \\theta}$", "choicesb": "$\\prod_{j=0}^{K}{ \\lambda_j \\over \\lambda_j - \\theta}$", "choicesc": "$\\prod_{j=0}^{K-1}{ \\lambda_j \\over \\lambda_j + \\theta}$", "choicesd": "$\\prod_{j=0}^{K-1}{ \\lambda_j \\over \\lambda_j - \\theta}$", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "\\begin{eqnarray*}\r\n&& P(X(T)\\geq K) = P(W_K \\leq T)\r\n\\\\&=&  \\int_0^\\infty P(T \\geq s|W_K=s) dP(W_k \\leq  s)\r\n= \\int_0^\\infty e^{-\\theta s } dP(W_k \\leq s)\r\n= E(e^{-\\theta W_K})\r\n\\\\&=&  E(e^{-\\theta (S_0+\\cdots+ S_{K-1})})\r\n=\\prod_{j=0}^{K-1} E(e^{-\\theta S_j})\r\n= \\prod_{j=0}^{K-1} \\int_0^\\infty e ^{-\\theta x} \\lambda_j e^{-\\lambda_j x} dx\r\n\\\\&=& \\prod_{j=0}^{K-1}{ \\lambda_j \\over \\lambda_j + \\theta}\r\n\\end{eqnarray*}", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "Please refer to Prob 6.2.1.", "messagefailure": "Oops, try it again. Hint: Use the concept of waiting time to convert the probability.", "messagesuccess": "Great! You should have a better understanding of Pure death process!", "sensitivity": 1.0, "gussingparameter": 0.2, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [96, 100, 101], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 194, "fields": {"code": "6.2.4", "category": 4, "problem": "In Expl 6.1.4, assume the life times of the $N$ Spartan soldiers are\r\niid. Let $X(t)$ be the number of surviving soldiers\r\nby time $t$, which statement below is correct?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Even if $X(t)$ is a pure death process, the common distribution of the soldier's life time is not exponential.", "choicesb": "$X(t)$ is a pure death process if and only if their common distribution is exponential.", "choicesc": "Even if the common distribution of the soldier's life time is exponential, $X(t)$ is not a pure death process.", "choicesd": "None of the above is correct.", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "If they are iid exponential, then $\\{X(t)\\}$ is known to be, not only\r\npure death, a linear pure death process. So the $\\textcolor[rgb]{1,0,0}{\\hbox{sufficiency}}$ is already shown in\r\nthe lecture notes. (Please refer to Expl 6.1.4.)\r\n\r\nLet $Y_1,..., Y_N$ be the life times of the $N$ Spartans which are iid with common\r\ncdf $F$.\r\n  If $\\{X(t)\\}$\r\nis pure death, then $S_0$ is exponential with parameter $\\mu_N$.\r\nThen, for any $t>0$,\r\n$$(1-F(t))^N = P(Y_1>t, ..., Y_N> t) = P(S_0>t)= e^{-\\mu_N t}.$$\r\nHence, $1-F(t)= e^{-\\mu_N t/N}$ is\r\nan exponential function in $t$, meaning that\r\n$Y_i$ follows exponential distribution with parameter\r\n$\\mu_N/N$. The $\\textcolor[rgb]{1,0,0}{\\hbox{necessity}}$ is proved.", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try it again. You may review Expl 6.1.4.", "messagesuccess": "Well done! You have mastered the concept of Linear pure death process.", "sensitivity": 1.0, "gussingparameter": 0.25, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [96, 102, 105], "rightproblems": [], "wrongproblems": [], "twinproblems": [183]}}]
export default data;