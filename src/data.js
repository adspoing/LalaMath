const data = 
[{"model": "mathematics.question", "pk": 2, "fields": {"code": "3.1.1", "category": 4, "problem": "Bunny rabbit has three dens A, B and C. It likes A better than B and\r\nC. If it's in B or C on any night, it will always take chance 0.9 to go to A\r\n and chance $0.1$ to go to the other den for the following night. Once it reaches\r\n A, it will stay there for two nights and the third night will be in B or C\r\n with equal chance 1/2. Let $X_n$ be the den Bunny stays for night $n$.\r\n What is the state space of the $\\{X_n\\}$?\r\n Is $\\{X_n\\}$ a $MC$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "The state space of $\\{X_n\\}$ is $\\{\\hbox{Den A, Den B, Den C}\\}$.\r\n$\\{X_n\\}$ is a $MC$.", "choicesb": "The state space of $\\{X_n\\}$ is $\\{\\hbox{Den A, Den B, Den C}\\}$.\r\n$\\{X_n\\}$ is not a $MC$.", "choicesc": "None of the above is correct.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The state space of $\\{X_n\\}$ is $\\{\\hbox{Den A, Den B, Den C}\\}$.\r\n$\\{X_n\\}$ is not a $MC$,\r\nbecause, for example,\r\n $$P(X_{n+1}=A|X_{n}=A, X_{n-1}=B)=1 \\not=0=P(X_{n+1}=A|X_n=A, X_{n-1}=A).$$\r\n$$\\qquad {\\textcolor[rgb]{1,0,0}{\\hbox{  (MC definition)}} }$$", "linkability1": 3.0, "linkability2": 0.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Alas! Please check the definition of Markov chain.", "messagesuccess": "Congratulations! You have mastered the definition of Markov chains.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [3], "rightproblems": [32], "wrongproblems": [31, 124], "twinproblems": [124]}}, {"model": "mathematics.question", "pk": 3, "fields": {"code": "3.1.3", "category": 4, "problem": "Suppose $X_n, n= 0, 1, 2, ...$ is\r\na discrete time $MC$. Let\r\n$0 \\leq n_0< n_1< n_2<   ...$ be a subsequence of the nonnegative integers\r\nand $Y_k = X_{n_k}$. Is $\\{Y_k: k =0,1, 2,...,\\}$\r\na $MC$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\{Y_k: k =0,1, 2,...,\\}$ is a $MC$.", "choicesb": "$\\{Y_k: k =0,1, 2,...,\\}$ is not a $MC$.", "choicesc": "None of the above is correct.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The definition of MC $\\{X_n\\}$, namely the past and future are\r\n independent given any fixed state of the present,   can be expressed as\r\n \\begin{eqnarray*}\r\n && P(X_{n+1}\\in A_1, X_{n+2} \\in A_2, ..., X_{n-1} \\in B_{n-1},   ..., X_0\r\n \\in B_0 | X_n = i)\r\n \\\\\r\n &=&  P(X_{n+1}\\in A_1, X_{n+2} \\in A_2, ...,|X_n=i)\r\n   \\times\r\n P(X_{n-1} \\in B_{n-1},  ..., X_0\r\n \\in B_0 | X_n = i)   \\qquad  (1)\r\n \\end{eqnarray*}\r\n$$\\qquad {\\textcolor[rgb]{1,0,0}{\\hbox{(MC definition and Conditional independence)}} }$$\r\n \r\n\r\n for any time $n$,\r\n any state $i$ and $any$ subsets of the state space, $A_1, A_2, ....,B_0, B_1, B_2, ...$; see the remark.\r\n Then,\r\n \\begin{eqnarray*}\r\n && P(Y_{k+1}\\in A_1, Y_{k+2} \\in A_2, ..., \\,\\,\\, Y_{k-1} \\in B_{k-1},  ..., Y_0\r\n \\in B_0 | Y_k=i  )\r\n \\\\\r\n &=&\r\n P(X_{n_{k+1}}\\in A_1, X_{n_{k+2}} \\in A_2, ...,\\,\\,\\, X_{n_{k-1}} \\in B_{k-1},   ..., X_{n_0}\r\n \\in B_0 | X_{n_k}=i  )\r\n \\\\\r\n && \\qquad \\hbox{(by definition of $Y_n$)}\r\n \\\\\r\n &=&\r\n P(X_{n_{k+1}}\\in A_1, X_{n_{k+2}} \\in A_2, ..., |X_{n_k}=i)\r\n  \\times P(X_{n_{k-1}} \\in B_{k-1},   ..., X_{n_0}\r\n \\in B_0 | X_{n_k}=i  )\r\n\\\\\r\n&& \\quad \\hbox{(by applying (1))}\r\n \\\\\r\n &=&\r\n P(Y_{k+1}\\in A_1, Y_{k+2} \\in A_2, ...,|Y_k=i)\r\n P(Y_{k-1} \\in B_{n-1}, Y_{k-2} \\in B_{n-2}, ..., Y_0\r\n \\in B_0 | Y_k = i).\r\n\\\\\r\n && \\quad \\hbox{(by definition of $Y_n$)}\r\n \\end{eqnarray*}\r\n \r\n<br>\r\n Remark. \r\n<br>\r\nThe mathematical expression that we often use as definition of $MC$ is\r\n $$P(X_{n+1}=j_1|X_n=i, X_{n-1}=i_{n-1}, ..., X_0=i_0)=P(X_{n+1}=j_1|X_n=i),\\qquad(2)$$\r\n for all time $n$, states $j_1,i, i_0,i_1,...,$\r\n which is a relatively simple version, but is actually equivalent to\r\n the seemingly more\r\n general version in (1). The following is to show the equivalency.\r\n Observe that (2) implies\r\n \\begin{eqnarray*}\r\n &&\r\n P(X_{n+k}=j_k, ..., X_{n+1}=j_1 | X_n=i,  X_{n-1}=i_{n-1}, ..., X_0=i_0)\r\n \\\\\r\n &=& P_{i, j_1} P_{j_1, j_2} \\cdots P_{j_{k-1}, j_k}\r\n \\\\\r\n &=&\r\n P(X_{n+k}=j_k, ..., X_{n+1}=j_1 | X_n=i),\r\n \\end{eqnarray*}\r\n which, by $DIY 3.1.2$, is equivalent to\r\n \\begin{eqnarray*}\r\n && P(X_{n+k}=j_k, ..., X_{n+1}=j_1,\r\n \\,\\,   X_{n-1}=i_{n-1}, ..., X_0=i_0 |X_n=i)\r\n \\\\\r\n &=&\r\n P(\\{X_{n+k}=j_k, ..., X_{n+1}=j_1\\} \\cap \\{   X_{n-1}=i_{n-1}, ..., X_0=i_0\\} |X_n=i)\r\n \\\\\r\n &=&\r\n P(X_{n+k}=j_k, ..., X_{n+1}=j_1 | X_n=i)\r\n P(X_{n-1}=i_{n-1}, ..., X_0=i_0 |X_n=i).\r\n \\end{eqnarray*}\r\n Then,\r\n \\begin{eqnarray*}\r\n &&\r\n P(X_{n+k}\\in A_k, ..., X_{n+1}\\in A_1,\r\n \\,\\,   X_{n-1}\\in B_{n-1}, ..., X_0\\in B_0 |X_n=i)\r\n \\\\\r\n &=&\r\n \\sum_{j_k \\in A_k} \\cdots \\sum_{j_1 \\in A_1} \\sum_{i_{n-1}\\in B_{n-1}} \\cdots\r\n \\sum_{i_0 \\in B_0}\r\n P(X_{n+k}=j_k, ..., X_{n+1}=j_1, \\,\\,\\,   X_{n-1}=i_{n-1}, ..., X_0=i_0|X_n=i)\r\n \\\\\r\n &=&\r\n \\sum_{j_k \\in A_k} \\cdots \\sum_{j_1 \\in A_1} \\sum_{i_{n-1}\\in B_{n-1}} \\cdots\r\n \\sum_{i_0 \\in B_0}\r\n \\\\\r\n && \\qquad\r\n P(X_{n+k}=j_k, ..., X_{n+1}=j_1 | X_n=i)\r\n P(X_{n-1}=i_{n-1}, ..., X_0=i_0 |X_n=i)\r\n \\\\\r\n &=&\r\n P(X_{n+k}\\in A_k, ..., X_{n+1}\\in A_1 |X_n=i)\r\n P(  X_{n-1}=B_{n-1}, ..., X_0=B_0 |X_n=i)\r\n .\r\n \\end{eqnarray*}\r\n This equality fits precisely the statement of the conditional independence of\r\n the future and the past given a fixed state of the present.", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Alas! Please check the definition of Markov chain and conditional independence.", "messagesuccess": "Congratulations! You have mastered the definition of Markov chains\r\nand conditional independence.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [3, 7], "rightproblems": [49], "wrongproblems": [48], "twinproblems": [49]}}, {"model": "mathematics.question", "pk": 4, "fields": {"code": "3.2.8", "category": 1, "problem": "(Mickey in Maze} continued from Example 3.2). \r\n(i) Compute the probability Mickey goes to cell 2 before it reaches cell 6, beginning\r\nfrom cell 1.\r\n<br>\r\n(ii). Compute the mean number of steps to reach cells 2 or 6, beginning\r\nfrom cell 4.\r\n<br>\r\n(iii). Compute the mean number of times Mickey visits cell 2 before reaching cell\r\n6, starting from cell 4.", "problempicture1": "theall/image/Expl3.2.8-1.png", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "(i) Set\r\n$$p_i = P(\\hbox{Mickey reaches cell 2 before reaching 6 } | \\hbox{ starting from i}).\r\n$$\r\nThen, for example,\r\n\\begin{eqnarray*}\r\np_0 &=& P_{01}p_1 + P_{03} p_3 = 1/2 p_1 + 1/2 p_3  \\\\\r\np_1 &=& 1/3p_0  + 1/3 p_4 + 1/3 p_2 = 1/3(p_0 +p_4) + 1/3 \\\\\r\np_2 &=& 1  \\\\\r\np_3 &=& 1/3 p_0 + 1/3 p_4 + 1/3p_6 = 1/3(p_0 +p_4) \\\\\r\np_4 &=& 1/4 p_1 + 1/4 p_3 + 1/4p_5 + 1/3p_7 \\\\\r\np_5 &=& 1/3 p_2 + 1/3 p_4 + 1/3p_8 = 1/3(p_0 +p_8) +1/3 \\\\\r\np_6 &=& 0 \\\\\r\np_7 &=& 1/3 p_4 + 1/3 p_6 + 1/3p_8 = 1/3(p_4 +p_8) \\\\\r\np_8 &=& 1/2 p_5 + 1/2 p_7\r\n\\end{eqnarray*}\r\nThe above linear equations can be solved for the answer\r\n$$ p_0=p_8=1/2, \\quad p_1=p_5 = 2/3, \\quad p_3=p_7=1/3 \\quad\r\np_4=1/2.$$\r\n\r\nIn fact, a short-cut to solving the eight equations is, by the\r\nsymmetry of the maze, to realize that $p_0=p_8$, $p_1=p_5$, $p_3=p_7$.\r\nTogether with the fact that $p_2=1$ and $p_6=0$, the number of\r\nequations can be immediately reduced to four about $p_0, p_1, p_3$\r\nand $p_4$.\r\n\r\n(ii). Set $w_i$ the mean number of steps to reach 2 or 6, starting from\r\ncell $i$. Then, obviously, $w_2=w_6=0$. By symmetry,\r\n$w_0=w_8$, $w_1=w_3=w_5=w_7$.\r\nMoreover,\r\n\\begin{eqnarray*}\r\nw_0 &=& 1 + 1/2(w_1 + w_3) = 1 + w_1 \\\\\r\nw_1 &=& 1+ 1/3(w_0+w_2+w_4)= 1 + 1/3(w_0 + w_4) \\\\\r\nw_4 &=& 1+ 1/4(w_1+w_3+w_5+w_7) = 1+w_1\r\n\\end{eqnarray*}\r\nSolving the three equations,\r\n$$w_0=w_4=w_8=6\\quad \\hbox{and} \\quad w_1=w_3=w_5=w_7=5.$$\r\n\r\n(iii).\r\nSet $w_i$ the mean number of visits of cell 2 before reaching cell 6, starting\r\nfrom cell $i$ (including the starting state).\r\n  Then, obviously, $w_6=0$. By symmetry,\r\n$w_0=w_8$, $w_1=w_5$ and $w_3= w_7$.\r\nMoreover,\r\n\\begin{eqnarray*}\r\nw_0 &=&  1/2(w_1 + w_3)   \\\\\r\nw_1 &=&  1/3(w_0+w_2+w_4)  \\\\\r\nw_2 &=& 1 + 1/2(w_1 + w_5) = 1+ w_1 \\\\\r\nw_3 &=& 1/3(w_0 + w_4 + w_6) = 1/3(w_0 +w_4) \\\\\r\nw_4 &=&  1/4(w_1+w_3+w_5+w_7) = 1/2(w_1+w_3)\r\n\\end{eqnarray*}\r\nSolving the three equations,\r\n$$w_0=w_4=w_8=3/2\\quad w_1=w_5= 2 \\quad w_2= 3 \\quad \\hbox{and} \\quad  w_3=w_7=1.$$\r\nStarting from cell 4,  Mickey's  mean number of visits of cell 2 before reaching cell 6\r\nis $w_4=3/2$.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Alas! Please return to the graph and review related knowledge.", "messagesuccess": "Congratulations! You have mastered the First Step Analysis method.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [5, 60], "wrongproblems": [8, 54, 130], "twinproblems": [8, 54, 130]}}, {"model": "mathematics.question", "pk": 5, "fields": {"code": "3.2.9", "category": 1, "problem": "( A Fecundity Model) \r\nThe life span of  women is divided into\r\nfive-year periods. In each period, a woman assumes\r\none of the following six states:\r\n$E_0$: prepuberty; $E_1$: single; $E_2$: married;\r\n$E_3$: divorced; $E_4$: widowed; $E_5$: out-of-population.\r\nAssume the transit process is a {MC}  with transition matrix\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\, E_0 \\quad E_1 \\quad E_2\\quad E_3\\quad E_4\\quad E_5\\\\\r\n {\\bf P}&=& \\matrix{E_0 \\cr E_1 \\cr E_2 \\cr E_3 \\cr E_4\\cr E_5 }\r\n  \\pmatrix{\r\n \\,\\,\\,\\,\\, &  0.9  &     &       &    & 0.1        \\cr\r\n &  0.5   &  0.4  &      &    &   0.1      \\cr\r\n  &       &  0.6   & 0.2 & 0.1  &   0.1    \\cr\r\n &    & 0.4   &  0.5  &     &     0.1     \\cr\r\n &     & 0.4  &      &  0.5   &  0.1     \\cr\r\n  &     &     &      &     &    1\r\n}\r\n\\end{eqnarray*}\r\nWhat is the mean marriage periods of a woman in her life time?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Set $w_i$ the mean periods in marriage in the remaining life time,\r\nstarting from state $E_i$ for the current period (including\r\nthe current period.)\r\nThen,\r\n\\begin{eqnarray*}\r\nw_0 &=& 0.9 w_1 +0.1 w_5 \\\\\r\nw_1 &=& 0.5w_1 + 0.4 w_2 + 0.1w_5 \\\\\r\nw_2 &=& 0.6w_2 + 0.2 w_3 + 0.1 w_4 + 0.1w_5 + 1 \\\\\r\nw_3 &=& 0.4 w_2  + 0.5 w_3 + 0.1 w_5\\\\\r\nw_4 &=& 0.4 w_2  + 0.5 w_4 + 0.1 w_5\\\\\r\nw_5 &=& 0\r\n\\end{eqnarray*}\r\nSolving the equations, we have\r\n$$w_0 = 4.5, \\quad w_1=5 \\quad w_2=5.25 \\quad \\hbox{and} \\quad w_3=w_4=5.$$\r\nThe mean marriage periods of a woman in her life time is $w_0 = 4.5$.\r\nIn other words, the mean marriage time of a woman in her life time\r\nis $4.5\\times 5 =22.5$ years.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Alas! Please return to the graph and review related knowledge.", "messagesuccess": "Congratulations! You have mastered the First Step Analysis method.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [4, 54], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 6, "fields": {"code": "3.3.12", "category": 1, "problem": "( The family tree of Confucius ) \r\n The following diagram shows the first few generations of the family tree\r\n of Confucius.", "problempicture1": "theall/image/Expl3.3.12-1.png", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Note that $\\xi_i^{(n)} $ denotes the number of sons fathered by\r\nthe $i$-th member of the $n$-generation of Kong Zi(Confucius),\r\nand $X_n$ is the number of (male) descendants of Kong Zi at\r\nthe $n$-th generation.\r\n\r\nAllegedly, the 80-th generation of Kong Zi  is around\r\n1,300,000. If the process is indeed a branching process. Then\r\na reasonable estimate of $\\mu$, the mean number of sons produced by\r\nany male descendent of Kong Zi, can be obtained by\r\n$$ \\hat \\mu^{80} = 1,300,000,$$\r\nbased on the formula that $E(X_n) = \\mu^n$, to be shown in\r\nthe following Proposition 3.1.\r\nIt turns out $\\hat \\mu = 1.1924$, meaning that\r\nthe average number of sons produced by any  Kong father  is about\r\n1.2. Taking one step further, if the process is indeed\r\na branching process,  the data is trustworthy and\r\nthe male Kongs can be regarded as typical of\r\nChinese males, the one might claim, on average, each Chinese\r\nmale produces about 1.2 sons.\r\nIt is quite clear that the process cannot be really or even\r\napproximately a branching process---$\\xi^{(n)}_i$ cannot be\r\niid, not over good/bad historical periods, and at least\r\nnot over periods with/without birth control policies.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Alas! Please return to the graph and review related knowledge.", "messagesuccess": "Congratulations! You have mastered the concept and fundamental characteristics of Branching Process.", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [14], "rightproblems": [59], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 7, "fields": {"code": "3.4.1", "category": 4, "problem": "For coin tossing, is there a similar phenomenon for pattern of length 2 to\r\nthat for pattern of length 3 shown Example 3.7?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes.", "choicesb": "No.", "choicesc": "I am not sure.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No. Only four patterns of length 2: HH, HT, TH and TT.\r\nTH occurs before HH with chance 3/4, before HT and TT with 1/2 each.\r\n(Also by symmetry),  HT occurs before TT with chance 3/4, before\r\nTH and HH with 1/2 each. There is no loop of three or four patterns\r\nin which one occurs before another with larger\r\nthan 1/2 chance, unlike patterns of length 3.", "linkability1": 3.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Or you may review Example 3.7.", "messagesuccess": "Great! You know the application of first step analysis.", "sensitivity": 1.0, "gussingparameter": 0.333, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [2, 3, 5, 9, 12], "rightproblems": [50], "wrongproblems": [36], "twinproblems": [50]}}, {"model": "mathematics.question", "pk": 8, "fields": {"code": "3.4.3", "category": 4, "problem": "Based on the fact that the mean number of tosses till\r\n first HH is 6, can you calculate the mean number of tosses till the first\r\n HHH occur (by using one equation)?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "14.", "choicesb": "12.", "choicesc": "10.", "choicesd": "9.", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "(This is a little tricky but still based only on the idea of first step analysis.)\r\nLet $w$ be the mean number till first  HHH  occur. Suppose  HH  occurs the first time\r\n at $n$-th toss. With 1/2 chance the $n+1$-th toss is H and it takes one addtional\r\n toss to have the first\r\n time HHH at $n+1$-th toss (remember, with 1/2 chance). The other 1/2 chance\r\n is for $n+1$-th toss being T. In this case, counting from $n+2$-th toss, it will take averagely\r\n w additional tosses to reach HHH. Hence,\r\n $$ w= 6 + 1/2 \\times 1 + 1/2 \\times (1+ w).$$\r\n Solve the equation, we have $w=14$.", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Here is a hint: apply first step analysis.", "messagesuccess": "Perfect! You have mastered the concept of first step analysis!", "sensitivity": 1.0, "gussingparameter": 0.2, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [3, 9, 13], "rightproblems": [51, 128], "wrongproblems": [4], "twinproblems": [4]}}, {"model": "mathematics.question", "pk": 9, "fields": {"code": "3.4.5", "category": 3, "problem": "A white rat is put into compartment 4 of the maze shown below.\r\nHe moves through the compartments at random; i.e., if there are k ways to\r\nleave a compartment, he chooses each of these with probability 1/k. What\r\nis the probability that the rat finds the food in compartment 3 before feeling\r\nthe electric shock in compartment 7?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\frac{7}{12}$", "choicesb": "$\\frac{1}{4}$", "choicesc": "$\\frac{5}{12}$", "choicesd": "$\\frac{1}{3}$", "choicese": "$\\frac{1}{6}$", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The question ask about some probability, probability $p_i$ have to be defined for each state.<br>\r\nNote: This probability is different from transition probability $P_{ij}$.Here $P_{ij}$ is $\\frac{1}{k}$.<br>\r\nSince the mouse have to reach 3 before 7, set\r\n\\begin{equation*}\r\np_3=1 \\qquad p_7=0\r\n\\end{equation*}\r\nThen, consider the probability of other states, now you need the transition probabilities\r\n\\begin{align*}\r\np_1&=\\frac{1}{2}p_2+\\frac{1}{2}p_4\\\\\r\np_2&=\\frac{1}{3}p_1+\\frac{1}{3}p_3+\\frac{1}{3}p_5\\\\\r\np_4&=\\frac{1}{3}p_1+\\frac{1}{3}p_5+\\frac{1}{3}p_7\\\\\r\np_5&=\\frac{1}{3}p_2+\\frac{1}{3}p_4+\\frac{1}{3}p_6\\\\\r\np_6&=\\frac{1}{2}p_3+\\frac{1}{2}p_5\\\\\r\n\\end{align*}\r\nThen do Gaussian elimination\r\n\\[\\left(\\begin{array}{ccccccc|c}\r\n1&-1/2&0&-1/2&0&0&0&0\\\\\r\n-1/3&1&-1/3&0&-1/3&0&0&0\\\\\r\n0&0&1&0&0&0&0&1\\\\\r\n-1/3&0&0&1&-1/3&0&-1/3&0\\\\\r\n0&-1/3&0&-1/3&1&-1/3&0&0\\\\\r\n0&0&-1/2&0&-1/2&1&0&0\\\\\r\n0&0&0&0&0&0&1&0\r\n\\end{array}\\right)\\]\r\n\r\nAfter tedious calculation, we have\r\n\\begin{align*}\r\np_1&=7/12\\\\\r\np_2&=3/4\\\\\r\np_4&=5/12\\\\\r\np_5&=2/3\\\\\r\np_6&=5/6\\\\\r\n\\end{align*}\r\n\r\nThe answer is 5/12.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [6, 9, 12], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 11, "fields": {"code": "3.1.5", "category": 4, "problem": "$\\{X_n, n= 1, 2, ...\\}$ is\r\n a Markov chain with state space $ \\{-1,0,1\\} $.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\{sin(X_n):n=1,2,\\ldots\\} $ is a Markov chain.", "choicesb": "$\\{cos(X_n):n=1,2,\\ldots\\} $ is a Markov chain.", "choicesc": "$\\{|X_n|:n=1,2,\\ldots\\} $ is a Markov chain.", "choicesd": "$\\{X_n^2:n=1,2,\\ldots\\} $ is a Markov chain.", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The definition of MC $\\{X_n\\}$, namely the past and future are\r\n independent given any fixed state of the present,   can be expressed as\r\n\r\n \t $$P(X_{n+1}=j|X_n=i, X_{n-1}=i_{n-1}, ..., X_0=i_0)=P(X_{n+1}=j_1|X_n=i),$$\r\n \r\n for all time $n$, states $j_1,i, i_0,i_1,...$\r\n \r\n With the defintion of Markov chain, we check the (a) to (d) one by one:\r\n \r\n (a). It is easy to see under the state space ${-1,0,1} $, $ sin(X_n) $ is an one-to-one map on the state space, so \r\n \\begin{eqnarray*}\r\n \t&& P(sin(X_{n+1})=j|sin(X_n)=i,sin(X_{n-1})=i_{n-1}, ..., sin(X_0)=i_0)\r\n \t\\\\\r\n \t&=&\r\n \tP(X_{n+1}=arcsin(j)|X_n=arcsin(i), X_{n-1}=arcsin(i_{n-1}), ..., X_0=arcsin(i_0)\r\n \t\\\\\r\n \t&=&\r\n \t\tP(X_{n+1}=arcsin(j)|X_n=arcsin(i))\r\n \t\\\\\r\n \t&& \\qquad \\hbox{(by applying (1))}\r\n \t\\\\\r\n \t&=&P(sin(X_{n+1})=j|sin(X_n)=i)\r\n\\end{eqnarray*}  \r\n\r\n(b). Under the state space $ \\{-1,0,1\\} $, $ cos(X_n) $ is not an one-to-one map on the state space.\r\n<br>\r\n(c). Under the state space $\\{-1,0,1\\} $, $ |X_n| $ is not an one-to-one map on the state space.\r\n<br>\r\n(d). Under the state space $\\{-1,0,1\\} $, $ X_n^2 $ is not an one-to-one map on the state space.\r\nAnd the map in the (b),(c),(d) are somehow equivalent under the state space ${-1,0,1} $. So they should be both right or both wrong. So they are not correct answer.\r\n<br>\r\nRemark: The question is a type of question: given the $\\{X_n\\}$ is a Markov chain, and ask whether the transformation of $\\{X_n\\}$, such as $\\{cos(X_n)\\}$ is still a Markov chain. What we should do is to check whether the transformation is a one-to-one map.", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "e", "alternativesolutions": "None", "messagefailure": "Oops! Please think about the definition of markov chain and conditional independence.", "messagesuccess": "Congratulations! You have mastered the definition of markov chains\r\nand known how to use it.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [3, 7], "rightproblems": [], "wrongproblems": [12, 13, 14], "twinproblems": [12, 14]}}, {"model": "mathematics.question", "pk": 12, "fields": {"code": "3.1.6", "category": 4, "problem": "$\\{X_n, n= 1, 2, ...\\}$ is\r\n a Markov chain with state space $ \\{1,2,\\ldots,10\\} $.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$ \\{X_n+X_{n-1}: n=1,2,\\ldots\\} $ is a Markov chain", "choicesb": "$ \\{X_n-X_{n-1}: n=1,2,\\ldots\\} $ is a Markov chain", "choicesc": "$ \\{X_nX_{n-1}: n=1,2,\\ldots\\} $ is a Markov chain", "choicesd": "$ \\{X_n/X_{n-1}: n=1,2,\\ldots\\} $ is a Markov chain", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "E", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "In the answer of  DIY 3.1.5, the transformation in the state space should be one-to-one.\r\n \r\n With the answer in  DIY 3.1.5, we check the (a) to (d).\r\n In (a), the transformation is $ f(x,y) = x+y $ which is not one-to-one. So (a) is not a correct answer. The same is as transformations $ f(x,y) = x-y $, $ f(x,y) = xy $, $ f(x,y) = x/y $ which are in the (b), (c) and (d) . So the right answer is (e).", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "A", "alternativesolutions": "", "messagefailure": "Oops! Please think about the definition of markov chain and conditional independence.", "messagesuccess": "Congratulations! You have mastered the definition of markov chains\r\nand known how to use it.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [3, 7], "rightproblems": [14], "wrongproblems": [11], "twinproblems": [11]}}, {"model": "mathematics.question", "pk": 13, "fields": {"code": "3.1.7", "category": 4, "problem": "Suppose we have three random variables $ X_0 $, $ X_1 $ and $ X_2 $ forming a Markov process with time domain $ \\{0,1,2\\} $. Then which following is right?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$ X_1, X_2,X_0 $ also form a Markov process.", "choicesb": "$ X_2, X_0,X_1 $ also form a Markov process.", "choicesc": "$ X_0, X_2,X_1 $ also form a Markov process.", "choicesd": "$ X_2, X_1,X_0 $ also form a Markov process.", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The definition of MC $\\{X_n\\}$, namely the past and future are\r\n independent given any fixed state of the present,   can be expressed as\r\n \r\n $$P(X_{n+1}=j|X_n=i, X_{n-1}=i_{n-1}, ..., X_0=i_0)=P(X_{n+1}=j_1|X_n=i),$$\r\n \r\n for all time $n$, states $j_1,i, i_0,i_1,...$\r\n \r\n With the defintion of Markov chain, we have \r\n \\begin{eqnarray*}\r\n P(X_2 = j|X_1 = i_1, X_0 = i_0)&=& P(X_2=j|X_1 = i_1)\r\n \\\\\r\nthen ~~\\frac{P(X_2 = j,X_1 = i_1, X_0 = i_0)}{P(X_1 = i_1, X_0 = i_0)} &=& \\frac{P(X_2=j, X_1 = i_1)}{P(X_1 = i_1)}\r\n \\\\\r\n then~~\\frac{P(X_2 = j,X_1 = i_1, X_0 = i_0)}{P(X_2=j, X_1 = i_1)}&=&\\frac{P(X_1 = i_1, X_0 = i_0)}{P(X_1 = i_1)} \r\n \\\\\r\n namely~~P(X_0 = i_0|X_2=j, X_1 = i_1)&=&P(X_0=i_0|X_1 = i_1)\r\n  \\end{eqnarray*}\r\n which means $ X_2, X_1,X_0 $ is a Markov chain.", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "A", "alternativesolutions": "", "messagefailure": "Oops! Please think about the definition of markov chain and conditional independence.", "messagesuccess": "Congratulations! You have mastered the definition of markov chains\r\nand known how to use it.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [3, 7], "rightproblems": [], "wrongproblems": [11], "twinproblems": []}}, {"model": "mathematics.question", "pk": 14, "fields": {"code": "3.1.8", "category": 4, "problem": "Mickey mouse and travels blindly independently in the maze in Example 3.2. And Donald duck follows Mickey's footstep.Let $ M_n $ be cell number of Mickey after n step transition and $ D_n $ be the cell number of Donald after n step transition. Then $ D_n = M_{n-1} $ for $ n\\geq1 $, with $ M_0 = D_1 = 4 $. Counting $ n = 1, 2, \\ldots $\r\n<br>\r\nThe maze is below:", "problempicture1": "theall/image/WX20170316-2301342x.png", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$ D_n^2 $ is a Markov chain.", "choicesb": "$ 2M_n +D_n $ is a Markov chain.", "choicesc": "$ M_nD_n $ is a Markov chain.", "choicesd": "$ M_n - D_n $ is a Markov chain.", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "In the remark of  DIY 3.1.5, the transformation in the state space should be one-to-one. The transformation as follow where we regard $ D_n = M_{n-1} $:\r\n <br>\r\n (a). $ f(x) = x^2 $\r\n <br>\r\n (b). $ f(x,y) = 2x + y $\r\n <br>\r\n (c). $ f(x,y) = xy $\r\n<br>\r\n (d). $ f(x,y) = x - y $\r\n <br>\r\n With the state space $ 0, 1, \\ldots, 8 $, $ f(x) $ in (a) is one-to-one. So the right answer is (a).", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "E", "alternativesolutions": "", "messagefailure": "Oops! Please think about the definition of markov chain and conditional independence.", "messagesuccess": "Congratulations! You have mastered the knowledge in the 3.1.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [3, 7], "rightproblems": [12], "wrongproblems": [11], "twinproblems": [11]}}, {"model": "mathematics.question", "pk": 15, "fields": {"code": "3.2.1", "category": 4, "problem": "The transition probability matrix is \r\n  \r\n  $$ \\qquad  \\,\\, \\,\\,\\, 0\\qquad 1\\qquad 2  $$\r\n  $${ P} =\\matrix{ 0 \\cr 1 \\cr 2    }\r\n  \\pmatrix{\r\n  \t0 &  2/3  &  1/3              \\cr\r\n  \t1/2\t&   0  &  1/2      \\cr\r\n  \t1/2\t &   1/2  &   0          \\cr\r\n  }\r\n  $$\r\n Then, $ P_{00}^{(3)} $ is", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1/2", "choicesb": "1/3", "choicesc": "1/4", "choicesd": "1/5", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "$ P_{00}^{(3)} $ means with initial point 0, the probability that it still at point 0 after 3 steps.\r\nWhen we are trying to know such kind probability, The easiest way is to compute $ P^{(3)} $. $ P^{(3)} = P\\times P \\times P$ .\r\n$$ \\qquad  \\,\\, \\,\\,\\, 0\\qquad 1\\qquad 2  $$\r\n$${ P^2} =\\matrix{ 0 \\cr 1 \\cr 2    }\r\n\\pmatrix{\r\n\t1/2 &  1/6  &  1/3              \\cr\r\n\t1/4\t&   7/12  &  1/6      \\cr\r\n\t1/4\t &   1/3  &   5/12          \\cr\r\n}\r\n$$\r\n$$ \\qquad  \\,\\, \\,\\,\\, 0\\quad 1 \\,\\quad 2  $$\r\n$${ P^3} =\\matrix{ 0 \\cr 1 \\cr 2    }\r\n\\pmatrix{\r\n\t1/4 &  *  &  *              \\cr\r\n\t*\t&   *  &  *      \\cr\r\n\t*\t &   *  &   *       \\cr\r\n}\r\n$$\r\nWe only need the first component of $ P^3 $ which is $ P_{00}^{(3)} $ . So the right answer is c.", "linkability1": 1.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Please check how to calculate transition matrix.", "messagesuccess": "Congratulations! You have known how to calculate the transition matrix.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [5], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 16, "fields": {"code": "3.4.6", "category": 4, "problem": "The transition probability matrix is \r\n   $$ \\qquad  \\,\\, \\,\\,\\, 0\\qquad 1\\qquad 2  $$\r\n  $${ P} =\\matrix{ 0 \\cr 1 \\cr 2    }\r\n  \\pmatrix{\r\n  \t0 &  2/3  &  1/3              \\cr\r\n  \t1/2\t&   0  &  1/2      \\cr\r\n  \t1/2\t &   1/2  &   0          \\cr\r\n  }\r\n  $$\r\n \r\n Then, starting from state 0, the mean number of visits of state 2 before coming back to state 0 is", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "5/9", "choicesb": "6/9", "choicesc": "7/9", "choicesd": "8/9", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The key to solve this question is use first step analysis. We use $ w_i $ to stand for starting from state i, the mean number of visits of state 2 before coming back to state 0. So we have\r\n \\begin{eqnarray*}\r\n w_0 &=& 2/3w_1 + 1/3w_2\\\\\r\n w_1 &=& 1/2 w_2\\\\\r\n w_2 &=& 1 + 1/2w_1\r\n \\end{eqnarray*}\r\nSo$ w_0 = 8/9, w_1 = 2/3, w_2 = 4/3$. So the answer is d.\r\n<br>\r\n First step analysis is a very useful way to sovle this question. If we do not use this analysis, the question will be very difficult.", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Try to use first step analysis to deal with the question.", "messagesuccess": "Congratulations! You have a right answer on this question about first step analysis.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [], "wrongproblems": [17, 18], "twinproblems": [17, 18, 19, 20]}}, {"model": "mathematics.question", "pk": 17, "fields": {"code": "3.4.7", "category": 4, "problem": "A Markov chain has one-step transition probability  matrix\r\n  \r\n  $$ \\qquad  \\,\\, ~~0 \\qquad 1 \\qquad 2\\qquad 3  $$\r\n $${\\bf P} =\\matrix{0 \\cr 1 \\cr 2 \\cr 3    }\r\n \\pmatrix{\r\n \t0 &  1/3  &  1/3   &   1/3           \\cr\r\n \t1/3\t&   1/3  &  1/3  &    0          \\cr\r\n \t1/2\t &   1/2  &   0  &   0          \\cr\r\n \t0 \t&   1/2   &  1/2   &   0\r\n }\r\n $$\r\n Then, starting from state 0, the chance of visits of state 3 before visiting state 2 is", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0", "choicesb": "1/5", "choicesc": "2/5", "choicesd": "3/5", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The key to solve this question is use first step analysis. We use $ w_i $ to stand for starting from state i, the chance of visiting state 3 before visitng state 2. So we have\r\n \\begin{eqnarray*}\r\n w_0 &=& 1/3w_1 + 1/3w_2 + 1/3w_3\\\\\r\n w_1 &=& 1/3 w_0 + 1/3 w_1 +1/3 w_2\\\\\r\n w_2 &=& 0\\\\\r\n w_3 &=& 1\r\n \\end{eqnarray*}\r\nSo $ w_0 = 2/5, w_1 = 1/5, w_2 = 0, w_3 = 1$. So the answer is c.", "linkability1": 3.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Try to use first step analysis to deal with the question.", "messagesuccess": "Congratulations! You have a right answer on this question about first step analysis.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [], "wrongproblems": [16], "twinproblems": [16]}}, {"model": "mathematics.question", "pk": 18, "fields": {"code": "3.4.8", "category": 4, "problem": "A Markov chain has one-step transition probability  matrix\r\n  \r\n $$ \\qquad  \\,\\, 0 \\qquad 1 \\qquad 2\\qquad 3  $$\r\n$${\\bf P} =\\matrix{0 \\cr 1 \\cr 2 \\cr 3    }\r\n\\pmatrix{\r\n\t0 &  1/3  &  1/3   &   1/3           \\cr\r\n\t1/3\t&   1/3  &  1/3  &    0          \\cr\r\n\t1/2\t &   1/2  &   0  &   0          \\cr\r\n\t0 \t&   1/2   &  1/2   &   0\r\n}\r\n$$\r\n Then, starting from state 0, the mean number of visits of state 2 from state 1 before visiting state 3, is", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1/4", "choicesb": "1/3", "choicesc": "1/2", "choicesd": "1", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The key to solve this question is use first step analysis. We use $ w_i $ to stand for starting from state i,the mean number of visits of state 2 from state 1 before visiting state 3. So we have\r\n \\begin{eqnarray*}\r\n w_0 &=& 1/3w_1 + 1/3w_2 + 1/3w_3\\\\\r\n w_1 &=& 1/3 w_0 + 1/3 w_1 +1/3 (w_2+1)\\\\\r\n w_2 &=& 1/2w_0 + 1/2w_1\\\\\r\n w_3 &=& 0\r\n \\end{eqnarray*}\r\nSo $ w_0 = 1, w_1 = 4/3, w_2 = 5/3, w_3 = 0$. So the answer is d.", "linkability1": 3.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Try to use first step analysis to deal with the question.", "messagesuccess": "Congratulations! You have a right answer on this question about first step analysis.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [], "wrongproblems": [16, 19], "twinproblems": [16]}}, {"model": "mathematics.question", "pk": 19, "fields": {"code": "3.4.9", "category": 4, "problem": "A Markov chain has one-step transition probability  matrix\r\n  $$ \\qquad  \\,\\,  0\\qquad 1\\qquad 2\\qquad 3  $$\r\n  $${\\bf P} =\\matrix{ 0 \\cr 1 \\cr 2 \\cr 3    }\r\n  \\pmatrix{\r\n  \t1/4 &  1/4  &  1/4  &   1/4            \\cr\r\n  1/4 & 1/4  &  1/2  &    0     \\cr\r\n  0\t &   0  &   1/2  &   1/2         \\cr\r\n 0 \t&   0   &  1/3   &   2/3    \r\n   }\r\n  $$\r\n  \r\n Starting from state 0, the mean number of visits of state 1 is", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1/2", "choicesb": "1/3", "choicesc": "1/4", "choicesd": "1/5", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "E", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The key to solve this question is use first step analysis. We use $ w_i $ to stand for starting from state i,the mean number of visits of state 1. So we have\r\n \\begin{eqnarray*}\r\n w_0 &=& 1/4w_0 + 1/4w_1 + 1/4w_2 + 1/4w_3\\\\\r\n w_1 &=& 1 + 1/4 w_0 + 1/4 w_1 +1/2 w_2\\\\\r\n w_2 &=& 1/2w_2 + 1/2w_3\\\\\r\n w_3 &=& 1/3w_2 + 2/3w_3\r\n \\end{eqnarray*}\r\nFirst we know $ w_2 = w_3 $. Then we can see that $ w_2 = w_3 = 0 $, since from stating 2 or 3, we can never back to state 1.\r\nSo $ w_0 = 1/8, w_1 = 3/8, w_2 = 0, w_3 = 0$. So the answer is e.", "linkability1": 3.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Try to use first step analysis to deal with the question.", "messagesuccess": "Congratulations! You have a right answer on this question about first step analysis.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [19], "wrongproblems": [18], "twinproblems": [16]}}, {"model": "mathematics.question", "pk": 20, "fields": {"code": "3.4.10", "category": 4, "problem": "Toss a fair coin a number of times. You always bet on the head to make one dollor if it is a head and to lose one dollar if it is a tail. You are stopped whenever you make 3 dollar or you lose 2 dollar. Then,", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "you end up winning with 2/3 probability", "choicesb": "you end up winning with 1/3 probability", "choicesc": "you end up winning with 3/4 probability", "choicesd": "you end up winning with 3/5 probability", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "E", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "First we need to find the markov process in the question. We have 6 states. In order not to make you confused, we use state $ i $ to stand for the number of your money you win. Negative means you lose.\r\n<br>\r\n\t\tSo the one-step transition probability  matrix in this question is:\r\n\t\t   $$ \\qquad \\,\\,\\quad-2 ~\\,\\quad -1 \\,~\\quad 0\\,\\,~\\quad 1 \\,\\,~\\quad 2\\,\\,~\\quad3  $$\r\n\t\t$${\\bf P} =\\matrix{-2 \\cr -1 \\cr 0 \\cr 1 \\cr 2 \\cr 3    }\r\n\t\t\\pmatrix{\r\n\t\t\t1 &  0  &  0  &   0 & 0 &0           \\cr\r\n\t\t\t1/2\t&   0  &  1/2  &    0  & 0 & 0    \\cr\r\n\t\t\t0\t &   1/2  &   0  &   1/2&0&  0        \\cr\r\n\t\t\t0 \t&   0   &  1/2   &   0&1/2&0       \\cr\r\n\t\t\t0 & 0 & 0 & 1/2 &0 &1/2             \\cr\r\n\t\t\t0 &0&0&0&0&1\r\n\t\t}\r\n\t\t$$\r\n\t\tThe key to solve this question is use first step analysis. We use $ w_i $ to stand for starting from state i,the chance we make 3 dollar before we lose 2 dollar. So we have\r\n\t\t\\begin{eqnarray*}\r\n\t\t\tw_{-2} &=& 0\\\\\r\n\t\t\tw_{-1} &=& 1/2w_{-2} + 1/2w_0 \\\\\r\n\t\t\tw_0 &=& 1/2w_{-1} + 1/2w_1\\\\\r\n\t\t\tw_1 &=& 1/2w_0 + 1/2w_2\\\\\r\n\t\t\tw_2 &=& 1/2w_1 + 1/2w_3\\\\\r\n\t\t\tw_3 &=& 1\r\n\t\t\\end{eqnarray*}\r\n\t\r\n\t\tSo $ w_{-2} =0, w_{-1} = 1/5 , w_0 = 2/5, w_1 = 3/5, w_2 = 4/5, w_3 = 1$. So the answer is e.", "linkability1": 3.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Try to use first step analysis to deal with the question.", "messagesuccess": "Congratulations! You have a right answer on this question about first step analysis.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [6, 9], "rightproblems": [], "wrongproblems": [], "twinproblems": [16]}}, {"model": "mathematics.question", "pk": 21, "fields": {"code": "0.1", "category": 5, "problem": "Two events A and B each having probability 0.5 and 0.7, respectively. The probability for A and B happen at the same time should be", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1", "choicesb": "smaller than 0.2", "choicesc": "at least 0.2", "choicesd": "0", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The key to solve this question is  $$ P(A\\cap B) = P(A) +P(B) - P(A\\cup B)$$\r\n With  $ P(A) = 0.5  $ and $ P(B) = 0.7 $, $ P(A\\cap B) $ = $ 1.2 -P(A\\cup B) $. Although we do not know the value of $ P(A\\cup B) $. But it must between 0.7 and 1. Larger than $ P(B) $ but less than 1. So the value of $ P(A\\cap B) $ must between 0.2 and 0.5. So the answer is c.", "linkability1": 1.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "A", "alternativesolutions": "", "messagefailure": "Don't worry! Just try it again.", "messagesuccess": "Congratulations! You have successfully finished your first question.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [23], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 22, "fields": {"code": "0.2", "category": 5, "problem": "There are three events: $ A $ and $ B $ and $ C $. We know $ P(A|B) = P(B|C) = 0.5$. Then $ P(A|C) $ should be", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0", "choicesb": "0.25", "choicesc": "0.5", "choicesd": "1", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "E", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Unfortunately, there is no answer for what is the value of $ P(A|C) $. In the different case, the value of $ P(A|C) $ is different. Think about the two later case.\r\n<br>\r\n The first one is $ A, B, C $ are independent with $ P(A) = P(B) = P(C) = 0.5$. We can see they satisfy the condition in the question.  $ P(A|C) $ there is 0.5.\r\n<br>\r\n The second one is $ A =C $ and they are independent of $ B $. We also have  $ P(A) = P(B) = P(C) = 0.5$. They satisfy the condition in the question.  $ P(A|C) $ there is 1.\r\n So there is no answer for what is the value of $ P(A|C) $.\r\n<br>\r\n Attention! $ P(A|B)  P(B|C) \\neq P(A|C)$.", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "B", "alternativesolutions": "", "messagefailure": "Don't worry! Just try it again.", "messagesuccess": "Congratulations! You have the right answer.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [25], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 23, "fields": {"code": "0.3", "category": 5, "problem": "For a random variable $ X $, which the statement below is correct?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "If $ X  \\sim$ Binomial$(n,p)$, then $ Var X = np $.", "choicesb": "If $ X  \\sim$ Poisson$(\\lambda)$, then $ Var X = \\lambda^2 $.", "choicesc": "If $ X  \\sim$ $N(\\mu,\\sigma^2)$, then $ Var X = \\mu $.", "choicesd": "If $ X \\sim $ Geometric$ (p) $, then $ Var X $ = $ \\frac{p}{(1-p)^2} $", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "We just see it one by one.\r\n<br>\r\n(a). If $ X  \\sim$ Binomial$(n,p)$, then $ Var X = np(1-p) $.\r\n<br>\r\n(b). If $ X  \\sim$ Poisson$(\\lambda)$, then $ Var X = \\lambda $.\r\n<br>\r\n(c). If $ X  \\sim$ $N(\\mu,\\sigma^2)$, then $ Var X = \\sigma^2 $.\r\n<br>\r\n(d). It is the right answer.", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "B", "alternativesolutions": "", "messagefailure": "Don't worry! Next time you will remember the variance.", "messagesuccess": "Congratulations! You have the correct answer.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [31], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 24, "fields": {"code": "0.4", "category": 5, "problem": "Two random variables $ X $ and $ Y $ follow the same distribution. Then", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "The distribution of $ X-Y $ must be symmetric about 0.", "choicesb": "The median of $ X-Y $ must be zero.", "choicesc": "The median of $ X+Y $ is twice of the median of $ X $.", "choicesd": "The mean of $ X-Y $, if finite, must be 0.", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The key to solve this question is that we do not know the relationship of $ X $ and $ Y $. $ X $ and $ Y $ may be independent or $ X = Y $, even $ X = -Y  $ in same speical case. So it is impossible to know the the distribution or the median of $ X - Y $ or $ X + Y $. However, we can know the mean of $ X + Y $. If the mean is finite, $ E[X-Y] = E[X]-E[Y]=0$. So the answer d is correct.\r\n<br>\r\nRemark: If you are still not sure even if you get correct answer, I suggest you think about whether a, b, c is wrong. Thinking about the counterexample will help you a lot.", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "B", "alternativesolutions": "", "messagefailure": "Don't worry! Just try it again.  If you cannot exclude wrong answers, just choose the one which is most likely correct.", "messagesuccess": "Congratulations! You have correct answer on this little difficult question.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [31], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 25, "fields": {"code": "0.5", "category": 5, "problem": "Toss a fair coin ten times. The chance there is neither consecutive heads nor consecutive tails is", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$ 2^{-1} $", "choicesb": "$ 2^{-5} $", "choicesc": "$ 2^{-9} $", "choicesd": "$ 2^{-10} $", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "First we count the number of total case. It is $ 2^{10} $. Then we count the number of case in which there is neither consecutive heads nor consecutive tails. We use H to stand for head and T to stand for tail. There are only $ 2 $ cases. It is \"HTHTHTHTHT\" and \"THTHTHTHTH\". So the chance is  $ 2/2^{10} $, which is $ 2^{-9} $. So the right answer is c.", "linkability1": 1.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "D", "alternativesolutions": "", "messagefailure": "Don't worry! Just try it again.", "messagesuccess": "Congratulations! You get the correct answer on this question.", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [23], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 26, "fields": {"code": "0.6", "category": 5, "problem": "Two random variables have zero correlation. Then,", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "They are independent.", "choicesb": "If they are normal random variables, they are independent.", "choicesc": "If they are Poisson random variables, they are independent.", "choicesd": "If they are random variables following uniform distributions, they are independent.", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The zero correlation may not be independence. But the independence must be zero correlation. Only in some special case, the zero correlation means independence. The normal distribution has that feature. But Poisson distribution and uniform distribution do not have that feature.", "linkability1": 3.0, "linkability2": 0.0, "linkability3": 1.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "E", "alternativesolutions": "", "messagefailure": "Don't worry! Just try it again.", "messagesuccess": "Congratulations! You get the correct answer on this question.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [32, 33], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 27, "fields": {"code": "0.7", "category": 5, "problem": "Let $ X = -10Y+10 $. Let $ r_1 $ be the correlation between $ X $ and $ Z $ and $ r_2 $ be the correlation between $ Y $ and $ Z $. Then,", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$ r_1 = r_2 $", "choicesb": "$ r_1 =10 r_2 $", "choicesc": "$ r_1 = -10r_2 $", "choicesd": "$ r_1 = -r_2 $", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "We should know how to calculate the correlation between two random variables.\r\n$$ r_{xy} =\\frac{E(X-EX)(Y-EY)}{\\sqrt{VarX VarY}}$$\r\nW.L.O.G. We assume that $ EY=0, EZ=0 $, so \r\n\\begin{eqnarray*}\r\nr_{xz}  &=& \\frac{E(-10YZ)}{\\sqrt{Var(10Y) VarZ}}\\\\\r\n&=&-\\frac{E(YZ)}{\\sqrt{VarY VarZ}}\\\\\r\n&=&-r_{yz}\r\n\\end{eqnarray*}\r\nSo $ r_1 = -r_2 $, which is answer d.\r\n<br>\r\nRemark: The assumption $ EY=0, EZ=0 $ does not affect the final answer. If you are not sure, you can prove it bu yourself. It will help you a lot.", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "C", "messagefailure": "Don't worry! Just try it again.", "messagesuccess": "Congratulations! You get the correct answer on this question.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [33], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 28, "fields": {"code": "0.8", "category": 5, "problem": "The density of a random variabel $ X $ is $$f(x)  \\propto x^{-1/2} , x\\in [0,1]$$\r\nand $ f(x) = 0 $ for $ x \\notin [0,1] $. Here $\\propto$ means proporional to. Then, the mean of $ X $ is", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$ 1/2 $", "choicesb": "$ 1/\\sqrt{2} $", "choicesc": "$ 1/3 $", "choicesd": "$ 1/4 $", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "With the information in the question, we know that density function $ f(x) = Cx^{-1/2}, x\\in [0,1]$ with.\r\nSince $ \\int_{0}^{\\infty} f(x) dx=1$. So $ \\int_{0}^{1} Cx^{-1/2} dx=1$. So the $ C= 1/2 $.\r\n<br>\r\nNow we know the density function. We can calculate the expectation of X.\r\n$ EX = \\int_{0}^{1} f(x)x dx$ = $ \\int_{0}^{1} 1/2 \\cdot x^{-1/2} \\cdot x dx  $ = 1/3.\r\n<br>\r\nSo the right answer is (c).", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "A", "alternativesolutions": "", "messagefailure": "Don't worry! Just try it again.", "messagesuccess": "Congratulations! You get the correct answer on this question.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 29, "fields": {"code": "0.9", "category": 5, "problem": "Toss a fair coin n times. Let $ S_n $ be the total number of heads. If $ n $ is large enough, then,", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$ S_n $ is close to the standard normal distribution.", "choicesb": "$ S_n $ is close to a Poisson distribution.", "choicesc": "$ S_n $ follows binomial distribution.", "choicesd": "$ S_n $ follows uniform distribution over integers $ 0,\\ldots,n. $", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "From the defintion of binomial distribution, it is easy to see that $ S_n $ follows binomial distribution.\r\n<br>\r\nRemark: Think about whether a, b are wrong.\r\n<br>\r\na. With central limit theorem, $ \\frac{S_n - \\frac{1}{2}n}{\\sqrt{n/4}}$ is close to standard normal distribution. Not  $ S_n $ is close to the standard normal distribution.\r\n<br>\r\nb. Poisson approximation only holds on the assumption that $ p $ is very small. Here $ p=1/2 $.", "linkability1": 2.0, "linkability2": 0.0, "linkability3": 1.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "A or B", "alternativesolutions": "", "messagefailure": "Don't worry! Just try it again.", "messagesuccess": "Congratulations! You get the correct answer on this question.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [29], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 30, "fields": {"code": "0.10", "category": 5, "problem": "Suppose $ P(|X|<1) = 1$ and $ P(|Y| =2) = 1 $. Then,", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "The standard deviation of $ X $ is smaller than that of $ Y $.", "choicesb": "The mean of $ X $ is smaller than that of $ Y $.", "choicesc": "The variance of $ X $ is larger than that of $ Y $.", "choicesd": "The median of $ X $ is equal to the median of $ Y $.", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "E", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "It is not easy question. We need to check them one by one.\r\n<br>\r\na. We choose $ Y = 2 $. So the standard deviation of $ Y $ is 0.\r\n<br>\r\nb. We choose $ P(Y=2)=1/2 $ and $P(Y=-2)=1/2 $. And we choose $ P(X>0) =1 $. So the mean of $ X $ is smaller than that of $ Y $, which is 0 here.\r\n<br>\r\nc. We choose $ X = 1/2 $. So $ Var(X) = 0 $.\r\n<br>\r\nd. We choose the case in a. The median of $ Y $ is 2. But the meidian of $ X$ is smaller than 1.", "linkability1": 3.0, "linkability2": 0.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "A, B, C or D", "alternativesolutions": "", "messagefailure": "Don't worry! Just try it again.", "messagesuccess": "Congratulations! You get the correct answer on this question. And you have finished the part of quiz 0.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [31], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 31, "fields": {"code": "3.1.1", "category": 1, "problem": "Let $\\xi_0=0$ and, for $i \\geq 1$,\r\n$\\xi_i$ =\r\n $$\\cases{ 1 & if the $i$-th toss is a Head (with probability $p$) \\cr\r\n0 & if the $i$-th toss is a Tail (with probability $1-p$)}$$\r\nSet $X_n= \\sum_{i=0}^n \\xi_i $, $n \\geq 0$. $X_n$ is the random number which counts\r\nthe number of Heads up to the $n$-th toss.What's the value of\r\n$P(X_{n+1}=j | X_0=0, X_1 = i_1, ..., X_{n-1}= i_{n-1}, X_n = i)?$", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "\\[  = \\left\\{\r\n\\begin{array}{ll}\r\n P(\\xi_{n+1}=1) & if ~j=i+1 \\\\\r\nP(\\xi_{n+1}=0) & if ~j=i\\\\\r\n0 & otherwise\\\\\r\n\\end{array} \r\n\\right. \\]", "choicesb": "$ { P(\\xi_{n+1}=1)}=1$", "choicesc": "None of the above is correct.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Just straight compute the probability\r\n\\begin{eqnarray*}\r\n&& P(X_{n+1}=j | X_0=0, X_1 = i_1, ..., X_{n-1}= i_{n-1}, X_n = i) \\\\\r\n&=& \\cases{ P(\\xi_{n+1}=1|X_0=0, X_1 = i_1, ..., X_{n-1}= i_{n-1}, X_n = i) & if $j=i+1$ \\cr\r\nP(\\xi_{n+1}=0|X_0=0, X_1 = i_1, ..., X_{n-1}= i_{n-1}, X_n = i) & if $j=i$ \\cr\r\n0 & otherwise \\cr\r\n}\\\\\r\n&=& \\cases{ P(\\xi_{n+1}=1) & if $j=i+1$ \\cr\r\nP(\\xi_{n+1}=0) & if $j=i$ \\cr\r\n0 & otherwise \\cr\r\n}\\\\\r\n&=& \\cases{ p & if $j=i+1$ \\cr\r\n1-p & if $j=i$ \\cr\r\n0 & otherwise \\cr\r\n}\\\\\r\n&=& \\cases{ P(\\xi_{n+1}=1|X_n=i) & if $j=i+1$ \\cr\r\nP(\\xi_{n+1}=0|X_n=i) & if $j=i$ \\cr\r\n0 & otherwise \\cr\r\n}\\\\\r\n&=& P(X_{n+1}=j |X_n=i)\r\n\\end{eqnarray*} <br>\r\n\r\n\r\nIt implies, once the present $X_n$ is fixed, the past history $X_0, ..., X_{n-1}$, shall not\r\naffect the future distribution of $X_{n+1}$.}\r\nHere, we are using time $n$ as present, time before $n$ is past, and time beyond $n$ is\r\nfuture.\r\n\r\nThe above statement is the same as saying that\r\nthe future $\\{X_k: k\\geq n+1\\}$ and the past $\\{X_k: k\\leq n-1\\}$ are\r\n$conditionally$ $independent$ given the present $X_n$ taking any fixed value.<br>\r\n$remark$   <br>Not only $n+1$ but, all future distribution $X_{n+1}, X_{n+2}, ....$ shall depend on\r\nthe past and now only through now.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Alas! Please check the definition of markov chain and conditional independence.", "messagesuccess": "Congratulations! You have mastered the definition of markov chains\r\nand conditional independence.", "sensitivity": 1.0, "gussingparameter": 3.0, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [3, 7], "rightproblems": [], "wrongproblems": [2], "twinproblems": []}}, {"model": "mathematics.question", "pk": 32, "fields": {"code": "3.1.2", "category": 1, "problem": "({ Mickey in Maze})\r\nMickey mouse travels in a maze with nine $3\\times 3$ cells. The cells are numbered\r\nas 0, 1, ..., 8 from left to right and top down. Each step Mickey travels\r\nfrom where it is to one\r\nof the surrounding connected cells with equal chance.Let $X_n$ denote the\r\ncell number of Mickey at step $n$. $(X_0=4)$.Is The process$\\{X_n: n=0, 1, 2, ...\\}$ a Markov chain?", "problempicture1": "theall/image/Example_3-1-2.PNG", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "yes", "choicesb": "no", "choicesc": "None of the above is correct.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The definition of MC $\\{X_n\\}$, namely the past and future are\r\n independent given any fixed state of the present, <br>\r\n \\begin{eqnarray*}\r\n&& P(X_{n+1}=j | X_0=0, X_1 = i_1, ..., X_{n-1}= i_{n-1}, X_n = i) \\\\\r\n&=&\r\nP(X_{n+1}=j |X_n=i).\r\n\\end{eqnarray*}\r\nSuppose currently Mickey is in cell 5, for example,\r\nthe future movement or path of Mickey is irrelevant with the past movement\r\nor path of Mickey. In other words, how Mickey has got to cell 5 in the past\r\nhas nothing to do with how Mickey would move around in the future.\r\nThe process $\\{X_n: n=0, 1, 2, ...\\}$ is a Markov chain.<br>\r\n{ \\it Example for fun} (``First Blood.\") John Rambo only  obeys\r\nthe order from  Colonel Samuel Trautman,\r\nwho supposedly only  obeys the order from the Pentagon. Then\r\nPentagon $\\longrightarrow$ Trautman $\\longrightarrow$\r\nRambo forms a $MC$.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "well,try again?", "messagesuccess": "good job!", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [3, 7], "rightproblems": [2], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 33, "fields": {"code": "3.1.3", "category": 1, "problem": "({Mickey in Maze run more steps)\r\nMickey mouse travels in a maze with nine $3\\times 3$ cells. The cells are numbered\r\nas 0, 1, ..., 8 from left to right and top down. Each step Mickey travels\r\nfrom where it is to one\r\nof the surrounding connected cells with equal chance.Compute $P^{(3)}_{4\\, 8}$ and $P^{(3)}_{1\\, 8}.$", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$P^{(3)}_{4\\, 8}$=$0.5$,$P^{(3)}_{1\\, 8}$=$0.5$", "choicesb": "$P^{(3)}_{4\\, 8}$=$0$,$P^{(3)}_{1\\, 8}$=$0$", "choicesc": "None of the above is correct.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Remember the basic definition and method of computing the transition  probability,\r\n  write\r\n$$P^{(3)}_{4\\, 8} = \\sum_{k=0}^\\infty P_{4\\, k} P_{k\\, 8}^{(2)}\r\n=\\sum_{k=1, 3, 5, 7} P_{4\\, k} P_{k\\, 8}^{(2)} = 1/4\\sum_{k=1, 3, 5, 7}   P_{k\\, 8}^{(2)}=0;$$\r\n$$P^{(3)}_{1\\, 8} = 1/3 \\times 1/2 \\times 1/3 + 1/3\\times 1/4 \\times 1/3 +\r\n1/3 \\times 1/4 \\times 1/3 =0.$$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": 2.0, "gussingparameter": 3.0, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [3, 6, 7], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 34, "fields": {"code": "3.1.4", "category": 1, "problem": "( An Inventory Mode)\r\nLet $X_n$ be the number of TV sets at a store in the end of day $n$ with $X_0=2$. Let $\\xi_n$ be the\r\nsales of the number of TVs on day $n$. Assume $\\xi_1, \\xi_2, ...$ are iid (independent, identically distributed) such that\r\n$$P(\\xi_n=i) = \\cases{0.5 & $i=0 $\\cr 0.4 & $i =1$ \\cr  0.1 & $i=2$ }$$\r\nAt the end of any day $n$, if $X_n=0$ or $-1$, two TVs will be sent to the store overnight. Moreover,\r\nin the case of  $X_n=-1$,\r\nanother TV will be sent directly to the customer's house. If $X_n =1$ or $2$,\r\nnothing happens. With this inventory policy,\r\n$$X_{n+1}= \\cases{ X_n - \\xi_{n+1} & if $X_n=1, 2$ \\cr 2- \\xi_{n+1} & if $X_n=-1, 0$.}$$\r\nThen, $X_0, X_1, X_2, ..., $ is a $MC$ with state space $\\{-1, 0, 1, 2\\}$ .What's the one-step trasition probability from 1 to 2($p_{1\\,2}$)", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1", "choicesb": "0", "choicesc": "None of the above is correct.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Remember the basic definition and method of computing the one-step transition  probability,<br>\r\n $X_0, X_1, X_2, ..., $ is a $MC$ with state space $\\{-1, 0, 1, 2\\}$ and with one-step\r\ntransition probability\r\n$$\\qquad \\,\\,\\,\\,\\,\\,\\, -1 \\qquad 0 \\qquad 1 \\qquad 2$$\r\n$$P=\\matrix{-1 \\cr 0 \\cr 1 \\cr 2}\\pmatrix{0 &0.1 &0.4 &0.5 \\cr 0 &0.1 &0.4 &0.5\\cr\r\n0.1 &0.4 &0.5 &  0 \\cr 0 &0.1 &0.4 &0.5 }.$$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [3, 6], "rightproblems": [], "wrongproblems": [], "twinproblems": [35, 90]}}, {"model": "mathematics.question", "pk": 35, "fields": {"code": "3.1.5", "category": 1, "problem": "{ ({ The Ehrenfest Model}) <br>\r\nThere are $2N$ particles in a jar separated by a membrane into two chambers A and B. Let\r\n$Y_n$ be the number of particles in A after $n$ crossings. Each crossing is a particle from\r\nA to B or from B to A. Assume when any once crossing happens, it happens to any one of the\r\n$2N$ particles with\r\neach equal chance $1/2N$.Then $Y_n, n\\geq 0$ is a $MC$ with state space $\\{0, 1, 2, ..., 2N\\}$.Compute the\r\n$P_{ij}$", "problempicture1": "theall/image/Example_3-1-5.PNG", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.5", "choicesb": "$1/3$", "choicesc": "None of the above is correct.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Write down the one-step transition,easily see that\r\n $$P_{ij}= P(Y_{n+1}=j|Y_n=i) = \\cases{ i/(2N) & if $j=i-1$ \\cr  1-i/(2N) & if $j=i+1$ \\cr\r\n0 & else; } $$\r\nfor $0 \\leq i \\leq j \\leq 2N$.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "wrong,another try?", "messagesuccess": "good job!", "sensitivity": 2.0, "gussingparameter": 3.0, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [3, 6], "rightproblems": [], "wrongproblems": [], "twinproblems": [34]}}, {"model": "mathematics.question", "pk": 36, "fields": {"code": "3.1.6", "category": 1, "problem": "Repeatedly toss a fair coin a number of times.  What's the expected number of\r\ntosses till the first two consecutive heads occur?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0", "choicesb": "$1/2$", "choicesc": "none of above is correct.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $\\xi_0=0$ and, for $i \\geq 1$,\r\n$$\\xi_i = \\cases{ 1 & if the $i$-th toss is a Head (with probability $1/2$) \\cr\r\n0 & if the $i$-th toss is a Tail (with probability $1/2$)}$$\r\nSet, for $n\\geq 1$,\r\n$$X_n= \\cases{ 0 & if the $n$-th toss is tail  \\cr\r\n1 & if $\\xi_{n-1}=0, \\xi_n=1$ \\cr\r\n2 & if $\\xi_{n-1}=1, \\xi_n=1$.}\r\n$$\r\nThen,\r\n $X_n$ is a $MC$ with state space $\\{0, 1, 2\\}$ and\r\n transition matrix\r\n \\begin{eqnarray*}\r\n && \\qquad \\,\\,\\, 0 \\qquad 1 \\qquad 2\\\\\r\n  P  &=& \\matrix{0\\cr 1 \\cr 2} \\pmatrix{\r\n 1/2 & 1/2 & 0 \\cr\r\n 1/2 & 0 & 1/2 \\cr\r\n 1/2 & 0 & 1/2\r\n }.\r\n \\end{eqnarray*}\r\nFor example,\r\n$$P_{00}= P(X_{n+1}=0|X_n=0)=P(\\xi_{n+1}=0|\\xi_n=0)=P(\\xi_{n+1}=0)=1/2.$$\r\nFor $k \\geq 1$, let $T_k= \\min\\{ n \\geq 0: X_{n+k}=2\\} $, which is the minimum number of\r\nadditional tosses till the first two consecutive heads occur, starting from (excluding)\r\nthe $k$-th toss. Then\r\n$$T_k=\\cases{0 &  if  $X_k=2$ \\cr\r\n T_{k+1} + 1 & if $X_k= 0$ or $1$ }.\r\n $$\r\n Let\r\n$$ w_0 = E( T_k|X_k=0) \\qquad w_1 = E(T_k |X_k=1).$$\r\nThen,\r\n\\begin{eqnarray*}\r\nw_0 &=& E(T_1 |X_1=0) = E(T_1 1_{\\{X_{1+1}=0 \\, {\\rm or} \\, 1  {\\rm or} \\, 2\\}} | X_1=0) \\\\\r\n&=& E(T_1 1_{\\{X_{2}=0 \\}} | X_1=0) + E(T_1\r\n1_{\\{X_{2}=  1\\}} | X_1=0) + E(T_1\r\n1_{\\{X_{2}=  2\\}} | X_1=0)\\\\\r\n&=&E(T_1|X_2=0, X_1=0)P(X_2=0|X_1=0)  \\\\\r\n&& + E(T_1|X_2=1, X_1=0)P(X_2=1|X_1=0)\r\n  + E(1 \\time 1_{\\{X_{2}=  2\\}} | X_1=0)\\\\\r\n  &=&E(T_2+1|X_2=0, X_1=0)P(X_2=0|X_1=0)  \\\\\r\n&& + E(T_2+1|X_2=1, X_1=0)P(X_2=1|X_1=0)\r\n  + P_{02}\\\\\r\n&=&(1+w_0)P_{00} + (1+w_1)P_{01} +  P_{02} \\\\\r\n&=& 1 + w_1P_{01}+ w_0P_{00}\r\n\\end{eqnarray*}\r\nLikewise,\r\n$$w_1 = 1 + P_{10} w_0 + P_{11} w_1. $$\r\nTogether, we have\r\n\\begin{eqnarray*}\r\n&& w_0= 1 + P_{00} w_0 + P_{01} w_1 = 1 + (1/2)(w_0+w_1) \\\\\r\n&& w_1 = 1 + P_{10} w_0 + P_{11} w_1 = 1 + (1/2) w_0 .\r\n\\end{eqnarray*}\r\nSolving the equation, we have $w_0=6$ and $w_1=4$.\r\nSince $X_1=0$ or $1$ with half chance.\r\nThe mean number of tosses till the first HH occur is\r\n$$ 1/2(w_0 + w_1) + 1 = 6.$$\r\n\r\n\r\n\r\nThe above example in fact, for the purpose of illustrating\r\n the method of first-step analysis, demonstrates a hard way of solving the problem.\r\n For this particular problem,\r\nthere is actually an easier method without invoking\r\nthe {\\it MC}  $\\{X_n, n\\geq 1 \\}$, but, rather, directly based on $\\{\\xi_i, i \\geq 1\\}$\r\n (Please DIY).\r\nThe idea is contained   in\r\nthe example called a dice game called craps presented in review and\r\nin the following example as well.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": 2.0, "gussingparameter": 3.0, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [3, 6, 9], "rightproblems": [], "wrongproblems": [7], "twinproblems": []}}, {"model": "mathematics.question", "pk": 37, "fields": {"code": "3.1.1", "category": 2, "problem": "A Markov chain $X_0$, $X_1$, ... on states $0, 1, 2$ has the transition probability matrix\r\n<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n 0.1 & 0.2   &0.7    \\cr\r\n 0.9 & 0.1   &0     \\cr\r\n 0.1  & 0.8   & 0.1\r\n}\r\n\\end{eqnarray*}\r\n\r\nand initial distribution $p_{0}$=Pr{$X_{0}$=0}=0.3, $p_{1}$=Pr{$X_{0}$=1}=0.4, and $p_{2}$=Pr{$X_{0}$=2}=0.3.   Determine\r\nPr{$X_{0}$=0,$X_{1}$=1,$X_{2}$=2}.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1", "choicesb": "0", "choicesc": "0.5", "choicesd": "0.6", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "compute,Pr{$X_{0}$=$0$,$X_{1}$=$1$,$X_{2}$=$2$}=Pr{$X_{1}$=$1$,$X_{2}$=2|$X_{0}$=0}$*$Pr{$X_{0}$=$1$}=\r\nPr{$X_{2}$=2|$X_{0}$=0,$X_{1}$=$1$}$*$Pr{$X_{1}$=1|$X_{0}$=0}$*$Pr{$X_{0}$=$1$}=$p_{12}$$*$$p_{01}$$*$$p_{0}$=0$*$0.2$*$0.3=0.<br>", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [3, 7], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 38, "fields": {"code": "3.1.2", "category": 2, "problem": "A Markov chain $X_0$;$X_1$;$...$on states $0, 1, 2$ has the transition probability matrix\r\n\\begin{eqnarray*}\r\n && \\quad \\quad  \\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n0.7& 0.2   &0.1    \\cr\r\n0   & 0.6  &0.4     \\cr\r\n0.5 & 0    & 0.5\r\n}\r\n\\end{eqnarray*}\r\n\r\nDetermine the conditional probability,Pr{$X_{2}$=1,$X_{3}$=1|$X_{1}$=0} and Pr{$X_{1}$=1,$X_{2}$=1|$X_{0}$=0}", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1; 1", "choicesb": "0.06; 0.06", "choicesc": "0.06;  1", "choicesd": "1; 0.06", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Throughout the course, the Markov process are with stationary transition probabilities, meaning that the transition does not have to do with beginning, but only have to do with the time steps of transition. By the definition, Pr{$X_{2}$=$1$,$X_{3}$=$1$|$X_{1}$=$0$}=Pr{$X_{3}$=$1$|$X_{1}$=$0$,$X_{2}$=$1$}$*$Pr{$X_{2}$=$1$|$X_{1}$=0}=Pr{$X_{3}$=$1$|$X_{2}$=$1$}$*$Pr{$X_{2}$=$1$|$X_{1}$=0}=$p_{1,1}$$*$$p_{0,1}$=0.1$*$0.6=0.06. \r\n<br>\r\nThe same way, we can get  Pr{$X_{1}$=1,$X_{2}$=1|$X_{0}$=0}=0.06.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [3, 7], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 39, "fields": {"code": "3.1.3", "category": 2, "problem": "A Markov chain $X_0$;$X_1$;$...$ on states $0, 1, 2$ has the transition probability matrix\r\n<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n0.6& 0.3   &0.1    \\cr\r\n 0.3& 0.3  &0.4     \\cr\r\n0.4   & 0.1       & 0.5\r\n}\r\n\\end{eqnarray*}\r\n\r\nIf it is known that the process starts in state $X_{0}$=1,Determine the  probability,Pr{$X_{0}$=1,$X_{1}$=0,$X_{2}$=2}.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.3", "choicesb": "0.03", "choicesc": "0.1", "choicesd": "0.2", "choicese": "none of the above", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "This is essentially two-step transition probabilities of a Markov chain. Pr{$X_{0}$=$1$,$X_{1}$=$0$,$X_{2}$=$2$}=Pr{$X_{2}$=$2$|$X_{1}$=$0$,$X_{0}$=$1$}$*$Pr{$X_{1}$=$0$|$X_{0}$=$1$}=Pr{$X_{2}$=$2$|$X_{1}$=$0$}$*$Pr{$X_{1}$=$0$|$X_{0}$=$1$}=$p_{1 0}$$*$$p_{0 2}$=0.3$*$0.1=0.03.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [3, 7], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 40, "fields": {"code": "3.1.4", "category": 2, "problem": "A Markov chain $X_0$;$X_1$;$...$ on states $0, 1, 2$ has the transition probability matrix\r\n<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n0.1& 0.1   &0.8   \\cr\r\n  0.2& 0.2  &0.6    \\cr\r\n 0.3   & 0.3   & 0.4\r\n}\r\n\\end{eqnarray*}\r\n\r\nDetermine the conditional probability, Pr{$X_{1}$=1,$X_{2}$=1|$X_{0}$=0} and Pr{$X_{2}$=1,$X_{3}$=1|$X_{1}$=0}", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.02; 0.03", "choicesb": "0.03; 0.03", "choicesc": "0.03; 0.02", "choicesd": "0; 1", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "This is again about the stationary transition probabilities.Pr{$X_{2}$=$1$,$X_{3}$=$1$|$X_{1}$=$0$}=Pr{$X_{3}$=$1$|$X_{1}$=$0$,$X_{2}$=$1$}$*$Pr{$X_{2}$=$1$|$X_{1}$=0}=Pr{$X_{3}$=$1$|$X_{2}$=$1$}$*$Pr{$X_{2}$=$1$|$X_{1}$=0}=$p_{1,1}$$*$$p_{0,1}$=0.1$*$0.2=0.02.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [3, 7], "rightproblems": [], "wrongproblems": [127], "twinproblems": [127]}}, {"model": "mathematics.question", "pk": 41, "fields": {"code": "3.1.5", "category": 2, "problem": "A Markov chain $X_0$;$X_1$;$...$on states $0, 1, 2$ has the transition probability matrix\r\n<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n0.3& 0.2   &0.5  \\cr\r\n0.5& 0.1  &0.4    \\cr\r\n0.5   & 0.2   & 0.3\r\n}\r\n\\end{eqnarray*}\r\nand initial distribution $p_{0}$=0.5,$p_{1}$=0.5.\r\nDetermine the  probability,Pr{$X_{1}$=1,$X_{2}$=1,$X_{0}$=0} and Pr{$X_{2}$=1,$X_{3}$=1,$X_{1}$=0}", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.01;  0.008", "choicesb": "0.01;  0.02", "choicesc": "0.008; 0.01", "choicesd": "0.02; 0.03", "choicese": "none of the above", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Pr{$X_{1}$=1,$X_{2}$=1,$X_{0}$=0} is   $p_{0}$$*$$p_{0 1}$$*$$p_{1 1}$=0.5$*$0.2$*$0.1=0.01.\r\n<br>\r\nOn the other hand, Pr{ $X_1$=0} =  $p_{0}$$*$$p_{0 0}$+ $p_{1}$$*$$p_{1 0}$ =0.4.\r\nThen, \r\nPr{$X_{2}$=1,$X_{3}$=1,$X_{1}$=0}=0.4$*$0.2$*$0.1=0.008.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [3, 7], "rightproblems": [127], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 42, "fields": {"code": "3.2.1", "category": 2, "problem": "A Markov chain {$X_{n}$} on the states 0;1;2 has the transition probability matrix\r\n<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n0.1& 0.2   &0.7  \\cr\r\n0.2& 0.2  &0.6    \\cr\r\n0.6 & 0.1    & 0.3\r\n}\r\n\\end{eqnarray*}\r\nWhat is the Pr{$X_{3}$=$1$|$X_{1}=0$}  and Pr{$X_{3}$=$1$|$X_{0}=0$} ?<br>\r\nhint: try compute the second step trasition matrix first.\r\nsolution:", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.13;0.16", "choicesb": "0.16;0.13", "choicesc": "0.13; 0.13", "choicesd": "0.16; 0.16", "choicese": "none of the above", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "By the definition<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P^{2}}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n0.47& 0.13  &0.4  \\cr\r\n0.42& 0.14  &0.44    \\cr\r\n 0.26 & 0.17      & 0.57\r\n}\r\n\\end{eqnarray*}\r\nSo that Pr{$X_{3}$=$1$|$X_{1}=0$}=$P_{0 1}^{(2)}$=0.13; <br>\r\nPr{$X_{3}$=$1$|$X_{0}=0$} =$\\sum_{i=0}^{2}$Pr{$X_{3}$=$1$,$X_{1}$=$i$|$X_{0}=0$}=$\\sum_{i=0}^{2}$Pr{$X_{3}$=$1$|$X_{0}=0$,$X_{1}$=$i$}$*$\r\nPr{$X_{1}$=$i$|$X_{0}$=$0$} =$\\sum_{i=0}^{2}$Pr{$X_{3}$=$1$|$X_{1}$=$i$}$*$\r\nPr{$X_{1}$=$i$|$X_{0}$=$0$}=$\\sum_{i=0}^{2}$$P_{i 0}^{(2)}$$*$\r\nPr{$X_{1}$=$i$|$X_{0}$=$0$}=0.16.", "linkability1": 1.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [3, 5], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 43, "fields": {"code": "3.2.2", "category": 2, "problem": "A particle moves among the states 0,1,2 according to a Markov process with  transition probability matrix\r\n<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n0& 0.5   &0.5  \\cr\r\n0.5& 0  &0.5    \\cr\r\n0.5 & 0.5      & 0\r\n}\r\n\\end{eqnarray*}\r\nLet $X_{n}$ denote the position of the particle at the $n$th move. Calculate Pr{$X_{4}$=$0$|$X_{0}$=0}?<br>\r\nhint: try compute the n step trasition matrix first.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.375", "choicesb": "0.3125", "choicesc": "0.25", "choicesd": "0.5", "choicese": "0.4", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "By the definition<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad\\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad\\quad1\\quad\\quad\\quad2\\\\\r\n {\\bf P^{4}}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n0.375& 0.3125  &0.3125  \\cr\r\n0.3125& 0.375  &0.3125    \\cr\r\n0.3125 & 0.3125      & 0.375\r\n}\r\n\\end{eqnarray*}\r\nSo that Pr{$X_{4}$=$0$|$X_{0}$=$0$}=0.375<br>\r\n\r\nAdditional challenge: What if I combine states 1 and 2 together to form one single state, say state\r\nN. Will it become a two state M.C. with transition prob matrix\r\n \\begin{eqnarray*}\r\n && \\quad \\quad  \\,\\,0\\quad \\quad N \\\\\r\n {\\bf P }&=& \\matrix{0 \\cr N    }\r\n  \\pmatrix{\r\n0 & 1    \\cr\r\n0.5& 0.5  \r\n}\r\n\\end{eqnarray*}\r\nThis is a big question! Think it over! If it is true, then the problem can be solved in \r\na much simpler way, as it becomes a problem with two states only.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [3, 5, 7], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 44, "fields": {"code": "3.2.3", "category": 2, "problem": "A Markov chain $X_0$, $X_1$, .... with transition probability matrix\r\n<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,0\\quad\\,\\,\\,1\\quad\\,\\,\\,2\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n0.7& 0.2   &0.1  \\cr\r\n  0& 0.6  &0.4    \\cr\r\n 0.5 & 0  & 0.5\r\n}\r\n\\end{eqnarray*}\r\nCalculate Pr{$X_{4}$=$1$|$X_{0}$=0}and Pr{$X_{3}$=$1$|$X_{0}$=0} ?<br>\r\nhint: try compute the $n$ step transition matrix first.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.254; 0.264", "choicesb": "0.274; 0.234", "choicesc": "0.234; 0.274", "choicesd": "0.254; 0.264", "choicese": "none of above is correct", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Another silly question. Use matrix product or figure out the path. \r\nVery boring exercise.  <br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad\\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad\\quad1\\quad\\quad\\quad2\\\\\r\n {\\bf P^{4}}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n0.4636& 0.254  &0.2824  \\cr\r\n 0.444& 0.2256  &0.3304    \\cr\r\n0.524 & 0.222     & 0.254\r\n}\r\n\\end{eqnarray*}\r\nSo that Pr{$X_{4}$=$1$|$X_{0}$=$0$}=0.254<br>\r\nIn the same way we can get Pr{$X_{3}$=$1$|$X_{0}$=$0$}=0.264\r\nanother way is using the slicing universe,try it.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [3, 5], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 45, "fields": {"code": "3.2.4", "category": 2, "problem": "A Markov chain $X_0$, $X_1$, $\\ldots$ ,who has the transition probability matrix\r\n<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n0.6& 0.3   &0.1  \\cr\r\n0.3& 0.3  &0.4    \\cr\r\n0.4 & 0.1     & 0.5\r\n}\r\n\\end{eqnarray*}\r\nIf it is known that the process starts in state $X_{0}$=1,determine the probability Pr{$X_{2}$=2}?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.54", "choicesb": "0.49", "choicesc": "none of the rest is correct", "choicesd": "0.3", "choicese": "0.34", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "By the definition<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad\\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P^{2}}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n 0.49& 0.28  &0.23  \\cr\r\n  0.43& 0.22  &0.35    \\cr\r\n  0.47& 0.22      & 0.33\r\n}\r\n\\end{eqnarray*}\r\nSo that Pr{$X_{2}$=$2$}=Pr{$X_{2}$=$2$|$X_{0}$=$1$}=0.35<br>", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [3, 7], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 46, "fields": {"code": "3.2.5", "category": 2, "problem": "A Markov chain $X_0$;$X_1$; : : :  who has the transition probability matrix\r\n<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n0.1& 0.1   &0.8  \\cr\r\n0.2& 0.2  &0.6    \\cr\r\n 0.3 & 0.3    & 0.4\r\n}\r\n\\end{eqnarray*}\r\ndetermine the conditional probability Pr{$X_{3}$=$1$|$X_{1}$=$0$} and Pr{$X_{2}$=$1$|$X_{0}$=$0$} ?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.32; 0.35", "choicesb": "0.27; 0.27", "choicesc": "0.24;  0.23", "choicesd": "0.33; 0.66", "choicese": "none of the above", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "One more silly and boring problem. (Do many of problems of this kind, we could become \r\nsillier than sharper). <br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad\\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P^{2}}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n0.27& 0.27  &0.46  \\cr\r\n0.24& 0.24  &0.52    \\cr\r\n 0.21& 0.21      & 0.58\r\n}\r\n\\end{eqnarray*}\r\nSo that Pr{$X_{3}$=$1$|$X_{1}$=$0$} =Pr{$X_{2}$=$1$|$X_{0}$=$0$}=$p_{0 1}^{2}$=0.27<br>\r\nNote that we already use that transition probabilities do not depend on beginning time!", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [3, 5], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 47, "fields": {"code": "3.2.6", "category": 2, "problem": "A Markov chain $X_0$;$X_1$;$...$on states $0, 1, 2$ has the transition probability matrix\r\n<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n0.3& 0.2   &0.5  \\cr\r\n   0.5& 0.1  &0.4    \\cr\r\n   0.5   & 0.2      & 0.3\r\n}\r\n\\end{eqnarray*}\r\nand initial distribution $p_{0}$=0.5,$p_{1}$=0.5.\r\nDetermine the  probability,Pr{$X_{3}$=0} and Pr{$X_{2}$=0}?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.42; 0.416", "choicesb": "0.42; 0.181", "choicesc": "0.416; 0.42", "choicesd": "0.181; 0.42", "choicese": "none of above", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Again and again, we work on the silly exercises. \r\nSlice the universe:\r\nPr{$X_{3}$=$0$}=Pr{$X_{3}$=$0$,$X_{0}$=$0$}+Pr{$X_{3}$=$0$,$X_{0}$=$1$}=Pr{$X_{3}$=$0$|$X_{0}$=$0$}$*$Pr{$X_{0}$=$0$}+Pr{$X_{3}$=$0$|$X_{0}$=$1$}$*$Pr{$X_{0}$=$1$}=$p_{0 0}^{(3)}$$*$$p_{0}$+$p_{1 0}^{(3)}$$*$$p_{1}$=\r\n0.412$*$0.5+0.42$*$0.5=0.416.\r\nThe same way we get Pr{$X_{2}$=$0$}=$p_{0 0}^{(2)}$$*$$p_{0}$+$p_{1 0}^{(2)}$$*$$p_{1}$=\r\n0.44$*$0.5+0.$*$0.5=0.42.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [3, 5, 7], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 48, "fields": {"code": "3.1.2", "category": 4, "problem": "For three events A, B and C,  we have the following three statements\r\n: (i) $P(A\\cap B|C)= P(A|C)P(B|C)$; (ii). $P(A|C \\cap B) = P(A|C)$;\r\nand (iii) $P(B|A\\cap C) = P(B|C)$. (Assume all quantities here are well defined.) Which of them are equivalent?\r\nNotice that statement (i) says that A and B are conditionally independent given C.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "(i) and (ii)", "choicesb": "(ii) and (iii)", "choicesc": "(i) and (iii)", "choicesd": "(i) and (ii) and (iii)", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "(i) $\\Longrightarrow$ (ii). Write\r\n$$P(A|C\\cap B)={ P(A \\cap C \\cap B) \\over P(C \\cap B)}=\r\n{ P(A \\cap B|C)P(C)  \\over P(B|C)P(C)  }= {P(A|C)P(B|C) \\over P(B|C)}= P(A|C).$$\r\n (ii) $\\Longrightarrow$ (iii). Write\r\n$$P(B|A\\cap C)={ P(A \\cap C \\cap B) \\over P(A \\cap C)}=\r\n{ P(A | B \\cap C)P(B \\cap C)  \\over P(A|C)P(C)  }= {P(A|C)P(B|C)P(C) \\over P(A|C)P(C)}= P(B|C).$$\r\n(iii) $\\Longrightarrow$ (i). Write\r\n$$P(A\\cap B | C)={ P(A \\cap B \\cap C) \\over P( C)}=\r\n{ P(B| A\\cap C)P(A\\cap C)  \\over  P(C)  }= {P(B|C)P(A|C) P(C) \\over P( C)}= P(A|C)P(B|C).$$\r\n\r\n\r\nRemark. In (i), $A$ and $B$ are exchangeable. Therefore, if (i) is equivalent to (ii), it must\r\nbe equivalent to (iii).", "linkability1": 3.0, "linkability2": 0.0, "linkability3": 2.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Alas! Please check the definition of conditional independence.", "messagesuccess": "Congratulations! You have mastered the definition of conditional independence.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [7], "rightproblems": [], "wrongproblems": [3], "twinproblems": []}}, {"model": "mathematics.question", "pk": 49, "fields": {"code": "3.1.4", "category": 4, "problem": "Suppose $\\{X_n: n= \\cdots -2, -1, 0, 1, 2, \\cdots\\}$\r\nis a discrete time $MC$ with the time being  all integers from\r\n$-\\infty$ to $\\infty$. Let $Y_n = X_{-n}$ for all integers $n$.\r\nIs $\\{Y_n: n= \\cdots -2, -1, 0, 1, 2, \\cdots\\}$ a $MC$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes", "choicesb": "No", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Yes. Write\r\n\\begin{eqnarray*}\r\n && P(\\{Y_{n+1}\\in A_1, Y_{n+2}\\in A_2, ... \\} \\cap \\{ Y_{n-1} \\in B_1 , Y_{n-2} \\in B_2 ,...\\}\r\n|Y_n=j)\r\n\\\\\r\n&=& P(\\{X_{-n-1}\\in A_1, X_{-n-2}\\in A_2, ... \\} \\cap \\{ X_{-n+1} \\in B_1 , X_{-n+2} \\in B_2 ,...\\}\r\n|X_{-n}=j)\r\n\\\\\r\n&=& P(\\{X_{-n-1}\\in A_1, X_{-n-2}\\in A_2, ... \\} |  X_{-n}=j)\r\nP(\\{ X_{-n+1} \\in B_1 , X_{-n+2} \\in B_2 ,...\\}\r\n|X_{-n}=j)\r\n \\\\\r\n&=& P(\\{Y_{ n+1}\\in A_1, Y_{ n+2}\\in A_2, ... \\} |  Y_{n}=j)\r\nP(\\{ Y_{n-1} \\in B_1 , Y_{n-2} \\in B_2 ,...\\}\r\n|Y_{n}=j)\r\n\\end{eqnarray*}\r\nProving the conditional independence of the past and future given a fixed state of the present.", "linkability1": 3.0, "linkability2": 0.0, "linkability3": 2.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Alas! Please check the definition of markov process.", "messagesuccess": "Congratulations! You have mastered the definition of markov process.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [2, 3, 4], "rightproblems": [3], "wrongproblems": [], "twinproblems": [3]}}, {"model": "mathematics.question", "pk": 50, "fields": {"code": "3.4.2", "category": 4, "problem": "In Example 3.7,   a\r\n$MC$ based on patterns of length 2 with 4 states is constructed\r\nto solve the problem. If Mr Y picks\r\nthe pattern HHH, what is the chance of Mr Z to win\r\n by picking THH? You may construct a $MC$ based on patterns of\r\nlength 3 (rather than 2) with 8 states to\r\ncalculate.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "5/8", "choicesb": "6/8", "choicesc": "7/8", "choicesd": "1", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $\\{X_n: n\\geq 3\\}$ be the MC with state space being\r\n the eight patterns of length 3:\r\n $$\\hbox{\\{ HHH (0), THH (1),  HTH (2), TTH (3),\r\n HHT (4) , THT (5), HTT (6), TTT (7) \\}.}$$\r\n and transition probability matrix:\r\n \\begin{eqnarray*}\r\n&& \\qquad  \\qquad \\qquad  0\\quad \\, 1 \\quad \\, 2 \\quad\\,\r\n 3\\quad \\,\\,  4 \\quad \\,\\, 5 \\quad \\, 6 \\quad \\,\\, 7 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{HHH (0)}  \\cr\r\n\\hbox{THH (1)} \\cr  \\hbox{HTH (2)} \\cr \\hbox{TTH (3)}\\cr\r\n\\hbox{HHT (4)} \\cr  \\hbox{THT (5)} \\cr\r\n\\hbox{HTT (6)}\\cr \\hbox{TTT (7)} }\r\n\\pmatrix{\r\n.5   & 0   & 0 & 0 & .5 & 0 & 0 & 0   \\cr\r\n.5 & 0 & 0 & 0 & .5 & 0 & 0 &0  \\cr\r\n0 & .5 & 0 & 0 & 0 & .5 & 0 & 0   \\cr\r\n0 & .5 & 0 & 0 & 0 & .5 & 0 & 0  \\cr\r\n0 & 0 & .5&  0 & 0 & 0 & .5 & 0   \\cr\r\n0 & 0 & .5&  0 & 0 & 0 & .5 & 0   \\cr\r\n0 & 0 & 0 & .5 & 0 & 0 &0 & .5   \\cr\r\n0 & 0 & 0 & .5 & 0 & 0 &0 & .5\r\n}\r\n\\end{eqnarray*}\r\n\r\nLet $p_i$ be the chance that THH occur before HHH, beginning with state\r\n$i$, ($X_3=i$).\r\nThen,\r\n$$p_0=0, \\quad p_1=1, \\quad p_i =  \\sum_{j=0}^7 P_{ik}p_k, \\quad \\hbox{for $i\\not=0, 1$}.$$\r\nSolving the equations, we have\r\n$$p_2= p_3= p_4= p_5 =  p_6=  p_7=1.$$\r\nSince $P(X_3=i)=1/8$ for $i=0,...,7$, the chance that Mr Z wins is\r\n$$1/8\\times (p_0+p_1 + \\cdots+p_7)= 7/8.$$\r\n\r\n\r\nRemark. Directly solving the above 6 equations appears to be too much of\r\ncomputation. But a quick look shall find that $p_2=p_3$, $p_4=p_5$\r\nand $p_6=p_7$. Thus the number of equations is actually only three.\r\n(More astute observation further leads to $p_4=p_5=p_6=p_7$, which reduces the\r\nnumber of equations to two.)", "linkability1": 3.0, "linkability2": 2.0, "linkability3": 3.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Reread Example 3.7 might be helpful!", "messagesuccess": "Congratulations! You have solved a difficult problem of first step analysis!", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [7, 130], "wrongproblems": [], "twinproblems": [7]}}, {"model": "mathematics.question", "pk": 51, "fields": {"code": "3.4.4", "category": 4, "problem": "A rabbit has three dens A, B and C.  If it stays in A for a night,\r\nthe following night it's always in B. If it stays in B or C for a night,\r\nthe following day,  it takes equal chance 1/3 to\r\neither continue to stay or go to one of the other two dens for the night.\r\nA wolf, trying to hunt down the rabbit, has a different pattern.\r\nEvery day, it takes chance .8 to go clockwise to the next den\r\nand  chance .2 to go anti-clockwise the other den for the night-stay.\r\nThe rabbit would be eaten by the wolf the night they are in the same den.\r\nSuppose at the night of day 0, the rabbit is in Den A and the wolf\r\nin Den B.   What is the mean life time in days of the rabbit?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "3.78", "choicesb": "2.54", "choicesc": "3.96", "choicesd": "4.21", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $X_n $ represent the den the rabbit stays for $n$-th night\r\nfollowed by the den the wolf stays for the $n$-th night, assuming the wolf and rabbit\r\nalways stay in the den forever once they are in the same den.\r\nThen, $\\{X_n: n\\geq 0\\}$ is a {\\it MC} with nine states\r\n$$ \\{ \\hbox{ AA (0) , AB (1), AC (2), BA (3), BB (4), BC (5), CA (6), CB (7), CC (8)}, \\}$$\r\nand\r\nwith transition probability matrix\r\n\\begin{eqnarray*}\r\n&& \\qquad\\quad AA \\quad \\,\\, AB\\quad \\,\\, AC\\quad \\,\\, BA\\quad \\,\\, BB\\quad \\,\\, BC\\quad \\,\r\n CA\\quad \\, CB\\quad \\,\\, CC \\\\\r\n{\\bf P} &=& \\matrix{AA \\cr AB \\cr AC \\cr BA \\cr BB \\cr BC\\cr CA \\cr CB \\cr CC}\r\n\\pmatrix{\r\n1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\cr\r\n0 & 0 & 0 & .8 & 0& .2 & 0 &0 & 0 \\cr\r\n0 & 0 & 0 & .2 & .8 & 0& 0 & 0 & 0 \\cr\r\n0 & .067 & .266 & 0 & .067 & .266 &0 & .067 & .266 \\cr\r\n0 & 0 & 0&  0 & 1 & 0 & 0 & 0 &0 \\cr\r\n.067&.266 & 0 & .067&.266 & 0  & .067&.266 & 0 \\cr\r\n0 & .067 & .266 & 0 & .067 & .266 &0 & .067 & .266 \\cr\r\n.266&0 &.067 & .266&0 & .067   & .266&0 & .067  \\cr\r\n0 & 0 & 0&  0 & 0 & 0 & 0 & 0 &1\r\n}\r\n\\end{eqnarray*}\r\nLet $w_i $ be the mean number of steps to reach AA, BB or CC, starting from\r\nstate $i$.\r\nThen\r\n$$w_0=w_4=w_8=0, \\qquad w_i = 1 + \\sum_{k=0}^8 P_{ik}w_i \\quad \\hbox{for $i\\not=0, 4, 8.$}$$\r\nSolve the equations,\r\n$$w_1= 3.78, \\,\\,\r\nw_2= 1.53, \\,\\,\r\nw_3= 2.69, \\,\\,\r\nw_5= 3.14, \\,\\,\r\nw_6= 2.69, \\,\\,\r\nw_7= 2.89 $$\r\nThe mean life time of rabbit is $w_1=3.78$ days ($X_0=1=$AB).", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "You may have a look at example 3.7?", "messagesuccess": "Congratulations! You mastered first step analysis!", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [9, 13], "rightproblems": [8, 129], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 52, "fields": {"code": "3.9.1", "category": 4, "problem": "Is it possible that $E(X_n) \\uparrow \\infty$ geometrically\r\nfast, and the chance of extinction is still positive?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes, it is possible.", "choicesb": "No, it is not possible.", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "$E(X_n) = \\mu^n$. Choose $\\xi $ such that\r\n$P(\\xi=2)=3/4$ and $P(\\xi=0)=1/4$, so that $\\mu = 3/2 > 1$ and\r\n$E(X_n)   \\uparrow \\infty$ geometrically. But, on the other hand,\r\n$\\phi(s) = 1/4 + 3/4s^2$ and $s=\\phi(s)$ has a solution $1/3$.\r\nTherefore $\\mu_\\infty = 1/3$ which is positive.", "linkability1": 3.0, "linkability2": 0.0, "linkability3": 2.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! You may check the definition of extinction probability.", "messagesuccess": "Congratulations! You mastered it!", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [16, 21], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 53, "fields": {"code": "3.9.3", "category": 4, "problem": "Suppose $P(\\xi\\geq 4)=0$, and  $P(\\xi=i)=p_i$ for\r\n$i=0,..., 3$. If we express $u_\\infty$ in terms of $p_0,...,p_3$, how many situations are there in total?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1", "choicesb": "3", "choicesc": "4", "choicesd": "5", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Consider equation $s=\\phi(s)$, which can be written as\r\n$$ p_3s^3+p_2s^2 + (p_1-1)s +p_0=0.$$\r\nAs $1$ is always a solution. The equation is\r\n$$   (s-1)(p_3 s^2 +(p_2+p_3)s  -p_0)=0$$\r\nSolving the equations, we have\r\n$$u_\\infty = \\cases { 0 &  if $p_0=0$;\r\n\\cr\r\n1 & if $p_0>0, p_2=p_3=0$;\r\n\\cr\r\n \\min(1, p_0/p_2) & if $p_0>0, p_2>0, p_3=0$;\r\n \\cr\r\n(1/2p_3) \\{ -p_2-p_3 + \\sqrt{ (p_2+p_3)^2 + 4p_0p_3} \\}&\r\nif $p_0>0, p_3>0 $ and $p_0 \\leq 2p_3+p_2$ \\cr\r\n1 & if $p_0>0, p_3>0 $ and $p_0 > 2p_3+p_2$.}\r\n$$", "linkability1": 3.0, "linkability2": 0.0, "linkability3": 2.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "You may check the definition of extinction problem.", "messagesuccess": "Great! You have considered all possibilities!", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [20, 21, 22], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 54, "fields": {"code": "3.4.5", "category": 4, "problem": "{Mickey in Maze with an exit.}\r\nSuppose now there is an exit in cell 2 (See the graph below). Assume Mickey moves around in the same way\r\nas before, except that, once in cell 2, he has chance 0.5 to get out and never return\r\nand chance equal chance 1/4 to go to cell 1 or 5.\r\nSuppose Mickey begins in cell 6, what's the mean number of visits of cell 1?", "problempicture1": "theall/image/DIY-3.4.5-1.PNG", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "6.", "choicesb": "5.", "choicesc": "4.", "choicesd": "3.", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $s_i$ be the mean number of visits of cell 1 beginning from\r\ncell $i$, $including$ the beginning state if it is 1.\r\nThen,\r\n\\begin{eqnarray*}\r\n&&s_0 = 1/2(s_1+s_3) \\qquad  \\qquad\r\ns_1= 1+ 1/3(s_0+s_2+s_4) \\qquad \\quad\r\ns_2= 1/4(s_1+s_5) \\\\\r\n&&s_3=1/3(s_0+s_4+s_6) \\qquad s_4=1/4(s_1+s_3+s_5+s_7) \\qquad  \\,\\,  s_5=1/3(s_2+s_4+s_8) \\\\\r\n&&s_6=1/2(s_3+s_7) \\qquad \\qquad s_7=1/3(s_4+s_6+s_8 ) \\qquad \\qquad \\,\\, \\,  s_8=1/2(s_5+s_7)\r\n\\end{eqnarray*}\r\nSolve this equation, we have\r\n$$s_0=3.375,\\,\\, s_1= 3.625,\\,\\, s_2=1.5,\\,\\, s_3=3.125,\\,\\, s_4=3,\\,\\, s_5=2.375,\\,\\,s_6=3,\\,\\,\r\n s_7=2.876,\\,\\, s_8=2.625.$$\r\n The mean number of visits of state 1 beginning from state 6 is 3.\r\n<br><br>\r\nRemark. \r\n<br>\r\nThe above exercise is a little tedious in solving the nine linear equations.\r\nIn the exam, the number equations would be much smaller. For this problem, no symmetry\r\ncan be used to shorten number of equations. If, instead, the problem is to\r\n calculate the mean number of times the MC visits state 4, beginning from\r\nstate 6, symmetry can be used to eliminate three of the nine equations.", "linkability1": 3.0, "linkability2": 3.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "Let $a_i$ be the mean number of visits of cell 1 beginning from\r\ncell $i$, $excluding$ the beginning state if it is 1.\r\n Then,\r\n\\begin{eqnarray*}\r\n&&a_0 = 1/2(a_1+1 +a_3) \\qquad\r\n\\,\\, a_1=   1/3(a_0+a_2+a_4) \\qquad \\qquad \\qquad\r\na_2= 1/4(a_1+1 +a_5) \\\\\r\n&&a_3=1/3(a_0+a_4+a_6) \\qquad a_4=1/4(a_1+1+a_3+a_5+a_7) \\qquad a_5=1/3(a_2+a_4+a_8) \\\\\r\n&&a_6=1/2(a_3+a_7) \\qquad\\qquad  a_7=1/3(a_4+a_6+a_8 ) \\qquad\\qquad \\qquad a_8=1/2(a_5+a_7)\r\n\\end{eqnarray*}\r\nThe equations are the same as those of Method 1 with $a_1+1=s_1$ and $a_i=s_i$ for $i\\not=1$.\r\nWe get the same solutions, (except for $a_1=s_1-1=2.625$).", "messagefailure": "Oops, try again. Get yourself familiar with the application of first step analysis!", "messagesuccess": "Well done! You have mastered the concept of first step analysis!", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [3, 5, 9, 15], "rightproblems": [5], "wrongproblems": [4], "twinproblems": [4]}}, {"model": "mathematics.question", "pk": 55, "fields": {"code": "3.9.2", "category": 4, "problem": "How is $u_\\infty$ related to $s=\\phi(s)$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "The larger solution of $s=\\phi(s)$ on $[0, \\infty]$ is $u_\\infty$.", "choicesb": "The smaller solution of $s=\\phi(s)$ on $[0, \\infty]$ is $u_\\infty$.", "choicesc": "There is no certain answers.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $a$ be a solution of\r\n$s=\\phi(s)$ on\r\n$[0, 1]$. Since $1$ is always a solution, $a$ is well defined.\r\nObserve that $u_1=p_0=\\phi(0)$. Since $\\phi(\\cdot)$ is\r\nnondecreasing, we have\r\n$a =\\phi(a) \\geq \\phi(0) = u_1$. For any $n \\geq 1$, assume now\r\n$a \\geq u_n$. Then\r\n$a =\\phi(a) \\geq \\phi(u_n) = u_{n+1}$.\r\nBy induction, it follows that\r\n$a \\geq u_n$ for all $n \\geq 1$.\r\nTaking limit, we know\r\n$a \\geq u_\\infty$.\r\nConsequently, $u_\\infty$ must be the smallest solution\r\non $[0, 1]$ of\r\nthe equation $s=\\phi(s)$.", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 3.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Get yourself familiar with probability generating function!", "messagesuccess": "Well done! You have mastered the concepts of probability generating function and branching process!", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 5, "calculateddifficulty": null, "linkneuron": [19, 20], "rightproblems": [131, 132], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 56, "fields": {"code": "3.1.1", "category": 3, "problem": "A simplified model for the spread of a disease goes this way: The\r\ntotal population size is $N=5$, of which some are diseased and the remainder\r\nare healthy. During any single period of time, two people are selected\r\nat random from the population and assumed to interact. The selection\r\nis such that an encounter between any pair of individuals in the\r\npopulation is just as likely as between any other pair. If one of these persons\r\nis diseased and the other not, then with probability $\\alpha=0.1$ the disease\r\nis transmitted to the healthy person. Otherwise, no disease transmission\r\ntakes place. Let $X_n$, denote the number of diseased persons in the\r\npopulation at the end of the $n$th period. Specify the transition probability\r\nmatrix.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "\\[\r\n\\begin{array}{c cccccc }\r\n\\,\\,\\,&0 \\,\\,\\,\\,\\,\\,\\,&1\\,\\,\\,\\,\\,\\,\\,&2\\,\\,\\,\\,\\,\\,\\,\r\n&3\\,\\,\\,\\,\\,\\,\\,&4\\,\\,\\,\\,\\,\\,\\,\r\n&5\\,\\,\\,\\,\\,\\,\\, \\\\\r\n\\end{array}\r\n\\]\r\n\\[\r\n\\begin{array}{c||cccccc||}\r\n0&1&0&0&0&0&0\\\\\r\n1&0&0.96&0.04&0&0&0\\\\\r\n2&0&0&0.94&0.06&0&0\\\\\r\n3&0&0&0&0.94&0.06&0\\\\\r\n4&0&0&0&0&0.96&0.04\\\\\r\n5&0&0&0&0&0&1\\\\\r\n\\end{array}\r\n\\]", "choicesb": "\\[\r\n\\begin{array}{c cccccc }\r\n\\,\\,\\,&0 \\,\\,\\,\\,\\,\\,\\,&1\\,\\,\\,\\,\\,\\,\\,&2\\,\\,\\,\\,\\,\\,\\,\r\n&3\\,\\,\\,\\,\\,\\,\\,&4\\,\\,\\,\\,\\,\\,\\,\r\n&5\\,\\,\\,\\,\\,\\,\\, \\\\\r\n\\end{array}\r\n\\]\r\n\\[\r\n\\begin{array}{c||cccccc||}\r\n0&0.96&0.04&0&0&0&0\\\\\r\n1&0&0.96&0.04&0&0&0\\\\\r\n2&0&0&0.94&0.06&0&0\\\\\r\n3&0&0&0&0.94&0.06&0\\\\\r\n4&0&0&0&0&0.96&0.04\\\\\r\n5&0&0&0&0&0&1\\\\\r\n\\end{array}\r\n\\]", "choicesc": "\\[\r\n\\begin{array}{c cccccc }\r\n\\,\\,\\,&0 \\,\\,\\,\\,\\,\\,\\,&1\\,\\,\\,\\,\\,\\,\\,&2\\,\\,\\,\\,\\,\\,\\,\r\n&3\\,\\,\\,\\,\\,\\,\\,&4\\,\\,\\,\\,\\,\\,\\,\r\n&5\\,\\,\\,\\,\\,\\,\\, \\\\\r\n\\end{array}\r\n\\]\r\n\\[\r\n\\begin{array}{c||cccccc||}\r\n0&0&0&0&0&0&0\\\\\r\n1&0&0.96&0.04&0&0&0\\\\\r\n2&0&0&0.94&0.06&0&0\\\\\r\n3&0&0&0&0.94&0.06&0\\\\\r\n4&0&0&0&0&0.96&0.04\\\\\r\n5&0&0&0&0&0&1\\\\\r\n\\end{array}\r\n\\]", "choicesd": "\\[\r\n\\begin{array}{c cccccc }\r\n\\,\\,\\,&0 \\,\\,\\,\\,\\,\\,\\,&1\\,\\,\\,\\,\\,\\,\\,&2\\,\\,\\,\\,\\,\\,\\,\r\n&3\\,\\,\\,\\,\\,\\,\\,&4\\,\\,\\,\\,\\,\\,\\,\r\n&5\\,\\,\\,\\,\\,\\,\\, \\\\\r\n\\end{array}\r\n\\]\r\n\\[\r\n\\begin{array}{c||cccccc||}\r\n0&1&0&0&0&0&0\\\\\r\n1&0&0.96&0.04&0&0&0\\\\\r\n2&0&0&0.94&0.06&0&0\\\\\r\n3&0&0&0&0.94&0.06&0\\\\\r\n4&0&0&0&0&0.96&0.04\\\\\r\n5&0.04&0&0&0&0&0.96\\\\\r\n\\end{array}\r\n\\]", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "State i: Number of people infected<br>\r\nType of Interaction that more people get infected($i\\to i+1$): One infected and one uninfected<br>\r\nProbability of such kind of interaction:\r\n$\\binom{i}{1}\\binom{N-i}{1}/\\binom{N}{2}$<br>\r\nTherefore,\r\n\\begin{align*}\r\nP_{i(i+1)}=\\alpha\\binom{i}{1}\\binom{N-i}{1}/\\binom{N}{2}\\\\\r\nP_{ii}=1-\\alpha\\binom{i}{1}\\binom{N-i}{1}/\\binom{N}{2}\r\n\\end{align*}\r\nNote that 0 and 5 are two absorbing state, their diagonal entries is 1.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 4.0, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [2, 4], "rightproblems": [], "wrongproblems": [74], "twinproblems": []}}, {"model": "mathematics.question", "pk": 57, "fields": {"code": "3.1.4", "category": 3, "problem": "The random variables $\\xi_1,\\xi_2,...$ are independent and with the common probability mass function\r\n\\[\\begin{array}{ccccc}\\hline\r\nk &=&0&1&2&3\\\\\r\n\\text{Pr}\\{\\xi=k\\}& =&0.1&0.3&0.2&0.4\\\\\\hline\r\n\\end{array}\r\n\\] Set $X_0=0$, and let $X_n$=max\\{$\\xi_1,...,\\xi_n$\\} be the largest $\\xi$ observed to date. Determine the transition probability matrix for the Markov chain \\{$X_n$\\}", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "\\[\r\n\\begin{array}{ccccc }\r\n\\,\\,\\,\\, & 0 \\,\\,\\,\\,\\,\\,\\,&1\\,\\,\\,\\,\\,\\,\\,&2\\,\\,\\,\\,\\,\\,\\,\r\n&3\\\\\r\n\\end{array}\r\n\\]\r\n\\[\r\n\\begin{array}{c||cccc||}\r\n0&0.1&0.3&0.2&0.4\\\\\r\n1&0.1&0.3&0.2&0.4\\\\\r\n2&0.1&0.3&0.2&0.4\\\\\r\n3&0.1&0.3&0.2&0.4\r\n\\end{array}\r\n\\]", "choicesb": "\\[\r\n\\begin{array}{c||cccc||}\r\n&0&1&2&3\\\\\r\n0&0.1&0.3&0.2&0.4\\\\\r\n1&0&0.3&0.2&0.4\\\\\r\n2&0&0&0.2&0.4\\\\\r\n3&0&0&0&0.4\r\n\\end{array}\r\n\\]", "choicesc": "\\[\r\n\\begin{array}{c||cccc||}\r\n&0&1&2&3\\\\\r\n0&0.1&0.3&0.2&0.4\\\\\r\n1&0&0.4&0.2&0.4\\\\\r\n2&0&0&0.6&0.4\\\\\r\n3&0&0&0&1\r\n\\end{array}\r\n\\]", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "$P_{ij}=0$ if $j\\neq i$ because we only consider the maximum.\r\nMaximum will remain unchanged when you roll a smaller value: those probabilities are added to $P_{ii}$.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [2, 4], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 58, "fields": {"code": "3.1.2", "category": 3, "problem": "Consider the problem of sending a binary message, 0 or 1, through a signal channel consisting of several stages, where transmission through each stage is subject to a fixed probability of error ?. Suppose that $ X_0 = 0 $ is the signal that is sent and let $ X_n $ be the signal that is received at the nth stage. Assume that$ \\{X_n\\} $ is a Markov chain with transition probabilities $ P_{00} = P_{11} = 1 - \\alpha $ and $ P_{01} = P_{10} = \\alpha $, where $ 0 < \\alpha < 1 $. <br>\r\n (a) Determine $ \\Pr\\{X_0 =0,X_1 =0,X_2 =0\\}$,the probability that no error occurs up to stage $ n = 2$.<br>\r\n (b) Determine the probability that a correct signal is received at stage 2. (Hint: This is $ \\Pr\\{X_0 = 0,X_1 = 0,X_2 = 0\\} + \\Pr\\{ X_0 = 0 ,X_1 = 1,X_2 =0\\}$.)", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. (a) $ 1 - \\alpha \\ $ ;  (b) $  1 - \\alpha $", "choicesb": "B. (a) $ 1 - \\alpha \\ $ ;  (b) $ (1 - \\alpha)^2   $", "choicesc": "C. (a) $ (1 - \\alpha)^2 \\ $ ; (b) $ (1 - \\alpha)^2 $", "choicesd": "D. (a) $ (1 - \\alpha)^2 \\ $ ;  (b) $ 1 - (1 - \\alpha)^2 $", "choicese": "E. (a) $ (1 - \\alpha)^2 \\ $ ; (b) $  1 - 2\\alpha(1 - \\alpha)   $", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "E", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "(a)\r\n\\begin{align*}\r\n\\Pr\\{X_0 = 0,X_1 = 0,X_2 = 0\\} = P_{00} \\cdot P_{00} = \\left( 1 - \\alpha \\right)^2\r\n\\end{align*}\r\n(b) \r\n\\begin{align*}\r\n \\Pr\\{X_0 = 0,X_1 = 0,X_2 = 0\\} + \\Pr\\{ X_0 = 0 ,X_1 = 1,X_2 =0\\}\r\n&= P_{00} \\cdot P_{00} + P_{01} \\cdot P_{10}\\\\\r\n&= \\left( 1 - \\alpha \\right)^2 + \\alpha^2 = 1 - 2\\alpha + 2\\alpha^2 \\\\\r\n&= 1 - 2\\alpha(1-\\alpha)\r\n\\end{align*}", "linkability1": 1.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, your answer is not correct. You may not fully understand the concept of Markov chain and transition probability. You could try other similar problems.", "messagesuccess": "Good job! You are quite familiar with the concept of Markov chain and transition probability.", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [3, 6], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 59, "fields": {"code": "3.3.11", "category": 1, "problem": "(The Confucius descendants) \r\nHypothetically, every male in the Kong family, since Confucius, produces his sons,\r\nindependent of everything else,\r\naccording to the same distribution\r\n<br>\r\n\\[\r\n\\begin{array}{ l cccccc c }\r\nNumber  \\,  of  \\,\\, sons:  & 0   &    1   &   2    &  3     &  4   &   5   &  6  \\,\\,  or\\,\\,  more  \\\\\r\nProbability :      &  0.17 & 0.50&  0.25 & 0.05 & 0.02&  0 .01    &  0 \\\\\r\n\\end{array}\r\n\\]\r\n<br>\r\n Then the mean number of sons of any Kong male is about 1.2.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $X_n$ be the number of the $n$-th generation of descendants of\r\n Confucius. (According to the old Chinese tradition, only males carrying\r\n the last name are counted as family descendants.)\r\n Then $\\{X_n: n \\geq 1\\}$ is a so-called branching process.\r\n Here $X_0=1$ meaning that the $0$-th generation is Confucius himself, alone.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Congratulations! You have mastered the concept and fundamental characteristics of Branching Process.", "messagesuccess": "Alas! Please return to the graph and review related knowledge.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [14], "rightproblems": [6], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 60, "fields": {"code": "3.2.7", "category": 1, "problem": "({\\sc Penney-ante}) Repeatedly tossing a fair coin.\r\nThere are 8 patterns of length 3:\r\n<br>\r\n$$HHH,  HHT, HTH, HTT, THH, THT, TTH, TTT.$$\r\n<br>\r\nTwo men, called  Yugong (Y) and Zhishou (Z),  are betting on whose choice\r\nof pattern would occur first. Mr Z  ``generously\"  allows\r\nMr Y to pick his favorite pattern first and, afterwards, he picks his own.\r\nWhatever Mr Y picks,  Mr Z, being ``Zhishou\", somehow\r\nalways beats him by a  chance at least 2/3! (This time, even infinite offspring\r\nto continue the game can't help Yugong.)", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $X_n, n \\geq 2$ denote the pattern of length 2 of tosses\r\n$n-1$ and $n$.\r\nThen $\\{X_n: n \\geq 2\\}$ is a $MC$  with state space $\\{$HH , HT, TH, TT$\\}$,\r\nwhich is accordingly denoted as $\\{0, 1, 2, 3\\}$, and transition matrix\r\n$$ \\qquad  \\,\\, 0 \\qquad 1 \\qquad 2\\qquad 3  $$\r\n$${\\bf P} =\\matrix{0 \\cr 1 \\cr 2 \\cr 3    }\r\n  \\pmatrix{\r\n  1/2 &  1/2  &     &              \\cr\r\n       &     &  1/2  &  1/2            \\cr\r\n  1/2 &  1/2   &     &             \\cr\r\n       &      &  1/2   &   1/2\r\n}\r\n$$\r\nSuppose Mr Y's pick is HTH. Mr Z can pick HHT  to beat Mr Y\r\nby a chance $2/3$. The following is the proof using first-step analysis.\r\n\r\nLet $p_i $ be the probability that pattern HTH occurs before HHT given\r\n$X_2=i$, $i=0, 1, 2, 3$.\r\nThen\r\n\\begin{eqnarray*}\r\np_0 &=& 1/2p_0 + 1/2 \\times 0 \\\\\r\np_1 &=& 1/2 \\times 1 + 1/2 p_3 \\\\\r\np_2 &=& 1/2 p_0 + 1/2p_1 \\\\\r\np_3 &=& 1/2 p_2 + 1/2 p_3\r\n\\end{eqnarray*}\r\nSolving the equations, we have\r\n$$p_0 = 0 \\quad p_1 = 2/3 \\quad p_2=p_3=1/3$$\r\nThen, the chance that Mr Y wins is\r\n$$ (p_0 + p_1 +p_2+p_3)/4 = 1/3,$$\r\nand Mr Z wins with chance $2/3$.\r\n\r\nGeneral results are:\r\n\r\nMr Y's preemptive Choice: $$ HHH , HHT , HTH , HTT , THH , THT , TTH , TTT $$\r\nMr Z's responsive Choice: $$ THH , THH , HHT , HHT ,  TTH , TTH , HTT , HTT $$\r\n $P$(Mr Z  wins):$$ 7/8\\quad  3/4\\quad  2/3\\quad 2/3\\quad 2/3 \\quad 2/3\\quad 3/4\\quad 7/8  $$\r\n%Mr Y's choice:\r\n % H & HHH &\r\n%Mr Z's choice:\r\n  % & THH &\r\n%Chance of Mr Z winning:\r\n  % & 7/8\r\n\\end{tabular}\r\n\\end{center}\r\n\\hfill $\\square$\r\n\r\nThe above problem may sound counter-intuitive in that Mr Y, with the right of\r\nchoosing first, is always in disadvantage. In other words, preemptive strike always\r\nloses to the counter-strike,   because there does NOT exist\r\nan optimal pattern superior to the rest seven patterns.\r\n It's essentially\r\nabout four random variables, numbers of tosses till the patterns of length 3, forming  a loop\r\nof one dominating another as follows:\r\n\r\nSuch a seemingly strange/bizarre  phenomenon is not uncommon. For a simpler occasion,\r\ntry to construct\r\nthree r.v.s, say, $X$, $Y$ and $Z$, such that\r\n$P(X>Y ) = P(Y> Z)= P(Z> X)=2/3$; which may appear equally bizarre at first\r\nglance. (Please DIY.\r\nHint:  ``TIAN GI SHAI MA\"---the horse racing strategy of the legendary Sun Bin suggested  to\r\nGeneral Tian in the kingdom\r\nof Qi.)\r\n\r\nSome more hints about an insightful explanation of Mr Z's strategy: HHH defeats THH only\r\nwhen the first three tosses turn  out HHH (why?). HHT defeats THH only when the first two tosses\r\nturn out HH (why?). For HTH vs HHT, the competition begins sometime with an H.\r\nIf the following one is H, Mr\r\nZ wins ($1/2$ chance already). If the following two is TH ($1/4$ chance) Mr Y wins,\r\nIf the following two is TT $(1/4$ chance), back to origin  waiting for the next H to\r\noccur. DIY: based on this reasoning, come up with one equation about the chance of Mr Z's HHT\r\ndefeating Mr Y's HTH. Work out the similar for the case of HTT vs HHT.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Alas! Please return to the graph and review related knowledge.", "messagesuccess": "Congratulations! You have mastered the First Step Analysis method.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [4], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 61, "fields": {"code": "3.2.3", "category": 3, "problem": "Let $X_n$ denote the quality of $n$th item produced by a production system with $X_n=0$ meaning \"good\" and $X_n=1$ meaning \"defective.\" Suppose that $X_n$ evolve as a Markov chain whose transition probability matrix is \r\n\\[P=\r\n\\begin{array}{c||cc||}\r\n&0&1\\\\\r\n0&0.99&0.01\\\\\r\n1&0.12&0.88\r\n\\end{array}\r\n\\]\r\nWhat is the probability that the fourth item is defective given that the first item is defective", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.973731", "choicesb": "0.026269", "choicesc": "0.315228", "choicesd": "0.684772", "choicese": "0.605752", "choicesf": "0.88", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "\\[P^{(4-1)}=P^{(3)}=\r\n\\begin{array}{c||cc||}\r\n&0&1\\\\\r\n0&0.973731&0.026269\\\\\r\n1&0.315228&0.684772\r\n\\end{array}\\]\r\nSince we know the first one is defective: original state is 1,\r\nwe want to calculate the probability the probability of defective (1), so we calculate\r\n$P_{11}^{(3)}=0.684772$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 5.0, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [2, 5, 10], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 62, "fields": {"code": "3.3.1", "category": 3, "problem": "An urn contains six tags, of which three are red and three green.\r\nTwo tags are selected from the urn. If one tag is red and the other is green,\r\nthen the selected tags are discarded and two blue tags are returned to the\r\nurn. Otherwise, the selected tags are returned to the urn. This process repeats\r\nuntil the urn contains only blue tags. Let $X_n$, denote the number of red\r\ntags in the urn after the $n$th draw, with $X_0=3$. (This is an elementary\r\nmodel of a chemical reaction in which red and green atoms combine to\r\nform a blue molecule.) Give the transition probability matrix.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "\\[\r\n\\begin{array}{c||cccc||}\r\n&0&1&2&3\\\\\r\n0&1&0&0&0\\\\\r\n1&1/15&14/15&0&0\\\\\r\n2&0&4/15&11/15&0\\\\\r\n3&0&0&3/5&2/5\\\\\r\n\\end{array}\\]", "choicesb": "\\[\r\n\\begin{array}{c||cccc||}\r\n&0&1&2&3\\\\\r\n0&1&0&0&0\\\\\r\n1&14/15&1/15&0&0\\\\\r\n2&0&11/15&4/15&0\\\\\r\n3&0&0&2/5&3/5\\\\\r\n\\end{array}\\]", "choicesc": "\\[\r\n\\begin{array}{c||cccc||}\r\n&0&1&2&3\\\\\r\n0&1&0&0&0\\\\\r\n1&0&14/15&1/15&0\\\\\r\n2&0&0&11/15&4/15\\\\\r\n3&0&0&0&1\\\\\r\n\\end{array}\\]", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Consider the possible state in the urn:<br>\r\n3 red, 3 green, 0 blue<br>\r\n2 red, 2 green, 2 blue<br>\r\n1 red, 1 green, 4 blue<br>\r\n0 red, 0 green, 6 blue<br>\r\n\r\nNow we know the transition probability \r\n\\[P_{i(i-1)}=X^2/\\binom{6}{2}\\]\r\n\\[P_{ii}=1-X^2/\\binom{6}{2}\\]\r\n\r\nSo we have the transition probability matrix is\r\n\\[P=\r\n\\begin{array}{c||cccc||}\r\n&0&1&2&3\\\\\r\n0&1&0&0&0\\\\\r\n1&1/15&14/15&0&0\\\\\r\n2&0&4/15&11/15&0\\\\\r\n3&0&0&3/5&2/5\\\\\r\n\\end{array}\\]", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [2, 6], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 63, "fields": {"code": "3.2.1", "category": 3, "problem": "Consider the Markov chain whose transition probability matrix is given by \r\n\\[ \r\n\\mathbf{P} = \r\n \\begin{array}{l}\r\n \\begin{array}{lllll}\r\n \\hspace{0.1cm} & \\ \\ \\ 0 \\ & \\  \\ 1 \\ \\  & \\hspace{0.05cm} \\ 2 \\ \\  & \\hspace{0.05cm}  3 \\  \\ \\ \\ \\ \\\r\n \\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n3\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n  0.4 & 0.3 & 0.2 & 0.1 \\\\ \r\n  0.1 & 0.4 & 0.3 & 0.2 \\\\ \r\n  0.3 & 0.2 & 0.1 & 0.4\\\\\t\r\n  0.2 & 0.1 & 0.4 & 0.3 \\\\ \r\n\\end{Vmatrix}\r\n \\end{array}\r\n \\]\r\nSuppose that the initial distribution is $ p_i = \\frac{1}{4} $ for $ i=0,1,2,3 $. Given that $ \\Pr\\{X_n = k\\} = \\frac{1}{4},k=0,1,2,3 $ for all $ n $, where $k$ is the state of the Markov chain. For $n \\in \\mathbb{N}$ (the set of all natural numbers), deduce a general result from this example.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. $ \\Pr\\{X_n = k\\} = \\frac{1}{N + 1} $", "choicesb": "B. $ \\Pr\\{X_n = k\\} = \\frac{1}{N} $", "choicesc": "C. $\\Pr\\{X_n = k\\} = \\frac{N}{N + 1} $", "choicesd": "D. $\\Pr\\{X_n = k\\} = \\frac{1}{k}$", "choicese": "E. No general result can be deduced. The result depends on the transition probabilities.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "\\[p_i = \\Pr\\{X_0 = i\\} =  \\frac{1}{4} \\ \\text{for} \\ i = 0, 1, 2, 3\\]\r\nFor $k = 0,1,2,3$ :\r\n\\begin{align*}\r\n\\Pr\\{X_1 = k\\} \r\n&= \\Pr\\{X_1 = k | X_0 = 0\\} \\cdot \\Pr\\{X_0 = 0\\} + \\cdots + \\Pr\\{X_1 = k | X_0 = 3\\} \\cdot \\Pr\\{X_0 = 3\\}\\\\\r\n& \\ \\ \\  (\\text{Slicing the universe according to } X_0 ) \\\\\r\n&= \\sum_{j=0}^{3} \\Pr\\{X_1 = k | X_0 = j\\} \\cdot \\Pr\\{X_0 = j\\} \\\\\r\n&= \\sum_{j=0}^{3} \\textbf{P}_{jk} \\cdot p_j \\ (\\text{by the definition of transition probability})\\\\\r\n&= \\frac{1}{4} \\cdot \\sum_{j=0}^{3} \\textbf{P}_{jk} \\\\\r\n&= \\frac{1}{4} \\cdot 1 \\ (\\text{by the property of transition probability}) \\\\\r\n&= \\frac{1}{4} = \\Pr\\{X_0 = k\\} \r\n\\end{align*}\r\nBy induction, $\\Pr\\{X_n = k\\} = \\Pr\\{X_{n-1} = k\\} = \\cdots = \\Pr\\{X_1 = k\\} = \\Pr\\{X_0 = k\\} = \\frac{1}{4} \\ \\forall n \\in \\mathbb{N}$ ($\\mathbb{N}$ is the set of all natural numbers). <br>\r\nTherefore, for any $N \\in \\mathbb{N}$, \\[\\text{if} \\ \\Pr\\{X_0 = k\\} = \\frac{1}{N + 1} \\ \\text{for} \\  k = 0, 1, ..., N\\] where $k$ is the state of a Markov chain, then \\[\\Pr\\{X_n = k\\} = \\frac{1}{N + 1} \\ \\forall n \\in \\mathbb{N}\\]", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 3.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [3, 6, 26], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 64, "fields": {"code": "3.3.7", "category": 3, "problem": "A component in a system is placed into service, where it operates until its failure, whereupon it is replaced at the end of the period with a new component having statistically identical properties, and the process repeats. The probability that a component lasts for $k$ periods is $\\alpha_k$, for $k=1,2,...$ Let$X_n$ be the remaining life of the component in service \\emph{at the end of period n}. Then $X_n=0$ means that $X_{n+1}$ will be the total operating life of the next component. Give the transition probabilities for the Markov chain $\\{X_n\\}$.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "\\[\r\n\\begin{array}{c||cccccc||}\r\n&0&1&2&\\dots&k&\\dots\\\\\r\n0&1&0&0&\\dots&0&\\dots\\\\\r\n1&1&0&0&\\dots&0&\\dots\\\\\r\n2&0&1&0&\\dots&0&\\dots\\\\\r\n\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\r\n\\end{array}\r\n\\]", "choicesb": "\\[\r\n\\begin{array}{c||cccccc||}\r\n&0&1&2&\\dots&k&\\dots\\\\\r\n0&0&\\alpha_1&\\alpha_2&\\dots&\\alpha_k&\\dots\\\\\r\n1&1&0&0&\\dots&0&\\dots\\\\\r\n2&0&1&0&\\dots&0&\\dots\\\\\r\n\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\r\n\\end{array}\r\n\\]", "choicesc": "\\[\r\n\\begin{array}{c||cccccc||}\r\n&0&1&2&\\dots&k&\\dots\\\\\r\n0&0&\\alpha_1&\\alpha_2&\\dots&\\alpha_k&\\dots\\\\\r\n1&1-\\alpha_1&\\alpha_1&0&\\dots&0&\\dots\\\\\r\n2&0&1-\\alpha_2&\\alpha_2&\\dots&0&\\dots\\\\\r\n\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\r\n\\end{array}\r\n\\]", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Note that the state always decrease by 1 ($i\\to i-1$) as times go by. Once it reaches 0, it is replaced ($0\\to k$).\r\nSo \r\n\\[P=\r\n\\begin{array}{c||cccccc||}\r\n&0&1&2&\\dots&k&\\dots\\\\\r\n0&0&\\alpha_1&\\alpha_2&\\dots&\\alpha_k&\\dots\\\\\r\n1&1&0&0&\\dots&0&\\dots\\\\\r\n2&0&1&0&\\dots&0&\\dots\\\\\r\n\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\r\n\\end{array}\r\n\\]", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [2, 6], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 66, "fields": {"code": "3.2.4", "category": 3, "problem": "Problem: \r\n Suppose $ X_n $ is a two-state Markov chain whose transition probability matrix is \r\n  \\[ \r\n \\mathbf{P} = \r\n \\begin{array}{l}\r\n \\begin{array}{lll}\r\n \\hspace{0.1cm} & \\ \\ \\ \\ \\ 0 \\ & \\  \\  \\  \\  \\  \\  1 \\ \\ \r\n \\end{array} \\\\\r\n \\begin{array}{c}\r\n 0\\\\\r\n 1\\\\\r\n \\end{array}\r\n \\begin{Vmatrix}\r\n \\alpha & 1 - \\alpha   \\\\ \r\n 1 - \\beta & \\beta   \\\\ \r\n \\end{Vmatrix}\r\n \\end{array}\r\n \\]\r\nThen, $ Z_n =(X_{n-1},X_n) $ is a Markov chain having the four states $\\{0, 1, 2, 3\\} = \\{(0,0),(0,1), (1,0), (1,1)\\}$. Determine the transition probability matrix.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. $ \\begin{pmatrix}\r\n \t\\alpha & 1 - \\alpha & 0 & 0\\\\\r\n \t1 - \\beta & \\beta & 0 & 0 \\\\\r\n \t0 & 0 & \\alpha & 1 - \\alpha\\\\\r\n \t0 & 0 & 1 - \\beta & \\beta\\\\\r\n \\end{pmatrix} $", "choicesb": "B. $ \\begin{pmatrix}\r\n0 & 0 & \\alpha & 1 - \\alpha\\\\\r\n0 & 0 & 1 - \\beta & \\beta\\\\\r\n\\alpha & 1 - \\alpha & 0 & 0\\\\\r\n1 - \\beta & \\beta & 0 & 0 \\\\\r\n\\end{pmatrix} $", "choicesc": "C. $ \\begin{pmatrix}\r\n\t\\alpha & 1 - \\alpha & 0 & 0\\\\\r\n0& 0& 1 - \\beta & \\beta \\\\\r\n\t\\alpha & 1 - \\alpha & 0 & 0\\\\\r\n0& 0& 1 - \\beta & \\beta  \\\\\r\n\\end{pmatrix} $", "choicesd": "D. $ \\begin{pmatrix}\r\n1 - \\beta & \\beta & 0 & 0 \\\\\r\n\\alpha & 1 - \\alpha & 0 & 0\\\\\r\n1 - \\beta & \\beta & 0 & 0 \\\\\r\n\\alpha & 1 - \\alpha & 0 & 0\\\\\r\n\\end{pmatrix} $", "choicese": "E. $ \\begin{pmatrix}\r\n0 & 0 &\\alpha & 1 - \\alpha\\\\\r\n1 - \\beta & \\beta & 0 & 0 \\\\\r\n0 & 0 & \\beta & 1 - \\beta  \\\\\r\n1 - \\alpha &  \\alpha & 0 & 0\\\\\r\n\\end{pmatrix} $", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "\\begin{align*}\r\n\\Pr\\{Z_0 = (0,0) = 0\\} \r\n&= \\textbf{P}_{00} = \\alpha\\\\\r\n\\Pr\\{Z_0 = (0,1) = 1\\} \r\n&= \\textbf{P}_{01} = 1 - \\alpha\\\\\r\n\\Pr\\{Z_0 = (1,0) = 2\\} \r\n&= \\textbf{P}_{10} = 1 - \\beta\\\\\r\n\\Pr\\{Z_0 = (1,1) = 3\\} \r\n&= \\textbf{P}_{11} = \\beta\\\\ \r\n\\\\\r\n\\Pr\\{Z_1 = 0 | Z_0 = 0\\} \r\n&= \\textbf{P}'_{00} = \\textbf{P}_{00}= \\alpha\\\\\r\n\\Pr\\{Z_1 = 1 | Z_0 = 0\\} \r\n&= \\textbf{P}'_{01} = \\textbf{P}_{01} = 1 - \\alpha\\\\\r\n\\Pr\\{Z_1 = 2 | Z_0 = 0\\} \r\n&= \\textbf{P}'_{02} = 0 = \\Pr\\{Z_1 = 3 | Z_0 = 0\\} = \\textbf{P}_{03} \\\\\r\n\\end{align*}\r\nOther transition probabilities are computed similarly, and finally the transition probability matrix is:\r\n \\[\r\n\\mathbf{P}' = \r\n\\begin{array}{l}\r\n\\begin{array}{lllll}\r\n\\hspace{0.075cm} & \\ \\ 0 \\ & \\hspace{0.1cm} \\ \\  1 \\ \\  & \\hspace{0.1cm} \\ \\ \\ \\ 2 \\ \\  & \\hspace{0.1cm} \\  3 \\  \\ \\ \\ \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n3\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n\\alpha & 1 - \\alpha & 0 & 0  \\\\ \r\n0 & 0 & 1 - \\beta & \\beta \\\\ \r\n\\alpha & 1 - \\alpha & 0 & 0  \\\\ \r\n0 & 0 & 1 - \\beta & \\beta \\\\ \r\n\\end{Vmatrix}\r\n\\end{array}\r\n\\]", "linkability1": 1.0, "linkability2": 3.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [6], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 67, "fields": {"code": "3.3.10", "category": 3, "problem": "Consider a discrete-time, periodic reciew inventory model and let $\\xi_n$ be the total demand in period $n$, and let $X_N$ be the inventory quantity on hand at the end-of-period $n$. An ($s,S$) inventory policy is used: If the end-of-oeriod stock is not greater than $s$, then a quantity is instantly procured to bring the level up to $S$. If the end-of-period stock exceeds s, then no replenishment takes place.<br>\r\nSuppose that $\\xi_1, \\xi_2,...$ are undependent random variables where $Pr\\{\\xi_n=0\\}$=0.1, Pr$\\{\\xi_n=1\\}$=0.3, Pr$\\{\\xi_n=2\\}$=0.3, Pr$\\{\\xi_n=3\\}$=0.2, Pr$\\{\\xi_n=4\\}$=0.1. Then $X_0,X_1,...$ is a Markov chain. Determine $P_{41}$ and $P_{04}$.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$P_{41}=0.2\\qquad P_{04}=0.4$", "choicesb": "$P_{41}=0.3\\qquad P_{04}=0.3$", "choicesc": "$P_{41}=0.1\\qquad P_{04}=0.2$", "choicesd": "$P_{41}=0.2\\qquad P_{04}=0.1$", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The only possible way to achieve ($4\\to1$) is the demand is 3. Therefore $P_{41}=\\xi_3=0.2$.<br>\r\nAs for $P_{04}$, the possible route can be $\\xi=0$ or $\\xi=4$ or $\\xi=3$\r\n$P_{04}=0.1+0.1+0.2=0.4$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 68, "fields": {"code": "3.3.2", "category": 3, "problem": "Three fair coins are tossed, and we let $ X_1 $ denote the number of heads that appear. Those coins that were heads on the first trial (there were $ X_1 $ of them) we pick up and toss again, and now we let $ X_2 $ be the total number of tails, including those left from the first toss. We toss again all coins showing tails, and let $ X_3 $ be the resulting total number of heads, including those left from the previous toss. We continue the process.The pattern is,count heads,toss heads, count tails, toss tails, count heads, toss heads, etc., and $ X_0 = 3 $. Then, $ {Xn} $ is a Markov chain. What is the transition probability matrix?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. $ \\begin{pmatrix}\r\n1 & 0 & 0 & 0\\\\\r\n1/2 & 1/2 & 0 & 0\\\\\r\n1/4 & 1/2 & 1/4 & 0 \\\\\r\n1/8 & 3/8 & 3/8 & 1/8\\\\\r\n\\end{pmatrix}  $", "choicesb": "B. $ \\begin{pmatrix}\r\n1/8 & 3/8 & 3/8 & 1/8\\\\\r\n0 & 1/4 & 1/2 & 1/4 \\\\\r\n0 & 0 & 1/2 & 1/2 \\\\\r\n0 & 0 & 0 & 1\\\\\r\n\\end{pmatrix}  $", "choicesc": "C. $ \\begin{pmatrix}\r\n0 & 0 & 0 & 1\\\\\r\n0 & 0 & 1/2 & 1/2 \\\\\r\n0 & 1/4 & 1/2 & 1/4 \\\\\r\n1/8 & 3/8 & 3/8 & 1/8\\\\\r\n\\end{pmatrix}  $", "choicesd": "D. $ \\begin{pmatrix}\r\n1/8 & 3/8 & 3/8 & 3/8\\\\\r\n1/4 & 1/2 & 1/4 & 0\\\\\r\n1/2 & 1/2 & 0 & 0 \\\\\r\n1 & 0 & 0 & 0 \\\\\r\n\\end{pmatrix}  $", "choicese": "E. $ \\begin{pmatrix}\r\n0 & 0 & 0 & 1\\\\\r\n0 & 0 & 1/2 & 1/2 \\\\\r\n0 & 1/3 & 1/3 & 1/3 \\\\\r\n1/4 & 1/4 & 1/4 & 1/4\\\\\r\n\\end{pmatrix}  $", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "\\[\\Pr\\{X_0 = 0\\} = \\frac{1}{8}, \\Pr\\{X_0 = 1\\} = \\frac{3}{8}, \\Pr\\{X_0 = 2\\} = \\frac{3}{8}, \\Pr\\{X_0 = 3\\} = \\frac{1}{8} \\]\r\n\r\nIf 0 head appears in the previous toss, then 3 tails must appear in this toss:\r\n\\begin{align*}\r\n\\rightarrow \\Pr\\{X_1 = 0| X_0 = 0\\} \r\n&= \\Pr\\{X_1 = 1| X_0 = 0\\} = \\Pr\\{X_1 = 2| X_0 = 0\\} = 0\\\\\r\n\\Pr\\{X_1 = 3| X_0 = 0\\} \r\n&= 1\\\\\r\n\\end{align*}\r\nIf 1 head appears in the previous toss, then 2 or 3 tails will appear in this toss where both probabilities are 1/2 :\r\n\\begin{align*}\r\n\\rightarrow \\Pr\\{X_1 = 2| X_0 = 1\\} \r\n&= \\frac{1}{2} = \\Pr\\{X_1 = 3| X_0 = 1\\} \\\\\r\n\\Pr\\{X_1 = 0| X_0 = 1\\} \r\n&= 0 = \\Pr\\{X_1 = 1| X_0 = 1\\} \\\\\r\n\\end{align*}\r\nIf 2 head appear in the previous toss, then 1, 2 or 3 tail(s) will appear in this toss where the corresponding probabilities are 1/4, 1/2 and 1/4 respectively :\r\n\\begin{align*}\r\n\\rightarrow \\Pr\\{X_1 = 3| X_0 = 2\\} \r\n&= \\frac{1}{4} = \\Pr\\{X_1 = 1| X_0 = 2\\}\\\\\r\n\\Pr\\{X_1 = 2| X_0 = 2\\}  \r\n&= \\frac{1}{2} \\\\\r\n\\Pr\\{X_1 = 0| X_0 = 2\\} \r\n&= 0  \\\\\r\n\\end{align*}\r\nIf 3 head appear in the previous toss, then 0, 1, 2 or 3 tail(s) will appear in this toss where the corresponding probabilities are 1/8, 3/8, 3/8 and 1/8 respectively :\r\n\\begin{align*}\r\n\\rightarrow \\Pr\\{X_1 = 3| X_0 = 3\\} \r\n&= \\frac{1}{8} = \\Pr\\{X_1 = 0| X_0 = 3\\} \\\\\r\n\\Pr\\{X_1 = 2| X_0 = 3\\} \r\n&= \\frac{3}{8} = \\Pr\\{X_1 = 1| X_0 = 3\\} \r\n\\end{align*}\r\nthe transition probability matrix is:\r\n \\[\r\n \\mathbf{P} = \r\n \\begin{array}{l}\r\n \\begin{array}{lllll}\r\n \\hspace{0.125cm} & \\ \\  \\ 0 \\ & \\hspace{0.125cm} \\ \\ 1 \\ \\  & \\hspace{0.125cm}  \\ 2 \\ \\  & \\hspace{0.125cm} \\  3 \\  \\ \\ \\ \\\\\r\n \\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n3\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n  0 & 0 & 0 & 1  \\\\ \r\n  0 & 0 & 1/2 & 1/2 \\\\ \r\n  0 & 1/4 & 1/2 & 1/4  \\\\ \r\n  1/8 & 3/8 & 3/8 & 1/8 \\\\ \r\n\\end{Vmatrix}\r\n \\end{array}\r\n \\]", "linkability1": 2.0, "linkability2": 3.0, "linkability3": 1.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [6], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 69, "fields": {"code": "3.3.5", "category": 3, "problem": "You are going to successively flip a quarter until the pattern $ HHT $ appears, that is, until you observe two successive heads followed by a tails. In order to calculate some properties of this game, you set up a Markov chain with the following states: 0,$ H $,$ HH $, and $ HHT $, where 0 represents the starting point, $ H $ represents a single observed head on the last flip, $ HH $ represents two successive heads on the last two flips, and $ HHT $ is the sequence that you are looking for. Observe that if you have just tossed a tails, followed by a heads, a next toss of a tails effectively starts you over again in your quest for the $ HHT $ sequence. Let the state $\\{0, H, HH, HHT\\}$ be state $\\{0, 1, 2, 3\\}$ of this Markov chain $X_n$. Set up the transition probability matrix.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. $ \\begin{pmatrix}\r\n1/2 & 1/2 & 0 & 0\\\\\r\n1/2 & 0 & 1/2 & 0\\\\\r\n1/2 & 0 & 0 & 1/2 \\\\\r\n1 & 0 & 0 & 0  \\\\\r\n\\end{pmatrix}  $", "choicesb": "B. $ \\begin{pmatrix}\r\n1/2 & 1/2 & 0 & 0\\\\\r\n0 & 1/2 & 1/2 & 0\\\\\r\n0 & 0 & 1/2 & 1/2 \\\\\r\n1 & 0 & 0 & 0  \\\\\r\n\\end{pmatrix}  $", "choicesc": "C.  $ \\begin{pmatrix}\r\n1/2 & 1/2 & 0 & 0\\\\\r\n0 & 1/2 & 1/2 & 0\\\\\r\n0 & 0 & 1/2 & 1/2 \\\\\r\n0 & 0 & 0 & 1  \\\\\r\n\\end{pmatrix}  $", "choicesd": "D.  $ \\begin{pmatrix}\r\n1/2 & 1/2 & 0 & 0\\\\\r\n1/2 & 0 & 1/2 & 0\\\\\r\n1/2 & 0 & 0 & 1/2 \\\\\r\n0 & 0 & 0 & 1 \\\\\r\n\\end{pmatrix}  $", "choicese": "E. $ \\begin{pmatrix}\r\n1/2 & 0 & 0 & 1/2\\\\\r\n0 & 1/2 & 0 & 1/2\\\\\r\n0 & 0 & 1/2 & 1/2 \\\\\r\n0 & 0 & 0 & 1  \\\\\r\n\\end{pmatrix}  $", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "If a head appears in the this toss and no head appeared in the previous toss, then it will become state 1; otherwise it returns to state 0:\r\n\\begin{align*}\r\n\\rightarrow \\Pr\\{X_1 = 0| X_0 = 0\\} \r\n&= \\frac{1}{2} = \\Pr\\{X_1 = 1| X_0 = 0\\} \\\\\r\n\\Pr\\{X_1 = 2| X_0 = 0\\} \r\n&= 0 = \\Pr\\{X_1 = 3| X_0 = 0\\} \\\\\r\n\\end{align*}\r\nIf a head appears in this toss and one head appeared in the previous toss, then it will become state 2; otherwise it returns to state 0:\r\n\\begin{align*}\r\n\\rightarrow \\Pr\\{X_1 = 0| X_0 = 1\\} \r\n&= \\frac{1}{2} = \\Pr\\{X_1 = 2| X_0 = 1\\} \\\\\r\n\\Pr\\{X_1 = 1| X_0 = 1\\} \r\n&= 0 = \\Pr\\{X_1 = 3| X_0 = 1\\} \\\\\r\n\\end{align*}\r\nIf a tail appears in this toss and two heads appeared in the last two tosses, then it will become state 3; otherwise it returns to state 0:\r\n\\begin{align*}\r\n\\rightarrow \\Pr\\{X_1 = 0| X_0 = 2\\} \r\n&= \\frac{1}{2} = \\Pr\\{X_1 = 3| X_0 = 2\\} \\\\\r\n\\Pr\\{X_1 = 1| X_0 = 2\\} \r\n&= 0 = \\Pr\\{X_1 = 2| X_0 = 2\\} \\\\\r\n\\end{align*}\r\nIf it is at state 3, you win the game which means you stay at state 3:\r\n\\begin{align*}\r\n\\rightarrow \\Pr\\{X_1 = 3| X_0 = 3\\} \r\n&= 1  \\\\\r\n\\Pr\\{X_1 = 0| X_0 = 3\\} \r\n&= 0 = \\Pr\\{X_1 = 1| X_0 = 3\\}  = \\Pr\\{X_1 = 2| X_0 = 3\\}\r\n\\end{align*}\r\nthe transition probability matrix is:\r\n \\[\r\n \\mathbf{P} = \r\n \\begin{array}{l}\r\n \\begin{array}{lllll}\r\n \\hspace{0.125cm} & \\ \\  \\ 0 \\ & \\hspace{0.125cm} \\ \\ 1 \\ \\  & \\hspace{0.1cm}  \\ 2 \\ \\  & \\hspace{0.125cm} \\  3 \\  \\ \\ \\ \\\\\r\n \\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n3\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n  1/2 & 1/2 & 0 & 0  \\\\ \r\n  1/2 & 0 & 1/2 & 0 \\\\ \r\n  1/2 & 0 & 0 & 1/2  \\\\ \r\n  0 & 0 & 0 & 1 \\\\ \r\n\\end{Vmatrix}\r\n \\end{array}\r\n \\]", "linkability1": 2.0, "linkability2": 3.0, "linkability3": 1.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [6], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 70, "fields": {"code": "3.3.8", "category": 3, "problem": "Two urns A and B contain a total of $ N $ balls. Assume that at time $ t $, there were exactly $ k $ balls in A. At time $ t+1 $, an urn is selected at random in proportion to its contents (i.e., A is chosen with probability $ k/N $ and B is chosen with probability $ (N - k)/N $). Then, a ball is selected from A with probability $ p $ or from B with probability $ q $ and placed in the previously chosen urn. Determine the transition matrix for this Markov chain. (Note that the states of this M.C. are $\\{0,1,2,...,N\\}$)", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. $ \\begin{pmatrix}\r\n1 & 0 & 0 & \\cdots & 0 & 0\\\\\r\np & q & 0 & \\cdots & 0 & 0\\\\\r\n0 & p & q & \\cdots & 0 & 0\\\\\r\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\r\n0 & 0 & 0 & \\cdots & p & q\\\\\r\n0 & 0 & 0 & \\cdots & 0 & 1\\\\\r\n\\end{pmatrix}  $", "choicesb": "B.$ \\begin{pmatrix}\r\n1 & 0 & 0 & \\cdots & 0 & 0\\\\\r\n\\left(\\frac{1}{N} \\right) p & \\left(\\frac{N-1}{N} \\right)q & 0 & \\cdots & 0 & 0\\\\\r\n0 & \\left(\\frac{2}{N} \\right)p & \\left(\\frac{N-2}{N} \\right)q & \\cdots & 0 & 0\\\\\r\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\r\n0 & 0 & 0 & \\cdots & \\left(\\frac{N-1}{N} \\right) p & \\left(\\frac{1}{N} \\right) q\\\\\r\n0 & 0 & 0 & \\cdots & 0 & 1\\\\\r\n\\end{pmatrix}  $", "choicesc": "C.  $ \\begin{pmatrix}\r\n1 & 0 & 0 & \\cdots & 0 & 0\\\\\r\n\\left(\\frac{N-1}{N} \\right) p & \\left(\\frac{1}{N} \\right)q & 0 & \\cdots & 0 & 0\\\\\r\n0 & \\left(\\frac{N-2}{N} \\right)p & \\left(\\frac{2}{N} \\right)q & \\cdots & 0 & 0\\\\\r\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\r\n0 & 0 & 0 & \\cdots & \\left(\\frac{1}{N} \\right) p & \\left(\\frac{N-1}{N} \\right) q\\\\\r\n0 & 0 & 0 & \\cdots & 0 & 1\\\\\r\n\\end{pmatrix}  $", "choicesd": "D. $ \\begin{pmatrix}\r\n1 & 0 & 0 & \\cdots & 0 & 0\\\\\r\n\\left(\\frac{1}{N} \\right) p & \\left(\\frac{N-1}{N} \\right) p + \\left(\\frac{1}{N} \\right) q  & \\left(\\frac{N-1}{N} \\right) q & \\cdots & 0 & 0\\\\\r\n0 & \\left(\\frac{2}{N} \\right) p & \\left(\\frac{N-2}{N} \\right) p + \\left(\\frac{2}{N} \\right) q & \\cdots & 0 & 0\\\\\r\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\r\n0 & 0 & 0 & \\cdots & \\left(\\frac{1}{N} \\right) p + \\left(\\frac{N-1}{N} \\right) q  & \\left(\\frac{1}{N} \\right) q\\\\\r\n0 & 0 & 0 & \\cdots & 0 & 1\\\\\r\n\\end{pmatrix}  $", "choicese": "E. $ \\begin{pmatrix}\r\n1 & 0 & 0 & \\cdots & 0 & 0\\\\\r\n\\left(\\frac{N-1}{N} \\right) p & \\left(\\frac{1}{N} \\right) p + \\left(\\frac{N-1}{N} \\right) q  & \\left(\\frac{1}{N} \\right) q & \\cdots & 0 & 0\\\\\r\n0 & \\left(\\frac{N-2}{N} \\right) p & \\left(\\frac{2}{N} \\right) p + \\left(\\frac{N-2}{N} \\right) q & \\cdots & 0 & 0\\\\\r\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\r\n0 & 0 & 0 & \\cdots & \\left(\\frac{N-1}{N} \\right) p + \\left(\\frac{1}{N} \\right) q  & \\left(\\frac{N-1}{N} \\right) q\\\\\r\n0 & 0 & 0 & \\cdots & 0 & 1\\\\\r\n\\end{pmatrix}  $", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "E", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $X_n$ be the number of balls in urn A at time $n$ for $n = 0,1,...,N$. Then, for $k=1,2,...,N-1$ and $n=0,1,...$,\r\n\\begin{align*}\r\nX_n \r\n&= k,\\\\\r\n\\Pr\\{X_{n+1} = k - 1 | X_n = k\\} \r\n&= (\\frac{N-k}{N}) \\cdot p,\\\\\r\n\\Pr\\{X_{n+1} = k|X_n = k\\} \r\n&= (\\frac{k}{N}) \\cdot p + (\\frac{N-k}{N}) \\cdot q, \\text{and} \\\\\r\n\\Pr\\{X_{n+1} = k + 1|X_n = k\\} \r\n&= (\\frac{k}{N}) \\cdot q \r\n\\end{align*}\r\nFor $k = 0$, $X_n = 0$, urn B must be chosen. Although urn A may be selected to take out one ball, urn A is empty and therefore number of balls in urn A and urn B will remain 0 and N respectively. It is like the end of the process, and we called this 'ending' state ($X_n = 0$) as '\\textit{absorbing state}'. The corresponding transition probabilities are    \r\n\\begin{align*}\r\n\\Pr\\{X_{n+1} = - 1 | X_n = 0 \\} \r\n&= 0\\\\\r\n\\Pr\\{X_{n+1} = 0 | X_n = 0 \\} \r\n&= 1\\\\\r\n\\Pr\\{X_{n+1} = 1 | X_n = 0 \\} \r\n&= 0\r\n\\end{align*}\r\nSimilarly, for $k = N$, $X_n = N$ urn A must be chosen. It is another absorbing state with transition probabilities \r\n\\begin{align*}\r\n\\Pr\\{X_{n+1} = N - 1 | X_n = N \\} \r\n&= 0\\\\\r\n\\Pr\\{X_{n+1} = N | X_n = N \\} \r\n&= 1\\\\\r\n\\Pr\\{X_{n+1} = N + 1 | X_n = N \\} \r\n&= 0\r\n\\end{align*}\r\nTherefore, the desired transition probability matrix is:\r\n \\[\r\n \\mathbf{P} = \r\n \\begin{array}{l}\r\n \\begin{array}{lllllll}\r\n \\hspace{0.175cm} & \\ \\ \\ \\ \\ \\ \\ \\ \\ 0 \\ & \\hspace{0.125cm} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ 1 \\ \\  & \\hspace{0.2cm} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ 2 \\ \\  & \\hspace{0.175cm} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\cdots & N-1 & N \\\\\r\n \\end{array}\\\\\r\n\\begin{array}[c]{c}\r\n0\\\\ \r\n1\\\\\r\n2\\\\\r\n\\vdots\\\\\r\n\\\\[-1em]\r\nN\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n  1 & 0 & 0  & \\cdots & \\ \\ \\ 0 & \\ \\ \\ \\ \\ 0 \\ \\ \\\\ \r\n    (\\frac{N-1}{N}) \\cdot p & (\\frac{1}{N}) \\cdot p + (\\frac{N-1}{N}) \\cdot q & (\\frac{1}{N}) \\cdot q & \\cdots & \\ \\ \\ 0 & \\ \\ \\ \\ \\ 0 \\ \\ \\\\ \r\n  0 & (\\frac{N-2}{N}) \\cdot p & (\\frac{2}{N}) \\cdot p + (\\frac{N-2}{N}) \\cdot q & \\cdots & \\ \\ \\ 0 & \\ \\ \\ \\ \\ 0 \\ \\ \\\\\r\n  \\vdots & \\vdots & \\vdots   & \\ddots & \\ \\ \\ 0 & \\ \\ \\ \\ 0 \\ \\\\\\ \r\n  0 & 0 & 0 & \\cdots & \\ \\ \\ 0 & \\ \\ \\ \\ \\ 1 \\ \\ \\\\ \r\n\\end{Vmatrix}\r\n \\end{array}\r\n \\]", "linkability1": 2.0, "linkability2": 3.0, "linkability3": 2.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [6], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 71, "fields": {"code": "3.4.6", "category": 3, "problem": "Consider the Markov chain whose transition matrix is\r\n\\[P=\r\n\\begin{array}{c||ccccc||}\r\n&0&1&2&3&4\\\\\r\n0&q&p&0&0&0\\\\\r\n1&q&0&p&0&0\\\\\r\n2&q&0&0&p&0\\\\\r\n3&q&0&0&0&p\\\\\r\n4&0&0&0&0&1\\\\\r\n\\end{array}\r\n\\]\r\nwhere $p+q=1$. Determine the mean time to reach state 4 starting from  state 0. That is, find $E[T|X_0=i]$ where $T=\\min\\{n\\geq 0; X_n=4\\}$", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$t_0=\\frac{1}{1-q-pq-p^2q-p^3q-p^4q}$", "choicesb": "$t_0=\\frac{1+p+p^2+p^3}{1-q-pq-p^2q-p^3q-p^4q}$", "choicesc": "$t_0=1+p+p^2+p^3$", "choicesd": "$t_0=\\frac{1}{1-q-pq-p^2q-p^3q-p^4q}$", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Now we want to find the mean time, so the unknowns are the mean time $t_i$. For each state, you have to take one more step to reach the next state.\r\n\\begin{align*}\r\nt_0=qt_0+pt_1+1\\\\\r\nt_1=qt_0+pt_2+1\\\\\r\nt_2=qt_0+pt_3+1\\\\\r\nt_3=qt_0+pt_4+1\\\\\r\nt_4=qt_0+pt_5+1\\\\\r\nt_5=0\r\n\\end{align*}\r\n\r\n\\begin{align*}\r\n\tt_3&=qt_0+p(qt_0+1)+1=qt_0+pqt_0+p+1\\\\\r\n\tt_2&=qt_0+p(qt_0+pqt_0+p+1)+1=qt_0+pqt_0+p^2qt_0+p+1\\\\\r\n\tt_1&=qt_0+p(qt_0+pqt_0+p^2qt_0+p+1)+1=qt_0+pqt_0+p^2qt_0+p^3qt_0+p^2+p+1\\\\\r\n\tt_0&=qt_0+pqt_0+p^2qt_0+p^3qt_0+p^4qt_0+p^3+p^2+p+1\r\n\\end{align*}", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [2, 9, 13], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 72, "fields": {"code": "3.4.9", "category": 3, "problem": "An urn contains five red and three yellow balls. The balls are chosen\r\nat random, one by one, from the urn. Each ball removed is replaced in the\r\nurn by a yellow ball. The selection process continues until all of the red balls\r\nhave been removed from the urn. What is the mean duration of the game?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "8", "choicesb": "12", "choicesc": "274/15", "choicesd": "50/3", "choicese": "44/3", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Define the mean duration $t_i$ of each state(number of red ball)<br>\r\nNow consider the transition probability \r\n\\begin{equation*}\r\nP_{i(i-1)}=i/8\r\n\\end{equation*}\r\nand \r\n\\begin{equation*}\r\nP_{ii}=1-i/8\r\n\\end{equation*}\r\n\r\nso the system of equations are\r\n\\begin{align*}\r\nt_5=\\frac{5}{8}t_4+\\frac{3}{8}t_5+1\\\\\r\nt_4=\\frac{1}{2}t_3+\\frac{1}{2}t_4+1\\\\\r\nt_3=\\frac{3}{8}t_2+\\frac{5}{8}t_3+1\\\\\r\nt_2=\\frac{1}{4}t_1+\\frac{3}{4}t_2+1\\\\\r\nt_1=\\frac{7}{8}t_1+1\\\\\r\n\\end{align*}\r\nso\r\n\\begin{align*}\r\nt_1=8\\\\\r\nt_2=12\\\\\r\nt_3=44/3\\\\\r\nt_4=50/3\\\\\r\nt_5=274/15\r\n\\end{align*}", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [2, 6, 9, 13], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 73, "fields": {"code": "3.4.12", "category": 3, "problem": "A Markov chain $X_0, X_1, X_2,...$ has the transition probability matrix\r\n\\[P=\r\n\\begin{array}{c||ccc||}\r\n&0&1&2\\\\\r\n0&0.3&0.2&0.5\\\\\r\n1&0.5&0.1&0.4\\\\\r\n2&0&0&1\r\n\\end{array}\r\n\\]\r\nand is known to start in state $X_0=0$. Eventually, the process will end up\r\nin state 2. What is the probability that when the process moves into state\r\n2, it does so from state 1?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.150943", "choicesb": "0.528302", "choicesc": "0.849057", "choicesd": "0.471698", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Note that two events have to be defined:<br>\r\n($1\\to2$) with win probability 1 <br>\r\n($0\\to2$) with win probability 0 <br>\r\n\r\nSo we define the unknown:win probability to be $p_1$ and $p_2$<br>\r\n\r\n\\begin{align*}\r\np_0&=0.3p_0+0.2p_1\\\\\r\np_1&=0.5p_0+0.1p_1+0.4(1)\\\\\r\n\\end{align*}\r\nso\r\n\\begin{equation*}\r\np_0=0.150943\\qquad p_1=0.528302\r\n\\end{equation*}\r\nSo the probability is 0.150943", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [2, 9, 12], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 74, "fields": {"code": "3.4.15", "category": 3, "problem": "A simplified model for the spread of a rumor goes this way: There\r\nare $N=5$ people in a group of friends, of which some have heard the\r\nrumor and the others have not. During any single period of time, two people\r\nare selected at random from the group and assumed to interact. The selection\r\nis such that an encounter between any pair of friends is just as\r\nlikely as between any other pair. If one of these persons has heard the\r\nrumor and the other has not, then with probability $a = 0.1$ the rumor is\r\ntransmitted. Let $X_n$, denote the number of friends who have heard the\r\nrumor at the end of the nth period.\r\nAssuming that the process begins at time 0 with a single person knowing\r\nthe rumor, what is the mean time that it takes for everyone to hear it?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "83.3333", "choicesb": "58.3333", "choicesc": "41.6777", "choicesd": "25", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The transition probability for $X_n$ can be written as\r\n$P_{i(i+1)}=\\alpha i(5-i)/\\binom{5}{2}$<br>\r\n$P_{ii}=1-i(5-i)/\\binom{5}{2}$<br>\r\n\r\nNow define the mean time $t_i$ starting at each state\r\n\\begin{align*}\r\nt_1=0.96t_1+0.04t_2+1\\\\\r\nt_2=0.94t_2+0.06t_3+1\\\\\r\nt_3=0.94t_3+0.06t_4+1\\\\\r\nt_4=0.96t_4+1\\\\\r\n\\end{align*} \r\nso\r\n\\begin{align*}\r\nt_1=83.3333\\\\\r\nt_2=58.3333\\\\\r\nt_3=41.6667\\\\\r\nt_4=25\r\n\\end{align*}\r\n\r\nThe answer is $t_1=83.3333$.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [], "wrongproblems": [56], "twinproblems": []}}, {"model": "mathematics.question", "pk": 75, "fields": {"code": "3.4.1", "category": 3, "problem": "A coin is being flipped. How many flips are needed to take, on average: successively flipping a quarter until the pattern $ HHT $ appears, i.e., until you observe two successive heads followed by a tails; or successively flipping a quarter until the pattern $ HTH $ appears?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. $HHT$: 14 flips; $HTH$: 10 flips.", "choicesb": "B. $HHT$: 13 flips; $HTH$: 11 flips.", "choicesc": "C. $HHT$: 12 flips; $HTH$: 12 flips.", "choicesd": "D. $HHT$: 11 flips; $HTH$: 13 flips.", "choicese": "E. $HHT$: 10 flips; $HTH$: 14 flips.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $u_i$ ,$v_j$ be the mean numbers of flip to reach $HHT$ and $HTH$ starting at state $i$ and $j$ respectively, for $i \\in \\{0, H, HH, HHT\\} = \\{0, 1, 2, 3\\}$ and $j \\in \\{0, H, HT, HTH\\} = \\{0, 1, 2, 3\\}$. Then, for the first case:\r\n\t\\begin{align}\r\n\tu_0 &= 1 + \\frac{1}{2} u_0 + \\frac{1}{2} u_1\\\\\r\n\tu_1 &= 1 + \\frac{1}{2} u_0 + \\frac{1}{2} u_2\\\\\r\n\tu_2 &= 1 + \\frac{1}{2} u_0 + \\frac{1}{2} u_3\\\\\r\n\tu_3 &= 0\r\n\t\\end{align}\r\n\tBy solving the above linear system of equations, we have $\\underline{u_0 = 14}$, $u_1 = 12$ and $u_2 = 8$. For the second case:\r\n\t\\begin{align}\r\n\tv_0 &= 1 + \\frac{1}{2} v_0 + \\frac{1}{2} v_1\\\\\r\n\tv_1 &= 1 + \\frac{1}{2} v_1 + \\frac{1}{2} u_2\\\\\r\n\tv_2 &= 1 + \\frac{1}{2} v_2 + \\frac{1}{2} u_3\\\\\r\n\tv_3 &= 0\r\n\t\\end{align}\r\n\tBy solving the above linear system of equations, we have $\\underline{v_0 = 10}$, $v_1 = 8$ and $v_2 = 6$. Therefore, the desired numbers of flips are 14 flips and 10 flips respectively.   \r\n<br> <br>\r\n\tAn intuitive way to think of the reason that the second case takes fewer flips is: it is harder to obtain $HH$ than $HT$ since you have to restart the steps from state 0 if you flipped a tail. In the second case, you stay at state of your previous step if you flipped a tail.", "linkability1": 1.0, "linkability2": 3.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 76, "fields": {"code": "3.4.4", "category": 3, "problem": "Consider the Markov chain whose transition probability matrix is given by\r\n \\[\r\n \\mathbf{P} = \r\n \\begin{array}{l}\r\n \\begin{array}{lllll}\r\n \\hspace{0.075cm} & \\ \\ \\ 0 \\ & \\hspace{0.125cm} \\  1 \\ \\  & \\hspace{0.125cm}   2 \\ \\  & \\hspace{0.125cm}  3 \\  \\ \\ \\ \\\\\r\n \\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n3\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n  1 & 0 & 0 & 0  \\\\ \r\n  0.1 & 0.2 & 0.5 & 0.2 \\\\ \r\n  0.1 & 0.2 & 0.6 & 0.2 \\\\ \r\n  0.2 & 0.2 & 0.3 & 0.3 \\\\ \r\n\\end{Vmatrix}\r\n \\end{array}\r\n \\]\r\nStarting in state $ X_0 = 1 $, determine the probability that the process never visits state 2.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. 0", "choicesb": "B. 11/52", "choicesc": "C. 41/52", "choicesd": "D. 17/26", "choicese": "E. 9/26", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $p_i = \\Pr\\{\\text{reaching state 2} \\ | \\ \\text{start from state} \\ i\\}$. Then, we have:\r\n\t\\begin{align}\r\np_0 &= \\textbf{P}_{00} \\cdot p_0 + \\textbf{P}_{01} \\cdot p_1 + \\textbf{P}_{02} \\cdot p_2 + \\textbf{P}_{03} \\cdot p_3 = p_0 = 0\\\\\r\np_1 &= 0.2 p_1 + 0.5 p_2 + 0.2 p_3\\\\\r\np_2 &= 1\\\\\r\np_3 &= 0.2 p_1 + 0.3 p_2 + 0.3 p_3\\\\\r\n\\end{align}\r\nBy solving the above linear system of equations, we have $p_1 = 41/52$, and $p_3 = 34/52$. The desired probability is\r\n\\begin{align*}\r\n1 - p_1 &= 11/52 \\\\\r\n\t\t&\\approx 0.2115\r\n\\end{align*}", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 77, "fields": {"code": "3.4.7", "category": 3, "problem": "Let $ X_n $ be a Markov chain with transition probabilities $ P_{ij} $. We are given a \u201cdiscount factor\u201d $ \\beta $ with $ 0 < \\beta < 1 $ and a cost function $ c(i) $, and we wish to determine the total expected discounted cost starting from state $ i $, de?ned by \r\n \\[h_i = E \\left[ \\sum_{n = 0}^{\\infty} \\beta^n c(X_n) | X_0 = i\\right] \\]\r\n Using a first step analysis show that $ h_i $ is a solution of one of the following system of linear equations for all states i.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. \\[c(i) + \\beta^i \\sum_{j} \\textbf{P}_{ij} h_j\\] \\", "choicesb": "B. \\[c(i) + \\beta \\sum_{j} \\textbf{P}_{ij} h_j\\] \\", "choicesc": "C. \\[c(0) + \\beta^j \\sum_{j} \\textbf{P}_{ij} h_j\\]", "choicesd": "D. \\[c(0) + \\beta \\sum_{j} \\textbf{P}_{ij} h_j\\]", "choicese": "E. \\[c(i) + \\beta \\sum_{j}  h_j\\]", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "\\begin{align*}\r\nh_i\r\n&= E \\left[ \\sum_{n = 0}^{\\infty} \\beta^n c(X_n) | X_0 = i\\right]\\\\\r\n&= E \\left[  \\sum_{n = 0}^{\\infty} \\beta^n c(X_n) \\cdot 1_{ \\{X_1 = i \\} } | X_0 = i \\right] \\ \\ \\left( \\ \\text{where} \\ \\ 1_{ \\{X_1 = i \\} } = \\begin{cases}\r\n1 &  \\ \\ \\text{if} \\ \\ X_1 = i\\\\\r\n0 &  \\ \\ \\text{if} \\ \\ X_1 \\neq i\\\\\r\n\\end{cases} \\ \\ \\right) \\\\\r\n&= E \\left[  \\sum_{n = 0}^{\\infty} \\beta^n c(X_n) \\cdot 1_{ \\{X_1 = 0 \\} } | X_0 = i \\right] + \r\n   E \\left[  \\sum_{n = 0}^{\\infty} \\beta^n c(X_n) \\cdot 1_{ \\{X_1 = 1 \\} } | X_0 = i \\right] + \\cdots \\\\\r\n&= E \\left[  \\sum_{n = 0}^{\\infty} \\beta^n c(X_n) | X_1 = 0, X_0 = i \\right] \\cdot \\Pr\\{X_1 = 0 | X_0 = i\\}  \\\\\r\n& \\ \\ + E \\left[  \\sum_{n = 0}^{\\infty} \\beta^n c(X_n) | X_1 = 1, X_0 = i \\right] \\cdot \\Pr\\{X_1 = 1 | X_0 = i\\} + \\cdots \\ \\left( \\ \\text{Conditional Independence} \\ \\right) \\\\\r\n&= E \\left[  \\sum_{n = 0}^{\\infty} \\beta^n c(X_n) | X_1 = 0, X_0 = i \\right] \\cdot \\textbf{P}_{i0} + \r\n   E \\left[  \\sum_{n = 0}^{\\infty} \\beta^n c(X_n) | X_1 = 1, X_0 = i \\right] \\cdot \\textbf{P}_{i1} + \\cdots \\\\\r\n&= E \\left[  \\sum_{n = 0}^{\\infty} \\beta^n c(X_n) | X_1 = 0 \\right] \\cdot \\textbf{P}_{i0} + \r\n   E \\left[  \\sum_{n = 0}^{\\infty} \\beta^n c(X_n) | X_1 = 1 \\right] \\cdot \\textbf{P}_{i1} + \\cdots \\ (\\text{property of Markov Chain})\\\\\r\n&= E \\left[  c(X_0) + \\beta c(X_1) + \\cdots | X_1 = 0 \\right] \\cdot \\textbf{P}_{i0} +\r\n   E \\left[  c(X_0) + \\beta c(X_1) + \\cdots | X_1 = 1 \\right] \\cdot \\textbf{P}_{i1} + \\cdots \\\\\r\n&= E \\left[ c(x_0) \\right] \\cdot (\\textbf{P}_{i0} + \\textbf{P}_{i1} + \\cdots) + \r\n   E \\left[ \\beta c(X_1) + \\cdots | X_1 = 0 \\right] \\cdot \\textbf{P}_{i0} +\r\n   E \\left[ \\beta c(X_1) + \\cdots | X_1 = 1 \\right] \\cdot \\textbf{P}_{i1} + \\cdots \\\\\r\n&= E \\left[ c(x_0) \\right] \\cdot 1 + \r\n   \\beta E \\left[  \\sum_{n = 1}^{\\infty} \\beta^{n-1} c(X_n) | X_1 = 0 \\right] \\cdot \\textbf{P}_{i0} +\r\n   \\beta E \\left[  \\sum_{n = 1}^{\\infty} \\beta^{n-1} c(X_n)  | X_1 = 1 \\right] \\cdot \\textbf{P}_{i1} + \\cdots \\\\\r\n&= E \\left[ c(x_0) \\right] + \r\n\\beta \\cdot \\left( E \\left[  \\sum_{n = 0}^{\\infty} \\beta^{n} c(X_{n+1}) | X_1 = 0 \\right] \\cdot \\textbf{P}_{i0} +\r\n\t\t\t\t   E \\left[  \\sum_{n = 0}^{\\infty} \\beta^{n} c(X_{n+1})  | X_1 = 1 \\right] \\cdot \\textbf{P}_{i1} + \\cdots\r\n\t\t    \\right) \\ (\\text{shifting index})\\\\\r\n&= c(i) + \r\n\\beta \\cdot \\sum_{j} \\left( E \\left[  \\sum_{n = 0}^{\\infty} \\beta^{n} c(X_{n+1}) | X_1 = j \\right] \\cdot \\textbf{P}_{ij} \\right) \\\\\r\n&= c(i) + \r\n\\beta \\cdot \\sum_{j} \\left( E \\left[  \\sum_{n = 0}^{\\infty} \\beta^{n} c(X_{n}) | X_0 = j \\right] \\cdot \\textbf{P}_{ij} \\right) \\ (\\text{it takes same number of steps from 0 to $n$ and from 1 to $n+1$}) \\\\\r\n&= c(i) + \r\n\\beta \\cdot \\sum_{j} \\left( h_j \\cdot \\textbf{P}_{ij} \\right) \r\n = \\underline{c(i) + \\beta \\sum_{j} \\textbf{P}_{ij} h_j} \\\\\r\n\\end{align*}", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 3.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [3, 7, 9], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 78, "fields": {"code": "3.4.16", "category": 3, "problem": "An urn contains five tags, of which three are red and two are green. A tag is randomly selected from the urn and replaced with a tag of the opposite color. This continues until only tags of a single color remain in the urn. Let $ X_n $ denote the number of red tags in the urn after the nth draw, with $ X_0 = 3 $. What is the probability that the game ends with the urn containing only red tags?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. 1/2", "choicesb": "B. 3/8", "choicesc": "C. 5/8", "choicesd": "D. 15/32", "choicese": "E. 17/32", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "E", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let \\[p_i = \\Pr \\{ \\text{reach state 5} \\ | \\ \\text{starting at state} \\ i \\}.\\] <br>\r\nThen, the transition probability matrix is \r\n \\[\r\n \\mathbf{P} = \r\n \\begin{array}{l}\r\n \\begin{array}{lllll}\r\n \\hspace{0.125cm} & \\ \\  \\ 0 \\ & \\hspace{0.125cm} \\ \\ 1 \\ \\  & \\hspace{0.125cm} \\  2 \\ \\  & \\hspace{0.125cm}  \\ 3 \\  \\ & \\hspace{0.125cm}  4 & \\hspace{0.125cm}  \\ \\ \\ 5\\\\\r\n \\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n3\\\\\r\n4\\\\\r\n5\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n  1 & 0 & 0 & 0 & 0 & 0 \\\\ \r\n  1/5 & 0 & 4/5 & 0 & 0 & 0  \\\\ \r\n  0 & 2/5 & 0 & 3/5 & 0 & 0  \\\\ \r\n  0 & 0 & 3/5 & 0 & 2/5 & 0 \\\\ \r\n  0 & 0 & 0 & 4/5 & 0 & 1/5 \\\\ \r\n  0 & 0 & 0 & 0 & 0 & 1 \\\\ \r\n\\end{Vmatrix}\r\n \\end{array}\r\n \\]\r\nThen, we have:\r\n\\begin{align}\r\np_0 &= 0\\\\\r\np_1 &= 1/5 p_0 + 4/5 p_2\\\\\r\np_2 &= 2/5 p_1 + 3/5 p_3\\\\\r\np_3 &= 3/5 p_2 + 2/5 p_4\\\\\r\np_4 &= 4/5 p_3 + 1/5 p_5\\\\\r\np_5 &= 1\r\n\\end{align}\r\nBy solving the above linear system of equations, we have $p_1 = 3/8$, $p_2 = 15/32$, $\\underline{p_3 = 17/32}$ and $p_4 = 5/8$. The desired probability is \\[p_3 = 17/32 = 0.53125\\]", "linkability1": 1.0, "linkability2": 3.0, "linkability3": 1.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 79, "fields": {"code": "3.8.3", "category": 3, "problem": "Families in a certain society choose the number of children that they will have according to the following rule: If the first child is a girl, they have exactly one more child. If the first child is a boy, they continue to have children until the first girl, and then cease childbearing. <br>\r\n (a) Let $\\xi$ denotes the number of children of a particular family. For $ k=0,1,2,... $, what is the probability that a particular family will have $ k $ children in total? <br>\r\n (b) Let $\\xi$ denotes the number of male children of a particular family. For $ k=0,1,2,... $, what is the probability that a particular family will have exactly $ k $ male children among their offspring?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. <br>\r\n(a). $P(\\xi = 0) = 0, P(\\xi = 1) = \\frac{1}{2}, P(\\xi = k) = \\left(  \\frac{1}{2}\\right)^k$ for $k \\geq 2$  <br>\r\n(b). $P(\\xi = 0) = \\frac{1}{4}, P(\\xi = k) = \\left(  \\frac{1}{2}\\right)^{k+1}$ for $k \\geq 1$  <br>", "choicesb": "B. <br>\r\n(a). $P(\\xi = 0) = 0, P(\\xi = 1) = 0, P(\\xi = 2) = \\frac{3}{4}, P(\\xi = k) = \\left(  \\frac{1}{2}\\right)^k$ for $k \\geq 3$  <br>\r\n\t(b). $P(\\xi = 0) = \\frac{1}{4}, P(\\xi = 1) = \\frac{1}{2}, P(\\xi = k) = \\left(  \\frac{1}{2}\\right)^{k+1}$ for $k \\geq 2$  <br>", "choicesc": "C. <br>\r\n\t(a). $P(\\xi = 0) = \\frac{1}{4}, P(\\xi = 1) = \\frac{1}{4}, P(\\xi = k) = \\left(  \\frac{1}{2}\\right)^k$ for $k \\geq 2$ <br>\r\n\t(b). $P(\\xi = 0) = \\frac{1}{4}, P(\\xi = 1) = \\frac{1}{2}, P(\\xi = k) = \\left(  \\frac{1}{2}\\right)^{k+1}$ for $k \\geq 2$  <br>", "choicesd": "D. <br>\r\n\t(a). $P(\\xi = 0) = 0, P(\\xi = 1) = 0, P(\\xi = 2) = \\frac{3}{4}, P(\\xi = k) = \\left(  \\frac{1}{2}\\right)^k$ for $k \\geq 3$ <br>\r\n\t(b). $P(\\xi = 0) = \\frac{1}{4}, P(\\xi = 1) = \\frac{1}{4}, P(\\xi = k) = \\left(  \\frac{1}{2}\\right)^{k}$ for $k \\geq 2$  <br>", "choicese": "E. <br>\r\n\t\t(a). $P(\\xi = 0) = \\frac{1}{4}, P(\\xi = 1) = \\frac{1}{4}, P(\\xi = k) = \\left(  \\frac{1}{2}\\right)^k$ for $k \\geq 2$ <br>\r\n\t(b). $P(\\xi = 0) = \\frac{1}{4}, P(\\xi = 1) = \\frac{1}{4}, P(\\xi = k) = \\left(  \\frac{1}{2}\\right)^{k}$ for $k \\geq 2$  <br>", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "(a).\r\n\\begin{align*}\r\nP(\\xi = 0) \r\n&= 0 = P(\\xi = 1), (\\text{all families will have at least 2 baby}) \\\\\r\nP(\\xi = 2) \r\n&= \\frac{1}{2} + \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{3}{4} \\\\\r\n& \\ \\ (\\text{first term refers to the case where first child is a girl, and second  }\\\\ \r\n& \\ \\ \\text{ term refers to another case where the second child is a girl}) \\\\\r\nP(\\xi = k)\r\n&= \\left( \\frac{1}{2} \\right)^k \\ \\text{for} \\ k \\geq 3\\\\\r\n& (\\text{the families stop having more children when their $k$th child is a girl})\r\n\\end{align*}\r\n(b).\r\n\\begin{align*}\r\nP(\\xi = 0) \r\n&= \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4} \\ \\ (\\text{first 2 children are both female}) \\\\\r\nP(\\xi = 1) \r\n&= \\frac{1}{2} \\cdot \\frac{1}{2} + \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{2} \\\\\r\n& \\ \\ (\\text{first term refers to the case where first child is a girl, and second child is a boy. }\\\\ \r\n& \\ \\ \\text{  The second term refers to another case where the second child is a girl}) \\\\\r\nP(\\xi = k)\r\n&= \\left( \\frac{1}{2} \\right)^{k+1} \\ \\text{for} \\ k \\geq 2\\\\\r\n& (\\text{the families stop having more children when their $k$th child is a girl, and so there} \\\\\r\n& \\text{ are (k-1) of boys} )\r\n\\end{align*}", "linkability1": 3.0, "linkability2": 2.0, "linkability3": 3.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [14], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 80, "fields": {"code": "3.9.2", "category": 3, "problem": "One-fourth of the married couples in a far-off society have exactly three children. The other three-fourths of couples continue to have children until the first boy and then cease childbearing. Assume that each child is equally likely to be a boy or girl. Let $\\xi$ denotes the number of male offspring of a pair of married couple, and assumes all the married couple will have at least one child. What is the probability that the male line of descent of a particular husband $u_\\infty$ will eventually die out?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. 1", "choicesb": "B. $\\sqrt{2}/2$", "choicesc": "C. $2(\\sqrt{2} - 1)$", "choicesd": "D. $(\\sqrt{2} - 1)/2$", "choicese": "E. It cannot be determined.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $p_k = \\Pr\\{\\xi = k\\}$. Then,\r\n\\begin{align*}\r\np_0 &= \\frac{1}{4} \\cdot \\left( \\frac{1}{2} \\right)^3 + \\frac{3}{4} \\cdot \\left( \\frac{1}{2} \\right)^3 = \\frac{1}{8}\\\\\r\np_1 &= \\frac{1}{4} \\cdot \\left( \\frac{3}{8} \\right) + \\frac{3}{4} \\cdot \\left( \\frac{1}{2} + \\frac{1}{4} + \\frac{1}{8} \\right) = \\frac{3}{4}\\\\\r\np_2 &= \\frac{1}{4} \\cdot \\left( \\frac{3}{8} \\right) + 0 = \\frac{3}{32}\\\\\r\np_3 &= \\frac{1}{4} \\cdot \\left( \\frac{1}{8} \\right) + 0 = \\frac{1}{32}\\\\\r\n\\end{align*}\r\nThe first terms are the contribution from one-fourth of the married couples who have exactly three children. The second terms are from the other three-fourth of the married couple. Therefore, the probability generation function $\\phi(s)$ is given as\r\n\\[\\phi(s) =E \\left( s^\\xi \\right) = \\sum_{k = 0}^{3} s^k p_k = \\frac{1}{8} + \\frac{3}{4} s + \\frac{3}{32} s^2 + \\frac{1}{32} s^3 \\]\r\nSolving $\\phi(s) - s = 0$, we have:\r\n\\begin{align*}\r\n\\phi(s) - s &= 0 \\\\\r\n \\longrightarrow s^3 + 3s^2 - 8s + 4 &= 0\\\\\r\n (s-1)(s^2 + 4s - 4) &= 0\\\\\r\n s &= 1 \\ \\text{or} \\ \\frac{-4 \\pm \\sqrt{16 + 16}}{2}\\\\\r\n   &= 1 \\ \\text{or} \\ -2 \\pm 2 \\sqrt{2}\\\\\r\n   &= -2(\\sqrt{2} + 1) \\  \\text{or} \\ 2(\\sqrt{2} - 1) \\ \\text{or} \\ 1\\\\\r\n\\because 0 < 2(\\sqrt{2} - 1) < 1, \\\\\r\n\\therefore u_\\infty &= \\underline{2(\\sqrt{2} - 1)} \\approx 0.828\r\n\\end{align*}", "linkability1": 2.0, "linkability2": 3.0, "linkability3": 1.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [14, 20, 21], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 81, "fields": {"code": "3.9.5", "category": 3, "problem": "At time 0, a blood culture starts with one red cell. At the end of 1 min, the red cell dies and is replaced by one of the following combinations with the probabilities as indicated:\r\n\\[\r\n\\begin{array}{rc}\r\n\\hline \r\n\\text{Two red cells} & \\frac{1}{4} \\\\ \r\n\\text{One red cell, One white cell} & \\frac{2}{3} \\\\\r\n\\text{Two white cells} & \\frac{1}{12} \\\\ \r\n \\hline\r\n\\end{array}\r\n\\]\r\nEach red cell lives for 1 min and gives birth to offspring in the same way as the parent cell. Each white cell lives for 1 min and dies without reproducing. Assume that individual cells behave independently.  <br>\r\n(a) Let $\\xi^{(n)}$ be the number of red cells after $n$th generation. At time $ n + \\frac{1}{2} $ min after the culture begins, what is the probability that no white cells have yet appeared? <br>\r\n(b) Let $\\xi$ be the number of red cells in the offspring of a red cell. What is the probability that the entire culture eventually dies out entirely?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. <br>\r\n\t (a). \\[\\left( \\frac{1}{4} \\right)^{2^n} \\] <br>\r\n\t (b). \\[1\\]", "choicesb": "B. <br>\r\n\t (a). \\[\\left( \\frac{1}{4} \\right)^{2^n} \\] <br>\r\n \t (b). \\[ \\frac{1}{3}\\]", "choicesc": "C. <br>\r\n\t (a). \\[\\left( \\frac{1}{4} \\right)^{2^n - 1} \\] <br>\r\n \t (b). \\[1\\]", "choicesd": "D. <br>\r\n \t(a). \\[\\left( \\frac{1}{4} \\right)^{2^n - 1} \\] <br>\r\n \t(b). \\[ \\frac{1}{3}\\]", "choicese": "E. <br>\r\n \t(a). \\[\\left( \\frac{1}{4} \\right)^{2n} \\] <br>\r\n \t(b). \\[ \\frac{1}{3}\\]", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "(a).\r\n\\begin{align*}\r\nP(\\xi^{(0)} =2) &= \\frac{1}{4}\\\\\r\nP(\\xi^{(1)} =2^2 = 4) &= \\left( \\frac{1}{4} \\right)^2\\\\\r\n\\vdots \\ \\ \\ \\ \\ \\ & \\ \\ \\ \\ \\ \\ \\ \\vdots\\\\\r\nP(\\xi^{(n-1)} = 2^{n}) &= \\left( \\frac{1}{4} \\right)^{2^{n-1}}\\\\ \r\n\\therefore \\ P( \\text{no white cells for} \\ n \\text{th generation})\r\n&= P(\\xi^{(0)} \\cap \\xi^{(1)} \\cap \\cdots \\cap \\xi^{(n-1)}) \\  (\\text{there are $n$ terms in total})\\\\\r\n&= \\left( \\frac{1}{4} \\right) \\left( \\frac{1}{4} \\right)^2 \\left( \\frac{1}{4} \\right)^4 \\cdots \\left( \\frac{1}{4} \\right)^{2^{n-1}}\\\\\r\n&=\\left( \\frac{1}{4} \\right)^{1 + 2 + 4 + \\cdots + 2^{n-1}}\\\\\r\n&= \\underline{\\left( \\frac{1}{4} \\right)^{2^n - 1}}\r\n\\end{align*}\r\n(b).\r\n\\[ P(\\xi = 0) = \\frac{1}{12}, \\ P(\\xi = 1) = \\frac{2}{3}, \\ P(\\xi = 2) = \\frac{1}{4} \\]\r\nThe probability generation function $\\phi(s)$ is given as\r\n\\[ \\phi(s) = \\frac{1}{12} + \\frac{2}{3} s + \\frac{1}{4} s^2 \\]\r\nSolving $\\phi(s) - s = 0$, we have:\r\n\\begin{align*}\r\n\\phi(s) - s &= 0 \\\\\r\n\\longrightarrow 3s^2 - 4s + 1 &= 0\\\\\r\n(s-1)(3s - 1) &= 0\\\\\r\ns &= 1 \\ \\text{or} \\ \\frac{1}{3}\\\\\r\n\\because 0 < \\frac{1}{3} < 1, \\ \\\r\n\\therefore u_\\infty &= \\underline{\\frac{1}{3}} \r\n\\end{align*}", "linkability1": 2.0, "linkability2": 3.0, "linkability3": 2.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [14, 20, 21], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 82, "fields": {"code": "3.8.2", "category": 3, "problem": "Let $Z=\\sum_{n=0}^{\\infty}X_n$, be the total family size in a branching process\r\nwhose offspring distribution has a mean $\\mu = E[\\xi] < 1$. Assuming that\r\n$X_0 = 1$, what is $E[Z]$?.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$1/(1+\\mu)$", "choicesb": "$1/(1-\\mu)$", "choicesc": "$\\mu/(1-\\mu)$", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "\\begin{align*}\r\nE[z]&=\\sum_{n=0}^{\\infty}E[X_n]=\\sum_{n=0}^{\\infty}\\mu^n=1/(1-\\mu)\r\n\\end{align*}", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [2, 16], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 83, "fields": {"code": "3.9.8", "category": 3, "problem": "Consider a branching process whose offspring follow the geometric distribution $p_k = ( 1 - c) \\ c^k$ for $k = 0,1,...$, where $0.5 < c < 1$. Determine the probability of eventual extinction.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. \\[0\\]", "choicesb": "B. \\[1\\]", "choicesc": "C. \\[\\frac{2c - 1}{2c}\\]", "choicesd": "D. \\[\\frac{1 -c}{1 + c}\\]", "choicese": "E. \\[\\frac{1 -c}{c}\\]", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "E", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "\\begin{align*}\r\np_k \r\n&= (1-c) \\ c^k \\ \\ \\text{for} \\ \\ k = 0,1,... , \\ \\text{where} \\ \\ c \\in (0,1) \\\\\r\n\\therefore \\ \\ \\phi(s)\r\n&= (1-c) + (1-c) \\ c \\cdot s + (1-c) \\ c^2 \\cdot s^2 + \\cdots \\\\\r\n&= (1-c) \\cdot [1 + cs + (cs)^2 + \\cdots]\\\\\r\n&= (1-c) \\ \\sum_{k = 0}^{\\infty} (cs)^k \\\\\r\n&= (1-c) \\cdot \\frac{1}{1 - cs} \\ \\text{(using Taylor Series Expansion to obtain the latter term from the infinite sum)}\\\\\r\n&= \\frac{1 - c}{1 - cs}\r\n\\end{align*}\r\nSolving $\\phi(s) - s = 0$, we have: \r\n\\begin{align*}\r\n\\frac{1 - c}{1 - cs}\r\n&= s \\\\\r\n\\rightarrow c \\cdot s^2 - s + (1 - c)\r\n&= 0\\\\\r\ns\r\n&= \\frac{1 \\pm \\sqrt{1^2 - 4c(1-c)}}{2c}\\\\\r\n&= \\frac{1 \\pm \\sqrt{4c^2 - 4c + 1}}{2c}\\\\\r\n&= \\frac{1 \\pm \\sqrt{(2c - 1)^2}}{2c} = \\frac{1 \\pm (2c -1)}{2c} = 1 \\ \\text{or} \\ \\frac{1-c}{c}\\\\ \\\\\r\n\\therefore \\ \\ u_\\infty \r\n&= \\begin{cases}\r\n1 & \\text{if} \\ \\ \\ c \\leq 0.5  \\\\\r\n\\frac{1 - c}{c} & \\text{if} \\ \\ \\ 0.5 < c < 1\\\\\r\n\\end{cases}\r\n\\end{align*}", "linkability1": 3.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [14, 20, 21], "rightproblems": [], "wrongproblems": [131], "twinproblems": [131]}}, {"model": "mathematics.question", "pk": 84, "fields": {"code": "3.9.1", "category": 3, "problem": "One-fourth of the married couples in a far-off society have no children\r\nat all. The other three-fourths of families have exactly three children,\r\neach child equally likely to be a boy or a girl. What is the probability that\r\nthe male line of descent of a particular husband will eventually die out?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0", "choicesb": "$\\frac{\\sqrt{69}-6}{3}$", "choicesc": "1", "choicesd": "$\\frac{-\\sqrt{69}-6}{3}$", "choicese": "none of the above", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The probability of having\r\n0 boy:$1/4+3/4(1/8)=11/32$<br>\r\n1 boy:$3/4(3/8)=9/32$<br>\r\n2 boy:$3/4(3/8)=9/32$<br>\r\n3 boy:$3/4(1/8)=3/32$<br>\r\nApply $u_{\\infty}=\\phi(u_{\\infty})$\r\n\\begin{equation*}\r\nu_{\\infty}=11/32+(9/32)u_{\\infty}+(9/32)u_{\\infty}^2+(3/32)u_{\\infty}^3\r\n\\end{equation*}\r\nSo\r\n\\begin{equation*}\r\nu_{\\infty}=\\frac{-\\sqrt{69}-6}{3}\\text{  or  }\\frac{\\sqrt{69}-6}{3}\\text{  or  }1\r\n\\end{equation*}\r\nSince $u_{\\infty}$ converge to the smallest solution between 0 and 1, the answer is $\\frac{\\sqrt{69}-6}{3}$.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [2, 20, 21, 22], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 85, "fields": {"code": "3.9.7", "category": 3, "problem": "Families in a certain society choose the number of children that\r\nthey will have according to the following rule: If the first child is a girl,\r\nthey have exactly one more child. If the first child is a boy, they continue\r\nto have children until the first girl and then cease childbearing. Let $\\xi$ be\r\nthe number of male children in a particular family. Determine the mean of $\\xi$ directly or by differentiating the generating function.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "2", "choicesb": "0.5", "choicesc": "1.75", "choicesd": "2.5", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Consider the second rule, $\\xi$ obviously follow the geometric distribution.\r\nConsider the first rule, there is half the probability having one male child.\r\n\r\nSo \r\nPr\\{$\\xi=0$\\}$=\\frac{1}{2^2}$<br>\r\nPr\\{$\\xi=1$\\}$=\\frac{1}{2^2}$<br>\r\nPr\\{$\\xi=k>1$\\}$=\\frac{1}{2^k}$<br>\r\n\\begin{align*}\r\n\\phi(s)=\\frac{1}{2^2}+\\frac{1}{2^2}s+\\sum_{k=2}^{\\infty}\\frac{1}{2^k}s^k=\\frac{1}{2^2}+\\frac{1}{2^2}s+\\frac{s^2/4}{1-s/2}\r\n\\end{align*} \r\nOnce we differentiate the probability generating function, \r\n\\begin{align*}\r\n\\phi'(s)=\\frac{1}{2^2}-\\frac{s(s-4)}{2(s-2)^2}\\\\\r\n\\phi'(1)=\\frac{1}{2^2}+\\frac{3}{2}\r\n\\end{align*}\r\nIf we consider the mean of $\\xi$ directly, it is complicated as it involve clever shifting of index so that the $k$ is cancelled.\r\n\\begin{align*}\r\nE[\\xi]&=\\frac{1}{2^2}+\\sum_{k=2}^{\\infty}\\frac{k}{2^{k-1}}(1-\\frac{1}{2})\\\\\r\n&=\\frac{1}{2^2}+\\sum_{k=2}^{\\infty}\\frac{k}{2^{k-1}}-\\frac{k}{2^{k}}\\\\\r\n&=\\frac{1}{2^2}+\\sum_{k=1}^{\\infty}\\frac{k+1}{2^{k}}-\\frac{k+1}{2^{k+1}}\\\\\r\n&=\\frac{1}{2^2}+\\sum_{k=0}^{\\infty}\\frac{k}{2^{k+1}}+\\left(\\frac{2}{2^{k+1}}-\\frac{1}{2^{k+2}}\\right)-\\sum_{k=1}^{\\infty}\\frac{k}{2^{k+1}}\\\\\r\n&=\\frac{1}{2^2}+\\sum_{k=0}^{\\infty}\\frac{k}{2^{k+1}}+\\left(\\frac{2}{2^{k+1}}-\\frac{1}{2^{k+2}}\\right)-\\sum_{k=0}^{\\infty}\\frac{k}{2^{k+1}}\\\\\r\n&=\\frac{1}{2^2}+\\sum_{k=0}^{\\infty}\\left(\\frac{2}{2^{k+1}}-\\frac{1}{2^{k+2}}\\right)\\\\\r\n&=\\frac{1}{2^2}+\\frac{3}{2}\\\\\r\n\\end{align*}\r\nAnother way of doing it is to calculate the expectation value of a geometric distribution from 1 is $\\frac{1}{p}=2$. Now we start at $k=2$, so we subtract the contribution of $k=1$:$\\frac{1}{2}$. So the value of the second term is $2-\\frac{1}{2}=\\frac{3}{2}$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 86, "fields": {"code": "3.9.10.a", "category": 3, "problem": "Suppose that in a branching process the number of offspring of an initial particle has a distribution whose generating function is $f(s)$. Each member of the first generation has a number of offspring whose distribution has generating function $g(s)$. The next generation has generating function $f$, the next has $g$ , and the distribution continue to alternate in this way from generation to generation. <br>\r\nDetermine the extinction probability of the process in terms of $f(s)$ and $g(s)$.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$(f+g)(u_{\\infty})$", "choicesb": "$(fg)(u_{\\infty})$", "choicesc": "$f\\circ g(u_{\\infty})$", "choicesd": "$f\\ast g(u_{\\infty})$<br>\r\n$\\ast$ denote convolution operation", "choicese": "$g\\circ f(u_{\\infty})$", "choicesf": "$f^g(u_{\\infty})$", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Here,  the key idea is to treat each two transition as one transition. First we have $\\xi_1\\to k_1$ and $\\xi_2\\to k_{2,i}$(i run from 1 to $k_1$, denoting the index of the second transition) for the two process $f$ and $g$.<br>\r\nThe outcome $k$ of each two transition(there are $k_1$ roll for $\\xi_2$, each $\\xi_2$ give out $k_2$) is then \r\n\\begin{equation*}\r\nk=\\sum_{i=0}^{k_1}k_{2,i}\r\n\\end{equation*}\r\nFor each outcome $\\sum_{i=0}^{k_1}k_{2,i}$,  the probability is\r\n\\begin{equation*}\r\nP(k)={\\xi}_1(k_1)\\prod_{i=0}^{k_1}{\\xi}_2(k_{2,i})\r\n\\end{equation*}\r\nNow we have our generating function $\\phi(s)$ for the two steps transition, note to run over all the outcomes, the summation is $\\sum_k=\\sum_{k_1}\\sum_{i=0}^{k_1}$\r\n\\begin{align*}\r\n\\phi(s)=\\sum_kP(k)s^k&=\\sum_{k_1}\\sum_{i=0}^{k_1}s^{\\sum_{i=0}^{k_1}k_{2,i}}{\\xi}_1(k_1)\\prod_{i=0}^{k_1}{\\xi}_2(k_{2,i})\\\\\r\n&=\\sum_{k_1}{\\xi}_1(k_1)\\sum_{i=0}^{k_1}\\prod_{i=0}^{k_1}{\\xi}_2(k_{2,i})s^{k_{2,i}}\r\n\\end{align*}\r\nSince each term have element $\\prod_{i=0}^{k_1}{\\xi}_2(k_{2,i})s^{k_{2,i}}$ and we have to run over all possible outcomes $k$ (usually come from multiplication of series), it is natural to think of $g^{k_1}$.<br>\r\nAfter identifying the $g^{k_1}$ term, everything is straight forward:\r\n\\begin{align*}\r\n\\phi(s)&=\\sum_{k_1}{\\xi}_1(k_1)\\sum_{i=0}^{k_1}\\prod_{i=0}^{k_1}{\\xi}_2(k_{2,i})s^{k_{2,i}}\\\\&=\\sum_{k_1}\\xi_1(k_1)g^{k_1}=f\\circ g(s)\r\n\\end{align*}\r\nTherefore, the extinction probability is $f\\circ g(u_{\\infty})$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [20, 21, 22, 28], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 87, "fields": {"code": "3.4.10", "category": 3, "problem": "You have five fair coins. You toss them all so that they randomly fall heads or tails. Those that fall tails in the first toss you pick up and toss again. You toss again those that show tails after the second toss, and so on, until all show heads. Let $ X $ be the number of coins involved in the last toss. Find $ \\Pr\\{X=1\\} $.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. 2/3", "choicesb": "B. 5/7", "choicesc": "C. 29/105", "choicesd": "D. 76/105", "choicese": "E. 157/217", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "E", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $X_n$ be the number of tails at the end of period $n$. Then, $\\{X_n\\}$ is a Markov chain with state $ \\{0, 1, 2, 3, 4, 5\\} $. \\\\ \\\\\r\nIf 0 tail appears in the previous toss, the process will be ended. So it is an absorbing state:\r\n\\begin{align*}\r\n\\Pr\\{X_1 = 0 | X_0 = 0\\} &= 1\\\\\r\n\\Pr\\{X_1 = 0 | X_0 \\neq 0\\} &= 0\\\\\r\n\\end{align*}\r\nIf 1 tail appears in the previous toss, 0 or 1 tail will appear in this toss where both probabilities are 1/2:\r\n\\begin{align*}\r\n\\Pr\\{X_1 = 0 | X_0 = 1\\} &= \\frac{1}{2} = \\Pr\\{X_1 = 1 | X_0 = 1\\} \\\\\r\n\\Pr\\{X_1 = 0 | X_0 \\neq 0 \\ \\text{or} \\  1 \\} &= 0\\\\\r\n\\end{align*}\r\nIf 2 tails appear in the previous toss, 0, 1 or 2 tail(s) will appear in this toss where the probabilities are 1/4, 1/2 and 1/4 respectively:\r\n\\begin{align*}\r\n\\Pr\\{X_1 = 0 | X_0 = 2\\} &= \\frac{1}{4} = \\Pr\\{X_1 = 2 | X_0 = 2\\} \\\\\r\n\\Pr\\{X_1 = 1 | X_0 = 2\\} &= \\frac{1}{2}\\\\\r\n\\Pr\\{X_1 = 0 | X_0 \\neq 0, 1 \\ \\text{or} \\  2 \\} &= 0\\\\\r\n\\end{align*}\r\nIf 3 tails appear in the previous toss, 0, 1, 2 or 3 tail(s) will appear in this toss where the probabilities are 1/8, 3/8, 3/8 and 1/8 respectively:\r\n\\begin{align*}\r\n\\Pr\\{X_1 = 0 | X_0 = 3\\} &= \\frac{1}{8} = \\Pr\\{X_1 = 3 | X_0 = 3\\} \\\\\r\n\\Pr\\{X_1 = 1 | X_0 = 3\\} &= \\frac{3}{8} = \\Pr\\{X_1 = 2 | X_0 = 3\\}\\\\\r\n\\Pr\\{X_1 = 0 | X_0 \\neq 0, 1, 2 \\ \\text{or} \\  3 \\} &= 0\\\\\r\n\\end{align*}\r\nIf 4 tails appear in the previous toss, 0, 1, 2, 3, 4 tail(s) will appear in this toss where the probabilities are 1/16, 1/4, 3/8, 1/4 and 1/16 respectively:\r\n\\begin{align*}\r\n\\Pr\\{X_1 = 0 | X_0 = 4\\} &= \\frac{1}{16} = \\Pr\\{X_1 = 4 | X_0 = 4\\} \\\\\r\n\\Pr\\{X_1 = 1 | X_0 = 4\\} &= \\frac{1}{4} = \\Pr\\{X_1 = 3 | X_0 = 4\\}\\\\\r\n\\Pr\\{X_1 = 2 | X_0 = 4\\} &= \\frac{3}{8}\\\\\r\n\\Pr\\{X_1 = 0 | X_0 = 5\\} &= 0\\\\\r\n\\end{align*}\r\nIf 5 tails appear in the previous toss, 0, 1, 2, 3, 4 or 5 tail(s) will appear in this toss where the probabilities are 1/32, 5/32, 5/16, 5/16, 5/32 and 1/32 respectively:\r\n\\begin{align*}\r\n\\Pr\\{X_1 = 0 | X_0 = 5\\} &= \\frac{1}{32} = \\Pr\\{X_1 = 5 | X_0 = 5\\} \\\\\r\n\\Pr\\{X_1 = 1 | X_0 = 5\\} &= \\frac{5}{32} = \\Pr\\{X_1 = 4 | X_0 = 5\\}\\\\\r\n\\Pr\\{X_1 = 2 | X_0 = 5\\} &= \\frac{5}{16} = \\Pr\\{X_1 = 3 | X_0 = 5\\}\\\\\r\n\\end{align*}\r\nTherefore, the transition probability matrix is given as\r\n \\[\r\n\\mathbf{P} = \r\n\\begin{array}{l}\r\n\\begin{array}{lllllll}\r\n\\hspace{0.125cm} &   \\ \\ \\ \\ 0 \\ & \\hspace{0.15cm} \\ \\ \\ 1 \\ \\  & \\hspace{0.125cm}  \\ \\ \\ 2 \\ \\ & \\hspace{0.125cm}  \\  \\ \\ 3 \\ \\ \\ \\ & \\hspace{0.125cm} \\  4 \\  \\ \\ \\ & \\hspace{0.125cm}  \\ 5 \\  \\ \\ \\ \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n3\\\\\r\n4\\\\\r\n5\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n1  \t & 0\t& 0 \t& 0 \t& 0 \t& 0    \\\\ \r\n1/2  & 1/2  & 0 \t& 0 \t& 0 \t& 0     \\\\ \r\n1/4  & 1/2  & 1/4 \t& 0 \t& 0 \t& 0   \\\\ \r\n1/8  & 3/8 \t& 3/8 \t& 1/8 \t& 0 \t& 0  \\\\ \r\n1/16 & 1/4 \t& 3/8 \t& 1/4 \t& 1/16 \t& 0  \\\\ \r\n1/32 & 5/32 & 5/16 \t& 5/16 \t& 5/32 \t& 1/32  \\\\ \r\n\\end{Vmatrix}\r\n\\end{array}\r\n\\]\r\nLet\r\n\\[ \\ p_i = \\Pr\\{ \\text{reaching state 1 without passing state 0} \\ | \\ \\text{begining from state} \\ i \\}\\]\r\n\\[ \\therefore p_0 = 0, p_1 = 1 \\]\r\n\\begin{align}\r\np_2 &= \\frac{1}{4} p_0 + \\frac{1}{2} p_1 + \\frac{1}{4} p_2 = \\frac{1}{2} + \\frac{1}{4} p_2 \\\\\r\np_3 &= \\frac{3}{8} + \\frac{3}{8} p_2 + \\frac{1}{8} p_3 \\\\\r\np_4 &= \\frac{1}{4} + \\frac{3}{8} p_2 + \\frac{1}{4} p_3 + \\frac{1}{16} p_4 \\\\\r\np_5 &= \\frac{5}{32} + \\frac{5}{16} p_2 + \\frac{5}{16} p_3 + \\frac{5}{32} p_4 + \\frac{1}{32} p_5\r\n\\end{align}\r\nBy solving the above linear system of equations, we have $p_2 = 2/3$, $p_3 = 5/7$, $p_4 = 76/105$ and $\\underline{p_5 = 157/217}$. The desired probability is\r\n\\begin{align*}\r\n\\Pr\\{X = 1\\} &=  p_5 \\\\\r\n&= 157/217 \\approx 0.7235\r\n\\end{align*}", "linkability1": 2.0, "linkability2": 3.0, "linkability3": 3.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 88, "fields": {"code": "3.4.19", "category": 3, "problem": "$\\textit{Computer Challenge}$. Let $ N $ be a positive integer and let $ Z_1,...,Z_N $ be independent random variables, each having the geometric distribution\r\n \\[\\Pr\\{ Z = k \\} = \\left( \\frac{1}{2} \\right) ^k , \\text{for} \\ k=1,2,....\\]\r\n  Since these are discrete random variables, the maximum among them may be unique, or there may be ties for the maximum. Let $ p_N $ be the probability that the maximum is unique. How does $ p_N $ behave when $ N $ is large? (Alternative formulation: You toss $ N $ dimes. Those that are heads you set aside; those that are tails you toss again. You repeat this until all of the coins are heads. Then, $ p_N $ is the probability that the last toss was of a single coin.)", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. It will converge to 0.", "choicesb": "B. It will converge to a certain number between 0 and 1.", "choicesc": "C. It will converge to 1.", "choicesd": "D. It will diverge.", "choicese": "E. It cannot be determined. It behaves like a random variable independent of $N$.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "theall/image/P3.4.19.png", "solutionspicture2": "", "solutionspicture3": "", "solutions": "This problem is an extension of Problem 3.4.10. In other words, the setup in Problem 3.4.10 is a special case of this problem with $ N $ = 5. The transition probability matrix is given as\r\n \\[\r\n\\mathbf{P} = \r\n\\begin{array}{l}\r\n\\begin{array}{lllllll}\r\n\\hspace{0.1cm} & \\ \\ \\ \\ \\  0 \\ & \\hspace{0.15cm} \\  1 \\ \\  & \\hspace{0.125cm} \\   2 \\ \\  & \\hspace{0.125cm}  \\ 3 & \\hspace{0.2cm}  \\cdots   & \\hspace{0.175cm}  N \\  \\ \\ \\ \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n3\\\\\r\n\\vdots\\\\\r\nN\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n1  \t & 0\t& 0 \t& 0 \t& \\cdots \t& 0    \\\\ \r\n1/2  & 1/2  & 0 \t& 0 \t& \\cdots \t& 0     \\\\ \r\n1/4  & 1/2  & 1/4 \t& 0 \t& \\cdots \t& 0   \\\\ \r\n1/8  & 3/8 \t& 3/8 \t& 1/8 \t& \\cdots \t& 0  \\\\ \r\n\\vdots & \\vdots \t& \\vdots \t& \\vdots \t& \\ddots \t& \\vdots  \\\\ \r\n\\frac{C^N_0}{2^N} & \\frac{C^N_1}{2^N} & \\frac{C^N_2}{2^N} \t& \\frac{C^N_3}{2^N} \t& \\cdots \t& \\frac{C^N_N}{2^N}  \\\\ \r\n\\end{Vmatrix}\r\n\\end{array}\r\n\\]\r\n\\[\r\n\\text{where} \\ C^n_r = \r\n\\begin{pmatrix}\r\nn\\\\ \r\nr\\\\\r\n\\end{pmatrix} = \\frac{n ! }{(n - r)! \\ r!} \\\r\n\\text{is called the \\textit{combination} or \\textit{binomial coefficient}.}\r\n\\]\r\nLet \\[ \r\np_i = \\Pr\\{\\text{reaching state 1 without passing through state 0} \\ | \\ \\text{begining from state} \\ i \\}.\r\n\\]\r\nThen, \\[\r\np_0 = 0, p_1 = 1, \\]\r\n\\[\r\np_i = \\sum_{j = 0}^{N} \\textbf{P}_{ij} p_j \\ \\text{for} \\ i \\neq 0 \\ \\text{or} \\ 1 \r\n\\]\r\nA matrix equation can be set up:\r\n\\[ \\left[\r\n\\mathbf{I}_N - \\begin{pmatrix}\r\n0 & 0 & 0 & \\cdots & 0\\\\\r\n0 & 0 & 0 & \\cdots & 0\\\\\r\n1/4 & 1/2 & 1/4 & \\cdots & 0\\\\\r\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\r\n\\frac{C^N_0}{2^N} & \\frac{C^N_1}{2^N} & \\frac{C^N_2}{2^N} \t& \\frac{C^N_3}{2^N} \t& \\cdots \t& \\frac{C^N_N}{2^N}  \\\\\r\n\\end{pmatrix}\r\n\\right] \\begin{bmatrix}\r\np_0\\\\\r\np_1\\\\\r\np_2\\\\\r\n\\vdots\\\\\r\np_N\\\\\r\n\\end{bmatrix}\r\n= \\begin{bmatrix}\r\n1\\\\\r\n0\\\\\r\n0\\\\\r\n\\vdots\\\\\r\n0\\\\\r\n\\end{bmatrix}\r\n\\]\r\n\\[\r\n\\text{where} \\ I \\ \\text{is the $N$ by $N$ identity matrix.}\r\n\\]\\\\\r\nBy solving the above matrix equation, we have:\r\n\\[\r\np_5 = \\frac{157}{217} \\approx 0.7235, \\ p_{10} = \\frac{1436}{1991} \\approx 0.7212, \\ p_{20} = \\frac{999}{1385} \\approx 0.7213,\\]\r\n\\[\r\np_{50} = \\frac{1429}{1981} \\approx 0.7214, \\ p_{100} = \\frac{919}{1274} \\approx 0.7214, \\ p_{150} = \\frac{409}{567} \\approx 0.7213\\]\r\nTherefore, we can see that the probability $p_N$ is converging to approximately 0.7213 when $ N $ is large. <br>\r\nYou can also see how $p_N$ varies from $N = 1$ to $N = 150$ in the following figure.", "linkability1": 3.0, "linkability2": 3.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [6, 9], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 89, "fields": {"code": "3.4.13", "category": 3, "problem": "A Markov chain $ X_0,X_1,X_2,... $ has the transition probability matrix \r\n  \\[\r\n \\mathbf{P} = \r\n \\begin{array}{l}\r\n \\begin{array}{llll}\r\n \\hspace{0.075cm} & \\ \\ \\ 0 \\ & \\hspace{0.125cm} \\  1 \\ \\  & \\hspace{0.125cm}   2 \\ \\  \\\\\r\n \\end{array}\\\\\r\n \\begin{array}{c}\r\n 0\\\\\r\n 1\\\\\r\n 2\\\\\r\n \\end{array}\r\n \\begin{Vmatrix}\r\n 0.3 & 0.2 & 0.5   \\\\ \r\n 0.5 & 0.1 & 0.4  \\\\ \r\n 0 & 0 & 1  \\\\ \r\n \\end{Vmatrix}\r\n \\end{array}\r\n \\]\r\n and is known to start in state $ X_0 =0 $. Eventually, the process will end up in state 2. What is the probability that the time $ T = \\min \\{n \\geq 0; X_n = 2 \\} $ is an odd number?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. 43/133", "choicesb": "B. 53/133", "choicesc": "C. 80/133", "choicesd": "D. 90/133", "choicese": "E. 100/133", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let\\[\r\nZ_n = (OE(T),X_n), \\\\ \\] \r\n\\[\\text{where} \\ OE(T) = \\begin{cases}\r\nO & \\text{if} \\ \\ \\frac{1 + (-1)^T}{2} = 1\\\\\r\nE & \\text{if} \\ \\ \\frac{1 + (-1)^T}{2} = 0\\\\\r\n\\end{cases}\r\n \\ \\text{is an odd/even number indicator function.}\r\n\\]\r\nThen, $Z_n$ is a Markov chain with state $ \\{0, 1, 2, 3, 4, 5 \\} = \\{(O,0), (O,1), (O,2), (E,0), (E,1), (E,2) \\} $. The transition probability matrix now becomes:\r\n \\[\r\n\\mathbf{P} = \r\n\\begin{array}{l}\r\n\\begin{array}{lllllll}\r\n\\hspace{0.075cm} & \\ \\ \\ 0 \\ & \\hspace{0.125cm} \\  1 \\ \\  & \\hspace{0.125cm}   2 \\ \\  & \\hspace{0.125cm}  3 \\  & \\hspace{0.15cm}  4 \\  \\ \\ & \\hspace{0.1cm}  5 \\  \\ \\ \\ \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n3\\\\\r\n4\\\\\r\n5\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n0  \t \t& 0\t\t& 0 \t& 0.3 \t& 0.2 \t& 0.5    \\\\ \r\n0  \t \t& 0\t\t& 0 \t& 0.5 \t& 0.1 \t& 0.4    \\\\  \r\n0  \t \t& 0\t\t& 0 \t& 0 \t& 0 \t& 1    \\\\ \r\n0.3 \t& 0.2 \t& 0.5  \t& 0 \t& 0 \t& 0    \\\\ \r\n0.5 \t& 0.1 \t& 0.4\t& 0 \t& 0 \t& 0    \\\\ \r\n0 \t\t& 0 \t& 1 \t& 0 \t& 0 \t& 0    \\\\ \r\n\\end{Vmatrix}\r\n\\end{array}\r\n\\]\r\nLet \\[\r\np_i = \\Pr\\{\\text{reaching state 2 without passing state 5} \\ | \\ \\text{begining from state} \\ i \\}.\r\n\\]\r\nThen, we have:\r\n\\[\r\np_2 = 1, p_5 = 0,\r\n\\]\r\n\\begin{align}\r\np_0 &= 0.3p_3 + 0.2p_4 + 0.5p_5\\\\\r\np_1 &= 0.5p_3 + 0.1p_4 + 0.4p_5\\\\\r\np_2 &= 1\\\\\r\np_3 &= 0.3p_0 + 0.2p_1 + 0.5p_2\\\\\r\np_4 &= 0.5p_0 + 0.1p_1 + 0.4p_2\\\\\r\np_5 &= 0\r\n\\end{align}\r\nBy solving the above linear system of equations, we have $\\underline{p_0 = 43/133}$, $p_1 = 53/133$, $p_3 = 90/133$ and $p_4 = 80/133$. The desired probability is\r\n\\[\r\np_0 = 43/133\r\n\\]", "linkability1": 2.0, "linkability2": 3.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 90, "fields": {"code": "3.3.1", "category": 2, "problem": "Consider a spare parts inventory model in which either 0, 1, or 2 repair parts \r\nare demanded in any period, with:  Pr$(\\xi_n=0)=0.4;$  Pr$(\\xi_n=1)=0.3$; Pr$(\\xi_n=2)=0.3;$\r\nand suppose $s=0$ and $S=3$. Determine the transition probability matrix for\r\nthe Markov chain $\\{X_n\\}$, where $\\{X_n\\}$ is defined to be the quantity on hand at the\r\nend-of-period $n$.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "\\begin{eqnarray*}\r\n&&  ~~ \\,\\, $-1$ \\quad \\, 0 \\quad \\, 1\\quad\\,\r\n2\\quad \\, 3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{-1}  \\cr \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n0 & 0  & .3 & .3 & .4    \\cr\r\n0 & 0  & .3 & .3 & .4    \\cr\r\n.3 & .3  & .4 & 0& 0   \\cr\r\n0 & .3  & .3 & .4 & 0    \\cr\r\n0 & 0  & .3 & .3 & .4\r\n}\r\n\\end{eqnarray*}", "choicesb": "\\begin{eqnarray*}\r\n&&  ~~ \\,\\, $-1$ \\quad \\, 0 \\quad \\, 1\\quad\\,\r\n2\\quad \\, 3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{-1}  \\cr \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n0 & .4  & .3 & .3 & 0    \\cr\r\n0 & 0  & .3 & .3 & .4    \\cr\r\n.3 & .3  & .4 & 0& 0   \\cr\r\n0 & .3  & .3 & .4 & 0    \\cr\r\n0 & 0  & .3 & .3 & .4\r\n}\r\n\\end{eqnarray*}", "choicesc": "\\begin{eqnarray*}\r\n&&  ~~ \\,\\, $-1$ \\quad \\, 0 \\quad \\, 1\\quad\\,\r\n2\\quad \\, 3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{-1}  \\cr \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n0 & 0  & .3 & .3 & .4    \\cr\r\n0 & 0  & .3 & .3 & .4    \\cr\r\n0 & .3  & .4 & .3& 0   \\cr\r\n0 & .3  & .3 & .4 & 0    \\cr\r\n0 & 0  & .3 & .3 & .4\r\n}\r\n\\end{eqnarray*}", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Solving this problem correctly requires more of understanding the problem correctly. In other words, it \r\nrequires more of language skills rather than mathematics/statistics skills.\r\nConsider that the each element(e.g. the element$(i,j)$) in the transition matrix shows the probability of transferring to state $j$ in the next step given the current state $i$.\r\n<br>Look at the first row, if the inventory today is $-1$ or $0$, then we'll get supplement . At the beginning of tomorrow, the number of inventory will be 3. The number of inventory sold could be$ 0,1,2$. So the remaining can only be $1,2,3$. So we can get the first row as:$ 0,0,0.3,0.3,0.4$.\r\nThe analysis to other rows is similar.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Oh, you'd better think this problem again.", "messagesuccess": "Congratulations, you get the point of transition matrix.", "sensitivity": null, "gussingparameter": 0.33, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [3, 6], "rightproblems": [], "wrongproblems": [], "twinproblems": [34, 92, 93]}}, {"model": "mathematics.question", "pk": 91, "fields": {"code": "3.3.2", "category": 2, "problem": "Consider two urns A and B containing a total of $N$ balls. An experiment is\r\nperformed in which a ball is selected at random (all selections equally likely) at\r\ntime $t(t=1,2,...)$ from among the totality of $N$ balls. Then, an urn is selected\r\nat random (A is chosen with probability $p$ and B is chosen with probability $q$)\r\nand the ball previously drawn is placed in this urn.The state of the system at\r\neach trial is represented by the number of balls in A. Determine the transition\r\nmatrix for this Markov chain.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$P_{i,i}=(\\frac{i}{N})p+(\\frac{N-i}{N})q$\r\n<br>\r\n$P_{i,i+1}=(\\frac{i}{N})q$\r\n<br>\r\n$P_{i,i-1}=(\\frac{N-i}{N})p$", "choicesb": "$P_{i,i}=(\\frac{i}{N})p+(\\frac{N-i}{N})q$\r\n<br>\r\n$P_{i,i+1}=(\\frac{N-i}{N})p$\r\n<br>\r\n$P_{i,i-1}=(\\frac{i}{N})q$", "choicesc": "$P_{i,i}=(\\frac{N-i}{N})p+(\\frac{i}{N})q$\r\n<br>\r\n$P_{i,i+1}=(\\frac{N-i}{N})p$\r\n<br>\r\n$P_{i,i-1}=(\\frac{i}{N})q$", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Given the", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Oh, you'd better think this problem again.", "messagesuccess": "Congratulations, you get the point of transition matrix.", "sensitivity": null, "gussingparameter": 0.33, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [6], "rightproblems": [], "wrongproblems": [], "twinproblems": [94]}}, {"model": "mathematics.question", "pk": 92, "fields": {"code": "3.3.3", "category": 2, "problem": "Consider the inventory model of Section 3.3.1. Suppose that $S=3$. Set up\r\nthe corresponding transition probability matrix for the end-of-period inventory\r\nlevel $X_n$. (Similar to Exer3.1.1, given $Pr(n=0)=0.5; Pr(n=1)=0.4; Pr(n=2)=0.1;$)", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "\\begin{eqnarray*}\r\n&&  ~~ \\,\\, -1 \\quad \\, 0 \\quad \\, 1\\quad\\,\r\n2\\quad \\, 3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{-1}  \\cr \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n0 & 0  & .1 & .4 & .5    \\cr\r\n0 & 0  & .1 & .4 & .5    \\cr\r\n0 & .1  & .4 & .5& 0   \\cr\r\n0 & .1  & .4 & .5 & 0    \\cr\r\n0 & 0  & .1 & .4 & .5\r\n}\r\n\\end{eqnarray*}", "choicesb": "\\begin{eqnarray*}\r\n&&  ~~ \\,\\, -1 \\quad \\, 0 \\quad \\, 1\\quad\\,\r\n2\\quad \\, 3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{-1}  \\cr \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n0 & 0  & .1 & .4 & .5    \\cr\r\n0 & 0  & .1 & .4 & .5    \\cr\r\n.1 & .4  & .5 & 0& 0   \\cr\r\n0 & 0  & .4 & .5 & .1    \\cr\r\n0 & 0  & .1 & .4 & .5\r\n}\r\n\\end{eqnarray*}", "choicesc": "\\begin{eqnarray*}\r\n&&  ~~ \\,\\, -1 \\quad \\, 0 \\quad \\, 1\\quad\\,\r\n2\\quad \\, 3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{-1}  \\cr \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n0 & 0  & .1 & .4 & .5    \\cr\r\n0 & 0  & .1 & .4 & .5    \\cr\r\n.1 & .4  & .5 & 0& 0   \\cr\r\n0 & .1  & .4 & .5 & 0    \\cr\r\n0 & 0  & .1 & .4 & .5\r\n}\r\n\\end{eqnarray*}", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The analysis is similar to Exer3.3.1, just calculate the probability of going to every state tomorrow given today's state.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Oh, you'd better think this problem again.", "messagesuccess": "Congratulations, you get the point of transition matrix.", "sensitivity": null, "gussingparameter": 0.33, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [6], "rightproblems": [], "wrongproblems": [], "twinproblems": [90]}}, {"model": "mathematics.question", "pk": 93, "fields": {"code": "3.3.4", "category": 2, "problem": "Consider the inventory model of Section 3.3.1. Suppose that $S=3$ and that the\r\nprobability distribution for demand is $Pr(\\xi_n=0)=0.1;Pr(\\xi_n=1)=0.4;Pr(\\xi_n=2)=0.3, and Pr(\\xi_n=3)=0.2. $<br>\r\nSet up the corresponding transition probability\r\nmatrix for the end-of-period inventory level $X_n$.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "\\begin{eqnarray*}\r\n&& \\qquad  \\,-2  \\, -1 \\quad \\, 0 \\quad \\, 1\\quad\\,\r\n2\\quad \\, 3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{-2} \\cr \\hbox{-1}  \\cr \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n0 & 0  & .2 & .3 & .4 &.1  \\cr\r\n0 & 0  & .2 & .3 & .4 &.1   \\cr\r\n0 & 0  & .2 & .3 & .4 &.1   \\cr\r\n.2 & .3 & .4 &.1& 0 & 0   \\cr\r\n0 & .2 & .3 & .4 &.1& 0 \\cr\r\n0 & 0 & .2 & .3 & .4 &.1\r\n}\r\n\\end{eqnarray*}", "choicesb": "\\begin{eqnarray*}\r\n&& \\qquad  \\,\\,-2  \\, -1 \\quad \\, 0 \\quad \\, 1\\quad\\,\r\n2\\quad \\, 3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{-2} \\cr \\hbox{-1}  \\cr \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n0 & 0  & .2 & .3 & .4 &.1  \\cr\r\n0 & 0  & .2 & .3 & .4 &.1   \\cr\r\n0 & .1  & .2 & .3 & .4 & 0   \\cr\r\n.2 & .3 & .4 &.1& 0 & 0   \\cr\r\n0 & .2 & .3 & .4 &.1& 0 \\cr\r\n0 & 0 & .2 & .3 & .4 &.1\r\n}\r\n\\end{eqnarray*}", "choicesc": "\\begin{eqnarray*}\r\n&& \\qquad  \\,\\,-2  \\, -1 \\quad \\, 0 \\quad \\, 1\\quad\\,\r\n2\\quad \\, 3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{-2} \\cr \\hbox{-1}  \\cr \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n0 & 0  & .2 & .3 & .4 &.1  \\cr\r\n0 & 0  & .2 & .3 & .4 &.1   \\cr\r\n0 & 0  & .2 & .3 & .4 &.1   \\cr\r\n.2 & .3 & .4 &.1& 0 & 0   \\cr\r\n0 & .2 & .3 & .4 &.1& 0 \\cr\r\n0 & .1 & .2 & .3 & .4 &0\r\n}\r\n\\end{eqnarray*}", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The analysis is similar to Exer3.1.1. The difference is that this problem introduces one more state. But all steps are the same.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Oh, you'd better think this problem again.", "messagesuccess": "Congratulations, you get the point of transition matrix.", "sensitivity": null, "gussingparameter": 0.33, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [6], "rightproblems": [], "wrongproblems": [], "twinproblems": [90]}}, {"model": "mathematics.question", "pk": 94, "fields": {"code": "3.3.5", "category": 2, "problem": "An urn initially contains a single red ball and a single green ball.A ball is\r\ndrawn at random, removed, and replaced by a ball of the opposite color, and\r\nthis process repeats so that there are always exactly two balls in the urn. Let $X_n$\r\nbe the number of red balls in the urn after n draws, with $X_0=1$. Specify the\r\ntransition probabilities for the Markov chain $\\{X_n\\}$.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "\\begin{eqnarray*}\r\n&& \\qquad  \\,\\,  0 \\quad 1 \\quad \\,  2 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0}  \\cr\r\n\\hbox{1} \\cr  \\hbox{2}  }\r\n\\pmatrix{\r\n0 & 1 & 0    \\cr\r\n.5 & .5 & 0 \\cr\r\n0 & 0 & 1\r\n}\r\n\\end{eqnarray*}", "choicesb": "\\begin{eqnarray*}\r\n&& \\qquad  \\,\\,  0 \\quad 1 \\quad \\,  2 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0}  \\cr\r\n\\hbox{1} \\cr  \\hbox{2}  }\r\n\\pmatrix{\r\n0 & 1 & 0    \\cr\r\n0 & .5 & .5 \\cr\r\n1 & 0 & 0\r\n}\r\n\\end{eqnarray*}", "choicesc": "\\begin{eqnarray*}\r\n&& \\qquad  \\,\\,  0 \\quad 1 \\quad \\,  2 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0}  \\cr\r\n\\hbox{1} \\cr  \\hbox{2}  }\r\n\\pmatrix{\r\n0 & 1 & 0    \\cr\r\n.5 & 0 & .5 \\cr\r\n0 & 1 & 0\r\n}\r\n\\end{eqnarray*}", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "If the $X_n=2$, which means the number of red balls in the urn is 2. So next time one of the two red balls will be replaced by one green ball. Then the number of red balls next time must be 1.<br>\r\nIf the $X_n=1$, which means the number of red balls in the urn is 1. So next time the red one has 50% chance to be replaced, then $P(X_{n+1}=0)=0.5$. There is the other 50% chance that the green one will be replaced. So $P(X_{n+1}=2)=0.5$.<br>\r\nIf the $X_n=2$, which means the number of red balls in the urn is 0. So next time one of the two green balls will be replaced by one red ball. Then the number of red balls next time must be 1.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.33, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [6], "rightproblems": [], "wrongproblems": [], "twinproblems": [91]}}, {"model": "mathematics.question", "pk": 95, "fields": {"code": "3.4.1", "category": 2, "problem": "Find the mean time to reach state 3 starting from state 0 for the Markov chain\r\nwhose transition probability matrix is<br>\r\n\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, 0 \\quad  \\quad1\\quad \\quad\r\n2\\quad \\quad3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n0.4 & 0.3  &0.2 & 0.1  \\cr\r\n0 & 0.7  &0.2 & 0.1 \\cr\r\n0 & 0  & 0.9 & 0.1 \\cr\r\n0 & 0  & 0 & 1 \r\n}\r\n\\end{eqnarray*}", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "7", "choicesb": "8", "choicesc": "9", "choicesd": "10", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "We denote $W_0,W_1,W_2,W_3$ as the expected steps of going to $state 3$ from $state0,1,2,3$. Obviously,$W_3=0$(means you are already at $state 3$) <br>\r\nFrom $state0$ to $state3$, we have to go forward at least one step. If you are lucky enough, you can reach $state 3$ directly after the first step. This probability is $0.1$ from the transition matrix. Otherwise, you must reach one of the other three states.<br>\r\nIf you go to $state2$ for example, since you haven't arrived at $state3$, you have to go further. Now the problem becomes: starting from $state2$, going to $state3$. The remaining expected steps are just $W_2$. The past first step has nothing to with the following. The probability of this scenario is 0.2<br>\r\nFrom the analysis above, we can derive the formula of $W_0$. It can be separated to 4 parts(corresponding to 4 scenarios)<br>\r\n$W_0=1+0.4*W_0+0.3*W_1+0.2*W_2+0.1*W_3$<br>\r\nThis is the equation in terms of starting from $state0$. Similarly we have other 3 equations according to the First Step Analysis.<br>\r\n$W_1=1+0*W_0+0.7*W_1+0.2W_2+0.1W_3$<br>\r\n$W_2=1+0*W_0+0*W_1+0.9W_2+0.1W_3$<br>\r\n$W_3=0$<br>\r\nBy solving these equations, we can get $W_0=10$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Oh, you'd better think this problem again.", "messagesuccess": "Congratulations, you get the point of First Step Analysis.", "sensitivity": null, "gussingparameter": 0.25, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [], "wrongproblems": [96, 128, 129], "twinproblems": [128, 129]}}, {"model": "mathematics.question", "pk": 96, "fields": {"code": "3.4.2", "category": 2, "problem": "Consider the Markov chain whose transition probablity matrix is given by\r\n<br>\r\n\\begin{eqnarray*}\r\n&& \\quad ~~~0 \\quad ~~~1\\quad ~~~\r\n2 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} }\r\n\\pmatrix{\r\n1 & 0  &0  \\cr\r\n0.1& 0.6 &0.3  \\cr\r\n0 & 0  & 1\r\n}\r\n\\end{eqnarray*}\r\n<br>\r\n(a) Starting in state 1, determine the probability that the Markov chain ends in\r\nstate 0.<br>\r\n(b) Determine the mean time to absorption.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.25;2.5", "choicesb": "0.6;2.5", "choicesc": "0.25;3", "choicesd": "0.6;3", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "(a).<br>\r\nWe use $P_0,P_1,P_2$ to denote the probabilities that the Markov chain starts from $state0,1,2$ and ends at $state 0$.<br>\r\nStarting from $state0$, it has 1 chance to ends at $state0$.<br>\r\n$P_0=1$<br>\r\nStarting from $state2$, it has 0 chance to ends at $state0$.<br>\r\n$P_2=0$<br>\r\nStarting from $state1$, it can goes to any of the three states after the first step. Each state has its probability of going to $state0$, so:<br>\r\n$P_1=0.1*P_0+0.6*P_1+0.3*P_2$ <br>\r\nBy solving these 3 equations, we get $P_1=0.25$\r\n<br>\r\n(b).<br>\r\nSimilar analysis.<br>\r\nAbsorption means stopping at one fixed state forever. Here we can easily find that if we reach $state0$ or $state2$, we can get out of that state any more.So the problem is the expected steps we need to reach $state0$ or $state2$. Suppose  $W_0,W_1,W_2$  represent the expectation steps of reaching $state0$ or $state2$ starting from $state0,1,2$ respectively. <br>\r\n$W_0=W_2=1$, because they have already at one of the two states.<br>\r\n$W_1=1+0.1*W_0+0.6*W_1+0.3*W_2$, because  from $state1$, you have to at least go one step further. Then you may reach any of the three states. You'll have new starting point. The remaining expected steps are associated with your starting point. Fortunately, they are just $W_0,W_1,W_2$.\r\nBy solving the 3 equations related to $W_0,W_1,W_2$, we can get $W_1=2.5$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Oh, you'd better think this problem again.", "messagesuccess": "Congratulations, you get the point of First Step Analysis.", "sensitivity": null, "gussingparameter": 0.25, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [], "wrongproblems": [95], "twinproblems": [97, 98]}}, {"model": "mathematics.question", "pk": 97, "fields": {"code": "3.4.3", "category": 2, "problem": "Consider the Markov chain whose transition probability matrix is given by<br>\r\n\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, 0 \\quad  \\quad1\\quad \\quad\r\n2\\quad \\quad3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n1 & 0  &0 & 0  \\cr\r\n0.1 & 0.6  &0.1 & 0.2 \\cr\r\n0.2 & 0.3  & 0.4 & 0.1 \\cr\r\n0 & 0  & 0 & 1 \r\n}\r\n\\end{eqnarray*}<br>\r\n(a) Starting in state 1, determine the probability that the Markov chain ends in\r\nstate 0.<br>\r\n(b) Determine the mean time to absorption.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\frac{1}{3};\\frac{7}{3}$", "choicesb": "$\\frac{1}{3};\\frac{10}{3}$", "choicesc": "$\\frac{8}{21};\\frac{10}{3}$", "choicesd": "$\\frac{8}{21};\\frac{7}{3}$", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The method is similar to Exer3.4.2 .<br>\r\n(a).<br>\r\nUse $P_0,P_1,P_2,P_3$ as the probabilities of ending at $state0$, starting from $state0,1,2,3$ .<bt>\r\nBy applying the First Step Analysis, we get 4 equations: <br>\r\n$P_0=1$<br>\r\n$P_1=0.1*P_0+0.6*P_1+0.1*P_2+0.2*P_3$<br>\r\n$P_2=0.2*P_0+0.3*P_1+0.4*P_2+0.1*P_3$<br>\r\n$P_3=0$<br>\r\nBy solving these 4 equations, we get $P_1=\\frac{8}{21}$<br>\r\n(b).<br>\r\nUse $W_0,W_1,W_2,W_3$ as the expected steps of going to absorption, starting from $state0,1,2,3$ .<br>\r\n$W_0=0$<br>\r\n$W_1=1+0.1*W_0+0.6*W_1+0.1*W_2+0.2*W_3$<br>\r\n$W_2=1+0.2*W_0+0.3*W_1+0.4*W_2+0.1*W_3$<br>\r\n$W_3=0$<br>\r\nBy solving these 4 equations, we get $W_1=\\frac{10}{3}$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Oh, you'd better think this problem again.", "messagesuccess": "Congratulations, you get the point of First Step Analysis.", "sensitivity": null, "gussingparameter": 0.25, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [], "wrongproblems": [], "twinproblems": [96, 100]}}, {"model": "mathematics.question", "pk": 98, "fields": {"code": "3.4.4", "category": 2, "problem": "A coin is tossed repeatedly until two successive heads appear. Find the mean\r\nnumber of tosses required.<br>\r\nHint: Let Xn be the cumulative number of successive heads. The state space is\r\n0;1;2, and the transition probability matrix is<br>\r\n\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, 0 \\quad 1\\quad \r\n2\\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} }\r\n\\pmatrix{\r\n\\frac{1}{2} & \\frac{1}{2}  &0  \\cr\r\n\\frac{1}{2}& 0 &\\frac{1}{2}  \\cr\r\n0 & 0  & 1\r\n}\r\n\\end{eqnarray*}<br>\r\nDetermine the mean time to reach state 2 starting from state 0 by invoking a first\r\nstep analysis", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "3", "choicesb": "4", "choicesc": "5", "choicesd": "6", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $Y_n$ be $1$ $(0)$ if the $n$-th toss is a head (tail).<br>\r\nSet $Y_0=Y_{-1}=0$ for notational convenience.<br>\r\nDefine, for $n \\geq 0$,\r\n$$ X_n= \\cases{ 0  & if $Y_n=0$; \\cr\r\n1 & if $Y_n=1, Y_{n-1}=0$; \\cr\r\n2 & if $Y_n=1, Y_{n-1}=1$.\r\n}$$\r\nAssume when HH occurs $(X_n=2)$, the coin toss stops and we\r\nlet $X_k=2$ for all $k \\geq n$ (see the following Remark for the irrelevancy\r\nof this assumption).\r\nThen, $\\{X_n: n \\geq 0\\}$ is a {\\it MC} with transition probability matrix\r\n\\begin{eqnarray*}\r\n&& \\qquad  \\,\\,  0 \\quad \\, 1 \\quad \\,  2 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0}  \\cr\r\n\\hbox{1} \\cr  \\hbox{2}  }\r\n\\pmatrix{\r\n.5 & .5 & 0    \\cr\r\n.5 & 0 & .5 \\cr\r\n0 & 0 & 1\r\n}\r\n\\end{eqnarray*}<br>\r\nLet $w_i$ be the mean time to reach state 2, beginning from state $i$.<br>\r\nThen,\r\n$$ w_2=0, \\quad w_0 = 1+ .5 w_0 +.5 w_1 \\quad w_1= 1 + .5w_0.$$\r\nSolving the equations gives $w_0=6$ and $w_1=4$.\r\nThe mean number of tosses till the first two consecutive heads occur is\r\n$w_0={6}$.\r\n\r\n<br>\r\nRemark. In fact, you might find that,  $w_0$ and $w_1$ are irrelevant\r\nwith the last row of ${\\bf P}$. In other words, transition probabilities\r\nfrom 2 to 0, 1 or 2 does not matter for computing $w_0$ and $ w_1$.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Oh, you'd better think this problem again.", "messagesuccess": "Congratulations, you get the point of First Step Analysis.", "sensitivity": null, "gussingparameter": 0.25, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [99], "wrongproblems": [], "twinproblems": [96, 99]}}, {"model": "mathematics.question", "pk": 99, "fields": {"code": "3.4.5", "category": 2, "problem": "A coin is tossed repeatedly until either two successive heads appear or two successive tails appear. Suppose the first coin toss results in a head. Find the probability that the game ends with two successive tails.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\frac{1}{2}$", "choicesb": "$\\frac{1}{3}$", "choicesc": "$\\frac{1}{4}$", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Analysis is the very similar to Exer3.4.4 .<br>\r\nLet $X_n$ be a random variable representing the  four states:$TT,HT,TH,HH$ where $X_n=0,1,2,3$ respectively. For example, $X_1=1$ means the first 2 tosses is:$HT$. We still use the transition matrix:<br>\r\n\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, 0 \\quad 1\\quad \r\n2\\quad 3\\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr  \\hbox{3} }\r\n\\pmatrix{\r\n1 & 0  &0 &0 \\cr\r\n0.5& 0 &0.5&0 \\cr\r\n0& 0.5 &0&0.5 \\cr\r\n0& 0 &0&1\r\n}\r\n\\end{eqnarray*}<br>\r\nNow the problem is the probability of going to $state0$ from $state2or3$. Since $state3$ is an absorption state, so we only need to consider the probability of reaching $state0$ from $state2$.<br>\r\nFrom the First Step Analysis, we can derive 4 equations.\r\n$P_0=1$<br>\r\n$P_1=0.5*P_0+0.5*P_2$ <br>\r\n$P_2=0.5*P_1+0.5*P_3$<br>\r\n$P_3=0$<br>\r\nHere $P_i$ means the probability that the Markov chain is started at $state/quad i$and absorbed at $state0$ . By solving these 4 equations, we get $P_2=\\frac{1}{3}$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Oh, you'd better think this problem again.", "messagesuccess": "Congratulations, you get the point of First Step Analysis.", "sensitivity": null, "gussingparameter": 0.33, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [98], "wrongproblems": [], "twinproblems": [98]}}, {"model": "mathematics.question", "pk": 100, "fields": {"code": "3.4.6", "category": 2, "problem": "Consider the Markov chain whose transition probability matrix is given by<br>\r\n\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, 0 \\quad  \\quad1\\quad \\quad\r\n2\\quad \\quad3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n1 & 0  &0 & 0  \\cr\r\n0.1 & 0.4  &0.1 & 0.4 \\cr\r\n0.2 & 0.1  & 0.6 & 0.1 \\cr\r\n0 & 0  & 0 & 1 \r\n}\r\n\\end{eqnarray*}<br>\r\n(a) Starting in state 1, determine the probability that the Markov chain ends in\r\nstate 0.<br>\r\n(b) Determine the mean time to absorption.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\frac{6}{23};\\frac{50}{23}$", "choicesb": "$\\frac{9}{23};\\frac{50}{23}$", "choicesc": "$\\frac{9}{23};\\frac{35}{23}$", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "This problem is very close to Exer3.4.3<br>\r\n(a). Let $u_i$ be the probability of ending at state 0, beginning from state i.\r\nThen,<br>\r\n\\begin{eqnarray*}\r\nu_1 &=& P_{10}+ P_{11}u_1 + P_{12}u_2 +0 = .1 + .4 u_1 + .1 u_2 \\\\\r\nu_2 &=& P_{20} + P_{21}u_1 + P_{22}u_2 + 0 = .2 + .1 u_1 + .6u_2.\r\n\\end{eqnarray*}<br>\r\nThe solutions of the equations  are $u_1=6/23$ and $u_2=13/23$.\r\nThe probability to compute is $u_1={6/23}$.\r\n<br>\r\n(b). Let $w_i$ be the mean to absorption starting from state $i$.\r\nThen,<br>\r\n$$w_1= 1+.4 w_1+ .1 w_2 \\qquad w_2= 1 + .1 w_1 + .6 w_2,$$\r\nand the solutions are $w_1=50/23$ and $w_2=70/23$.<br>\r\nThe mean time to absorption starting from state 1 is ${50/23}$.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Oh, you'd better think this problem again.", "messagesuccess": "Congratulations, you get the point o", "sensitivity": null, "gussingparameter": 0.33, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [], "wrongproblems": [], "twinproblems": [97]}}, {"model": "mathematics.question", "pk": 101, "fields": {"code": "4.1.1", "category": 3, "problem": "Five balls are distributed between two urns, labeled A and B. Each period, an urn is selected at random, and if it is not empty, a ball from that urn is removed and placed into the other run. In the long run, what fraction of time is urn A is empty?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1/10", "choicesb": "1/5", "choicesc": "1/14", "choicesd": "Cannot be determined", "choicese": "None of above", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "First we define the random variable $X$ be the number of ball in urn A. It takes value from 0 to 5.<br>\r\nTo consider the transitional matrix, we know $P_{01}=P_{54}$ must be 1 as one urn is empty. For other possible transition, we know $P_{i(i+1)}=P_{i(i-1)}=1/2$. Therefore, \r\n\\[P=\r\n\\begin{array}{c|cccccc|}\r\n&0&1&2&3&4&5\\\\\r\n0&0&1&0&0&0&0\\\\\r\n1&1/2&0&1/2&0&0&0\\\\\r\n2&0&1/2&0&1/2&0&0\\\\\r\n3&0&0&1/2&0&1/2&0\\\\\r\n4&0&0&0&1/2&0&1/2\\\\\r\n5&0&0&0&0&1&0\r\n\\end{array}\\]\r\nThen we consider $\\pi_j=\\sum_kP_{kj}\\pi_k$\r\n\\begin{align*}\r\n\\pi_0&=\\pi_1/2\\\\\r\n\\pi_1&=\\pi_0+\\pi_2/2\\\\\r\n\\pi_2&=\\pi_1/2+\\pi_3/2\\\\\r\n\\pi_3&=\\pi_2/2+\\pi_4/2\\\\\r\n\\pi_4&=\\pi_3/2+\\pi_5\\\\\r\n\\pi_5&=\\pi_4/2\r\n\\end{align*}\r\nand\r\n\\begin{equation*}\r\n\\pi_1+\\pi_2+\\pi_3+\\pi_4+\\pi_5=1\r\n\\end{equation*}\r\nSolving the equations we have $\\pi_0=0.1$\r\nTherefore, 1/10 of the time urn A is empty.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "1. A clearer way to understand the relation $\\pi_j=\\sum_kP_{kj}\\pi_k$ is the new state $\\pi_j$ is equal to all possible transition from old states $\\pi_k$. <br>\r\n2. A quicker way to solve the system is by symmetry. We know $\\pi_0=\\pi_5$, $\\pi_1=\\pi_4$, $\\pi_2=\\pi_3$. Then by the second equation we have $\\pi_1=\\pi_2$. As $\\pi_0+\\pi_1+\\pi_2=0.5$, it is then obvious that $\\pi_0=0.1$, $\\pi_1=\\pi_2=0.2$", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [3, 37, 39], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 102, "fields": {"code": "4.1.4", "category": 3, "problem": "A finite state regular Markov chain has transition probability matrix $\\mathbf{P}=|P_{ij}|$ and limiting distribution $\\pi=|\\pi_i|$. In the long run, what fraction of the transitions are from the prescribed state $k$ to a prescibed state $m$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\pi_k$", "choicesb": "$\\frac{\\pi_mP_{mk}}{\\pi_k}$", "choicesc": "$\\frac{\\pi_kP_{km}}{\\pi_m}$", "choicesd": "$\\pi_m$", "choicese": "$\\frac{P_{km}}{\\sum_iP_{im}}$", "choicesf": "None of the above", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Mathematically, we are considering\r\n\\begin{equation*}\r\nP(X_{n+1}=m\\cap X_n=k|X_{n+1}=m)\r\n\\end{equation*}\r\nas\r\n\\begin{equation*}\r\nP(X_{n+1}=m\\cap X_n=k)=P(X_{n+1}=m|X_n=k)P(X_n=k)=P_{km}\\pi_k\r\n\\end{equation*}\r\nand\r\n\\begin{equation*}\r\nP(X_{n+1}=m)=\\pi_m\r\n\\end{equation*}\r\nso\r\n\\begin{equation*}\r\nP(X_{n+1}=m\\cap X_n=k|X_{n+1}=m)=\\frac{\\pi_kP_{km}}{\\pi_m}\r\n\\end{equation*}", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "You can prove your answer with another method, which is related to Prob4.1.13.\r\nConsider\r\n\\begin{align*}\r\nP(X_{n+1}=m\\cap X_n=k|X_{n+1}=m)&=P(X_{n+1}=m|X_n=k\\cap X_{n+1}=m)P(X_n=k|X_{n+1}=m)\\\\\r\n&=1\\times P(X_n=k|X_{n+1}=m)\r\n\\end{align*}\r\nNow you can match the answer with Prob 4.1.13.", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [3, 27, 37], "rightproblems": [], "wrongproblems": [134, 135, 136], "twinproblems": [105, 134, 135]}}, {"model": "mathematics.question", "pk": 103, "fields": {"code": "4.1.7", "category": 3, "problem": "Determine the limiting distribution for the Markov chain whose transition probability matrix is\r\n\\[P=\r\n\\begin{array}{c|cccc|}\r\n&0&1&2&3\\\\\r\n0&\\frac{1}{2}&0&0&\\frac{1}{2}\\\\\r\n1&1&0&0&0\\\\\r\n2&0&\\frac{1}{2}&\\frac{1}{3}&\\frac{1}{6}\\\\\r\n3&0&0&1&0\r\n\\end{array}\r\n\\]", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "\\begin{align*}\r\n\\pi_0=1/11\\\\\r\n\\pi_1=2/11\\\\\r\n\\pi_2=4/11\\\\\r\n\\pi_3=4/11\r\n\\end{align*}", "choicesb": "\\begin{align*}\r\n\\pi_0=2/11\\\\\r\n\\pi_1=1/11\\\\\r\n\\pi_2=4/11\\\\\r\n\\pi_3=4/11\r\n\\end{align*}", "choicesc": "The Markov chain is not regular.", "choicesd": "\\begin{align*}\r\n\\pi_0=2/11\\\\\r\n\\pi_1=2/11\\\\\r\n\\pi_2=4/11\\\\\r\n\\pi_3=3/11\r\n\\end{align*}", "choicese": "None of the above", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Consider the relation $\\pi_j=\\sum_kP_{kj}\\pi_k$, one can read off the system of equations(skim through the matrix column by column ):\r\n\\begin{align*}\r\n\\pi_0&=\\pi_0/2+\\pi_1\\\\\r\n\\pi_1&=\\pi_2/2\\\\\r\n\\pi_2&=\\pi_2/3+\\pi_3/6\\\\\r\n\\pi_3&=\\pi_2\r\n\\end{align*}\r\nSolving the equations we have\r\n\\begin{equation*}\r\n\\pi_0:\\pi_1:\\pi_2:\\pi_3=1:2:4:4\r\n\\end{equation*}\r\nRemember the conservation of probability\r\n\\begin{equation*}\r\n\\pi_0+\\pi_1+\\pi_2+\\pi_3=1\r\n\\end{equation*}\r\nTherefore, $\\pi_0=1/11$, $\\pi_1=2/11$, $\\pi_2=\\pi_3=4/11$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [3, 37, 39], "rightproblems": [134], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 104, "fields": {"code": "4.1.10", "category": 3, "problem": "Consider a Markov chain with transition probability matrix\r\n\\[P=\r\n\\begin{array}{|ccccc|}\r\np_0&p_1&p_2&\\dots&p_N\\\\\r\np_N&p_0&p_1&\\dots&p_{N-1}\\\\\r\np_{N-1}&p_N&p_0&\\dots&p_{N-2}\\\\\r\n\\vdots&\\vdots&\\vdots&&\\vdots\\\\\r\np_1&p_2&p_3&\\dots&p_0\r\n\\end{array}\r\n\\]\r\nwhere $0<p_0<1$ and $p_0+p_1+\\dots+p_n=1$. Determine the limiting distribution.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\pi_j=p_j$", "choicesb": "$\\pi_j=p_{N+1-j}$", "choicesc": "$\\pi_j=1/(N+1)$", "choicesd": "None of the above", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Since the transition is so symmetric, we can guess the answer is 1/(N+1).\r\nWe shall prove this by verifying the condition $\\pi_j=\\sum_{k=0}^NP_{jk}\\pi_k$ where \r\n$P_{jk}=\\begin{cases}\r\np_{j-i}& \\text{if } j-i\\geq0\\\\\r\np_{j-i+N+1}&\\text{if } j-i<0\r\n\\end{cases}$\r\nSo \r\n\\begin{equation*}\r\n\\sum_{j=0}^NP_{jk}\\pi_k=\\frac{1}{N+1}\\sum_{k=0}^Np_k=1/(N+1)=\\pi_j\r\n\\end{equation*}", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [3, 37, 39], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 105, "fields": {"code": "4.1.13", "category": 3, "problem": "A Markov chain has the transition probability matrix\r\n\\[P=\r\n\\begin{array}{c|ccc|}\r\n&0&1&2\\\\\r\n0&0.4&0.4&0.2\\\\\r\n1&0.6&0.2&0.2\\\\\r\n2&0.4&0.2&0.4\r\n\\end{array}\r\n\\]\r\nAfter a long period of time, you observe the chain and see that it is in state 1. What is the conditional probability that the previous state was 2? That is, find\r\n\\begin{equation*}\r\n\\lim_{n\\to\\infty}\\text{Pr}\\{X_{n-1}=2|X_n=1\\}\r\n\\end{equation*}", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.2", "choicesb": "12/70", "choicesc": "6/7", "choicesd": "1/4", "choicese": "None of the above", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The transition matrix is about $P(X_{n}|X_{n-1})$, so the obvious choice is to apply Bayes' theorem.\r\n\\begin{align*}\r\n\\lim_{n\\to\\infty}\\text{Pr}\\{X_{n-1}=2|X_n=1\\}&=\\lim_{n\\to\\infty}\\text{Pr}\\{X_n=1|X_{n-1}=2\\}\\frac{\\text{Pr}\\{X_{n-1}=2\\}}{\\text{Pr}\\{X_n=1\\}}\\\\\r\n&=P_{21}\\frac{\\pi_2}{\\pi_1}\r\n\\end{align*}\r\nSolving the long time behaviour, we have $\\pi_0=11/24$, $\\pi_1=7/24$, $\\pi_2=1/4$. Therefore, \r\n\\begin{equation*}\r\n\\lim_{n\\to\\infty}\\text{Pr}\\{X_{n-1}=2|X_n=1\\}=12/70\r\n\\end{equation*}", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [3, 25, 27, 37, 39], "rightproblems": [], "wrongproblems": [], "twinproblems": [102]}}, {"model": "mathematics.question", "pk": 106, "fields": {"code": "4.2.3", "category": 3, "problem": "Suppose that a production process changes state according to a Markov process whose transition probability matrix is given by \r\n\\[P=\r\n\\begin{array}{c|cccc|}\r\n&0&1&2&3\\\\\r\n0&0.2&0.2&0.4&0.2\\\\\r\n1&0.5&0.2&0.2&0.1\\\\\r\n2&0.2&0.3&0.4&0.1\\\\\r\n3&0.1&0.2&0.4&0.3\r\n\\end{array}\r\n\\]\r\nSuppose that states 0 and 1 are \"in-control\", while states 2 and 3 are deemed \"out-of-control\", In the long run, what fraction of time is the process out-of-control?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "11/20", "choicesb": "26/51", "choicesc": "14/51", "choicesd": "The  MC is irregular", "choicese": "None of the above", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "So we want to calculate $\\pi_2+\\pi_3$\r\nThe system of equation can be written as \r\n\\begin{align*}\r\n\\pi_0&=\\pi_0/5+\\pi_1/2+\\pi_2/5+\\pi_3/10\\\\\r\n\\pi_1&=\\pi_0/5+\\pi_1/5+3\\pi_2/10+\\pi_3/5\\\\\r\n\\pi_2&=2\\pi_0/5+\\pi_1/5+2\\pi_2/5+2\\pi_3/5\\\\\r\n\\pi_3&=\\pi_0/5+\\pi_1/10+\\pi_2/10+3\\pi_3/10\r\n\\end{align*}\r\nwith \r\n\\begin{equation*}\r\n\\pi_0+\\pi_1+\\pi_2+\\pi_3=1\r\n\\end{equation*}\r\n$\\pi_0=13/51$? $\\pi_1=4/17$, $\\pi_2=6/17$, $\\pi_3=8/51$ and $\\pi_2+\\pi_3=26/51$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [3, 37, 39, 40], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 107, "fields": {"code": "4.2.6", "category": 3, "problem": "Consider a computer system that fails on a given day with probability $p$ and remains \"up\" with probability $1-p$. Suppose the repair time is a random variable $N$ having the probability mass function $p(k)=\\beta(1-\\beta)^{k-1}$ for $k=1,2,...,$ where $0<\\beta<1$. Let $X_n=1$ if the computer is operating on day $n$ and $X_n=0$ if not. Determine the long run probability that the computer is operating in terms of $\\beta,p$.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\pi_0=\\pi_1=1/2$", "choicesb": "$\\pi_0=p(1-\\beta) \\qquad \\pi_1=p\\beta$", "choicesc": "$\\pi_0=p \\qquad \\pi_1=q$", "choicesd": "$\\pi_0=\\frac{p}{\\beta+p} \\qquad \\pi_1=\\frac{\\beta}{\\beta+p}$", "choicese": "None of the above", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Observe that the repair day follow a geometric distribution, it is clearly memoryless. We can conclude that when the computer is down, it has $\\beta$ chance getting repaired and $\\alpha=1-\\beta$ chance not getting repaired. When the computer is up, it has $q$ chance remain up, and $p$ chance being down. The transition matrix is then\r\n\\[\r\n\\begin{array}{c|cc|}\r\n&0&1\\\\\r\n0&1-\\beta&\\beta\\\\\r\n1&p&1-p\r\n\\end{array}\r\n\\]\r\nThe long run probability then have the relation $\\beta\\pi_0=p\\pi_1$ and $\\pi_0+\\pi_1=1$\r\n\\begin{equation*}\r\n\\pi_0=\\frac{p}{\\beta+p} \\qquad \\pi_1=\\frac{\\beta}{\\beta+p}\r\n\\end{equation*}", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [3, 34, 37, 39], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 108, "fields": {"code": "4.1.5", "category": 1, "problem": "English premiere league football team Chelsea  recently fired the coach\r\n  Scolari  and hired Mr. N. (not Hiddink), who, known for his firmness with rules,\r\n  declares to strictly employ a naive\r\n  tactic on using for center forward\r\n  one of the two equally good strikers: Didier Drogba or Nicholas Anelka.\r\n  On every match, the player (D or A) has chance $1-p$ to  be performing (up to\r\n  Mr. N.'s standard), in which case\r\n  he continues to play for the next match, and chance $p$ to be non-performing, in\r\nwhich case, he is suspended   for the next two matches (as a kind of penalty or distrust)\r\n  and afterwards waits for his turn to play when the other  is suspended.\r\n  Mr. N. lets Drogba play the first match.\r\n  Let $X_n$ be the number of ``ready\" players at match $n$, playing\r\n  or in bench\r\n  $(X_1=2)$.\r\n  Let $Y_n$ be the number of players that is suspended in\r\n   match $n$\r\n  but would be ``ready\" for the $n+1$-th match $(Y_1=0)$.\r\n  To illustrate the situation, consider the following\r\n  possible path:\r\n\r\nRight now Mr. N. worries about the chance of\r\nhis nightmare match: both Drogba and Anelka are suspended.", "problempicture1": "theall/image/example4-1-5.PNG", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "no", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Note first that $\\{X_n\\}$ is NOT a {MC}, since, for example,\r\n$$P(X_{n+1}=1|X_n=1, X_{n-1}=2) = 1-p \\quad \\hbox{but}\r\n\\quad P(X_{n+1}=1|X_n=1, X_{n-1}=1, X_{n-2}=2)=p.$$\r\n  But   $\\{(X_n, Y_n): n \\geq 0\\}$ is a {\\it MC} with\r\n  state space of $(X_n, Y_n)$\r\n  is $\\{(2, 0), (1,1), (1,0), (0, 1)\\}$ and\r\n  transition probability matrix:\r\n  \\begin{eqnarray*}\r\n && \\qquad \\qquad \\,\\, ({\\rm 2, 0})\\,\\,\\,\\,\\,\\, \\,\\,({\\rm 1, 0})\r\n \\,\\,\\,\\,\\,\\,\\,\r\n({\\rm 1, 1})\\,\\,\\,\\,\\,\\, ({\\rm 0, 1}) \\\\\r\n  P &=&  \\matrix{\r\n  ({\\rm 2, 0}) \\cr ({\\rm 1, 0}) \\cr\r\n({\\rm 1, 1}) \\cr ({\\rm 0, 1}) }\r\n\\pmatrix{\r\n\\,\\,\\,\\, 1-p \\,\\,\\, & \\,\\,\\,\\, p \\,\\,\\,\\,\\,\r\n& \\,\\,\\,\\,\\,\\,\\,     &\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,  \\cr\r\n & & 1-p & \\,\\,\\, p \\,\\,\\cr\r\n1-p & p & & \\cr\r\n&  &   1 & \\,\\,\\, \\,\\, }\r\n\\end{eqnarray*}\r\nLet 0, 1, 2 and 3 denote ({2, 0}), ({1, 0}),\r\n({1, 1}) and ({0, 1}), respectively.\r\nEquations (4.2)-(4.3) becomes\r\n$$\\cases{(\\pi_0 \\,\\, \\pi_1 \\,\\, \\pi_2 \\,\\, \\pi_3 )\r\n= (\\pi_0 \\,\\, \\pi_1 \\,\\, \\pi_2 \\,\\, \\pi_3 )P & \\cr\r\n\\pi_0 + \\pi_1 +\\pi_2 + \\pi_3 = 1 }\r\n$$\r\nSolving the equations gives\r\n$$\\pi_0 = {1-p  \\over 1+p+p^2}, \\quad \\pi_1=\\pi_2= {p \\over 1+p+p^2},\r\n  \\quad\\hbox{and}\\quad\r\n \\pi_3={p^2 \\over 1+p+p^2}.$$\r\nThe long run fraction of the Chelsea nightmare matches\r\nis\r\n$$ \\lim_{n \\to \\infty} P(X_n  =0 )\r\n= \\lim_{n\\to \\infty} P((X_n, Y_n) =(0, 1))\r\n =\\pi_3 = p^2/(1+p+p^2)\r\n$$\r\nIf Mr. N. has the mind of\r\ntrusting whoever employed and un-employing whoever distrusted,\r\n(Yong Ren Bu Yi, Yi Ren Bu Yong--a old Chinese saying)  he sets $p$ low.\r\nThe fraction of nightmare match is at the order\r\nof $p^2$ rather than $p$.\r\nFor example, if $p=1/10$, meaning that a player is nonperforming\r\nin one match is 1 out of 10.\r\nHaving two players, the nightmare matches would be only\r\n1 out of 111, which is relatively small.\r\nIf, instead, Mr. N.   is too harsh and demanding,\r\n  he could set $p$ as 0.4 for example.\r\nThen, his chance of nightmare match is a little over\r\n1 out of 10.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 5, "calculateddifficulty": null, "linkneuron": [37, 40], "rightproblems": [137], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 109, "fields": {"code": "4.1.6", "category": 1, "problem": "({ Stock Price Movement})<br>\r\nSuppose, on each trading day, the HSBC stock is either\r\nup (U) or down (D). And today's up or down depends on the\r\nhistorical movement only through   that of the\r\nprevious two days in the following\r\nway:(figure)<br>\r\n  What's the long run fraction of days the stock price is up?", "problempicture1": "theall/image/example4-1-6.PNG", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "At first glance, it appears that both down and up trends tend to persist, with\r\n  the downs only slightly more so. One might expect that a little over\r\n  50% of the days would be down. It turns out the down days are nearly\r\n  twice as often as up days! Surprising, isn't it?\r\n\r\n  Let $Y_n={\\rm U}$ of D if the stock price on day $n$ is\r\n  up or down, respectively. Then, $\\{Y_n\\}$ is NOT a {$MC$}, because\r\n  $$P(Y_{n+2}={U} | Y_{n+1} = {U}, Y_{n}={U}) = 0.8 \\not=\r\n  0.6= P(Y_{n+2}={U} | Y_{n+1} = {\\rm U}, Y_{n}={\\rm D}). $$\r\n  As a result, the limit theorem of regular {$MC$} does not\r\n  apply to $\\{Y_n\\}$. Looks like a dead end.\r\n\r\n% shan chong shui fu yi wu lu,\r\n% Liu An hua ming yiu yi chun. ---Lu Yiu.\r\n\r\nRecall the tricks of constructing {MC} using\r\npattern of length 2 or 3 in Examples  and exercises\r\nabout coin tossing. The idea applies here.\r\nSet\r\n$ X_n= (Y_{n-1 }, Y_{n})$. Then $\\{X_n\\}$ is a {MC}\r\nwith state space $\\{({\\rm U, U}), ({\\rm D, U}),\r\n({\\rm U, D}),   ({\\rm D, D})\\}$ and transition probability\r\nmatrix\r\n\\begin{eqnarray*}\r\n && \\qquad \\qquad ({\\rm U, U})\\,\\,\\,\\, ({\\rm D, U})\\,\\,\\,\r\n({\\rm U, D})\\,\\, ({\\rm D, D}) \\\\\r\n  P &=&  \\matrix{\r\n  ({\\rm U, U}) \\cr ({\\rm D, U}) \\cr\r\n({\\rm U, D}) \\cr ({\\rm D, D}) }\r\n\\pmatrix{\r\n\\,\\,\\,\\, 0.8 \\,\\,\\, & \\,\\,\\,\\,   \\,\\,\\,\\,\\,\r\n& \\,\\,\\,\\,0.2 \\,\\,\\,     &\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,  \\cr\r\n0.6 &  & 0.4& \\cr\r\n & 0.4&  & \\,\\,\\, 0.6 \\,\\,\\cr\r\n& 0.1 &   & \\,\\,\\,0.9\\,\\, }\r\n\\end{eqnarray*}\r\nLet 0, 1, 2 and 3 denote ({\\rm U, U}), ({\\rm D, U}),\r\n({\\rm U, D}) and ({\\rm D, D}), respectively.\r\nEquations (4.2)-(4.3) becomes\r\n$$\\cases{(\\pi_0 \\,\\, \\pi_1 \\,\\, \\pi_2 \\,\\, \\pi_3 )\r\n= (\\pi_0 \\,\\, \\pi_1 \\,\\, \\pi_2 \\,\\, \\pi_3 )P & \\cr\r\n\\pi_0 + \\pi_1 +\\pi_2 + \\pi_3 = 1 }\r\n$$\r\nSolving the equations gives\r\n$$\\pi_0 = 3/11, \\qquad \\pi_1=\\pi_2=1/11 \\qquad \\pi_3=6/11.$$\r\nThe long run fraction of   the up days\r\nis\r\n\\begin{eqnarray*}\r\n&& \\lim_{n \\to \\infty} P(Y_n={\\rm U})\r\n= \\lim_{n\\to \\infty} [P(Y_n={\\rm U}, Y_{n+1}={\\rm U}) +\r\nP(Y_n={\\rm U}, Y_{n+1}={\\rm D})]\\\\\r\n&=& \\lim_{n\\to \\infty} [P(X_{n+1}=({\\rm U, U}) )+ P(X_{n+1}=({\\rm U, D}))]\r\n= \\lim_{n\\to \\infty} [P(X_{n+1}=0) + P(X_{n+1}=2)]\\\\\r\n&=& \\pi_0 + \\pi_2\r\n= 1/11 + 3/11 = 4/11.\r\n\\end{eqnarray*}\r\nOn average and over a long time, four out of eleven trading days\r\nthe HSBC stock is up and seven out of eleven trading days it's down.\r\n $\\square$\r\n\r\n\r\nThe so-called age replacement policies in the following example\r\n are very common in industry.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 5, "calculateddifficulty": null, "linkneuron": [3, 37], "rightproblems": [], "wrongproblems": [137], "twinproblems": [137]}}, {"model": "mathematics.question", "pk": 110, "fields": {"code": "4.1.7", "category": 1, "problem": "{\\bf Example 4.7} \\ {\\textcolor[rgb]{1, .5,0} {$\\star\\star\\star\\star$}}\r\n \\ ({\\sc Age replacement policies}) \\\r\n To ensure flight safety,\r\nOasis Hong Kong Airline  has a policy to retire aging Boeing 777 with new replacement: as long as\r\nthe airplane fails a regular year-end   examination or, even\r\nif it passes the examination, it has served for $N$ years.\r\nShould there be a replacement, it always takes place in New Year's eve.\r\nSuppose the life time (till failure of examination)\r\nof a Boeing 777 is $Y$ . Let\r\n$a_j = P(Y=j), j=1,2,...$, and\r\n$$p_k=P(Y=k+1|Y>k)={a_{k+1}\\over a_{k+1}+a_{k+2} + \\cdots},\r\n\\quad k=0, 1, 2, ...$$\r\nThen, $p_k$ is the chance the airplane fails next year's test\r\nafter it has serviced $k$ years. What's the long run\r\nfraction of replacements?", "problempicture1": "theall/image/example4-1-7.PNG", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "no", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "To ensure flight safety,\r\nOasis Hong Kong Airline  has a policy to retire aging Boeing 777 with new replacement: as long as\r\nthe airplane fails a regular year-end   examination or, even\r\nif it passes the examination, it has served for $N$ years.\r\nShould there be a replacement, it always takes place in New Year's eve.\r\nSuppose the life time (till failure of examination)\r\nof a Boeing 777 is $Y$ . Let\r\n$a_j = P(Y=j), j=1,2,...$, and\r\n$$p_k=P(Y=k+1|Y>k)={a_{k+1}\\over a_{k+1}+a_{k+2} + \\cdots},\r\n\\quad k=0, 1, 2, ...$$\r\nThen, $p_k$ is the chance the airplane fails next year's test\r\nafter it has serviced $k$ years. What's the long run\r\nfraction of replacements?\r\n\r\nDefine $X_n$ as  the number of years of service of\r\n the airplane servicing on Jan 1 of year $n$.  ($X_0=0$.)\r\n $\\{X_n\\}$ is a {\\it MC} with state space $\\{0, 1, ..., N-1\\}$\r\nand transition probability matrix\r\n\\begin{eqnarray*}\r\n&&  \\qquad \\quad \\qquad 0 \\qquad \\quad 1 \\qquad \\quad 2 \\qquad \\quad \\,\\, 3\r\n\\qquad \\, \\cdots \\quad  N-1 \\\\\r\nP &=& \\matrix{0 \\cr 1 \\cr 2 \\cr 3 \\cr \\vdots \\cr N-2 \\cr N-1}\r\n\\pmatrix{ p_0 & 1-p_0 & 0 &  0 & \\cdots & 0 \\cr\r\n          p_1 & 0     &1-p_1& 0& \\cdots & 0 \\cr\r\n          p_2 & 0     &0    &1-p_2& \\cdots & 0 \\cr\r\n          p_3 & 0     &0    & 0 & \\cdots & 0 \\cr\r\n          \\vdots&\\vdots & \\vdots &\\vdots & \\vdots & \\vdots \\cr\r\n          p_{N-2} & 0& 0& 0& \\cdots & 1-p_{N-2} \\cr\r\n          1 & 0 & 0 & 0 & \\cdots & 0 }.\r\n          \\end{eqnarray*}\r\nFor illustration, consider a typical path with $N=3$ as follows.\r\n(figure)\r\nFor example, for $k\\leq  N-2$,\r\n\\begin{eqnarray*}\r\nP_{k,0}&=& P(X_{n+1}=0|X_n=k) \\\\\r\n&=&  P(\\hbox{the plane fails\r\nthe  year-end examination at year $n$} \\, \\\\\r\n&& \\quad\r\n| \\,\\, \\hbox{the plane in service on Jan 1st of year $n$ has served $k$ years}) \\\\\r\n&=& p_k, \\\\\r\nP_{k, k+1} &=& P(X_{n+1}=k+1|X_n=k) = 1-p_k;\r\n\\end{eqnarray*}\r\n and $P_{N-1, 0} = P(X_{n+1}=0|X_n=N-1)=1.$\r\nThe equation (4.2)-(4.3) can be rewritten as\r\n$$\\cases{ \\pi_0=p_0\\pi_0 +p_1 \\pi_1 + \\cdots +p_{N-2}\r\n\\pi_{N-2} +\\pi_{N-1} & \\cr\r\n\\pi_1=(1-p_0)\\pi_0 & \\cr\r\n\\pi_2=(1-p_1)\\pi_1 & \\cr\r\n\\vdots & \\cr\r\n\\pi_{N-1}=(1-p_{N-2})\\pi_{N-2} & \\cr\r\n\\pi_0 + \\cdots + \\pi_{N-1} = 1 & \\cr\r\n}\r\n$$\r\nIt follows that $\\pi_k = (1-p_{k-1})\\cdots (1-p_0)\\pi_0$\r\nfor $1 \\leq k \\leq N-1$.\r\nHence,\r\n$$\\pi_0 \\{ 1 + (1-p_0) + (1-p_0)(1-p_1) + \\cdots\r\n+ \\prod_{i=0}^{N-2} (1-p_i) \\} = 1.$$\r\nAnd, finally, we have\r\n\\begin{eqnarray*}\r\n\\pi_0 &=& { 1\\over 1 + (1-p_0) + (1-p_0)(1-p_1) + \\cdots\r\n+ \\prod_{i=0}^{N-2} (1-p_i)  } \\\\\r\n&=&{1 \\over P(Y> 0) +P(Y>1)+ \\cdots + P(Y > N-1)} \\\\\r\n&=&{1 \\over E[\\min(Y, N) ]}.  \\qquad \\hbox{(See {\\it Exercise 4.2})}\r\n\\end{eqnarray*}\r\n$\\pi_0$ is the long run fraction of\r\nreplacements. In other words, by our\r\n{\\it time average} interpretation,\r\n$\\pi_0 \\approx   R_n/n$ for a large $n$,\r\nwhere $R_n$ is number of replacements over\r\n$n$ years. As a result,\r\n$$ R_n \\times E[\\min(Y, N) ] \\approx n$$\r\nSince $E[\\min(Y, N)]$ is the mean length of\r\nservice of an airplane, the above approximation\r\nindeed makes sense.\r\n $\\square$\r\n\r\nRemark. Suppose $\\xi_i$ is the time of service of the $i$-th Boeing 777.\r\nThen $\\xi_1, \\xi_2, ...$ are iid positive r.v.s following the common distribution\r\nas that of $\\min(Y, N)$.  This is the key element in the definition\r\nof a renewal process   $R(t)$,   the number of\r\nreplacements by time $t$, to be specified  in Chapter 7.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 5, "calculateddifficulty": null, "linkneuron": [3, 37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 111, "fields": {"code": "4.1.8", "category": 1, "problem": "Consider transition probability matrix:\r\n\\begin{eqnarray*}\r\n&&\\qquad \\,\\,\\, 0 \\qquad 1 \\qquad 2 \\qquad 3 \\\\\r\nP &=& \\matrix{0 \\cr 1 \\cr 2 \\cr 3} \\pmatrix{\r\n1/2 & 1/2 & 0 & 0 \\cr\r\n1/2 & 0 & 1/2 & 0 \\cr\r\n0 & 0 & 1/4 & 3/4 \\cr\r\n0 & 0 & 1/3 & 2/3\r\n}\r\n\\end{eqnarray*}\r\n\r\nState $j$ is called {accessible} from state\r\n$i$, denoted as $i \\to j$,\r\n if $P_{ij}^{(k)} > 0$ for some $k > 0$.\r\nIn other words, from $i$ there is a positive chance\r\nthe $MC$ can reach state $j$. In the above\r\nexample, $0 \\to 1$, $0 \\to 2$, but $2 \\nrightarrow 0$.\r\n\r\nState $i$ {\\it communicates } with state $j$, denoted\r\nas $i \\leftrightarrow j $, if\r\n$i \\to j$ and $j \\to i$. In the above example,\r\n$0 \\leftrightarrow 1$, $2 \\leftrightarrow 3$, but\r\n$0 \\nleftrightarrow 2$.\r\n\r\nIt is easily seen that communicativeness has properties:\r\n(a). reflexivity: $i \\leftrightarrow i$; (b). symmetry:\r\n$i \\leftrightarrow j$ is the same as $j \\leftrightarrow i$;\r\n(c). transitivity: if $i \\leftrightarrow j$ and\r\n$j \\leftrightarrow k$, then $i \\leftrightarrow k$.\r\nA relation satisfies the above three properties\r\nusually defines classes of certain equivalency.\r\nAll states of a {\\MC} can be partitioned into\r\na number of classes such that states within a class\r\nall communicate with each other and states belong to\r\ndifferent classes do not.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "A {MC}  is called { irreducible}\r\n if all states communicate with each\r\nother. The {\\it MC} with transition probability matrix\r\nin Example 4.8 is  not irreducible. A regular {MC}\r\nis irreducible, but an irreducible {MC} is not necessarily\r\nregular.\r\n\r\nState $i$ is {\\periodic} if, for some $k>1$, $P_{ii}^{(n)}=0$\r\nfor {\\all} $n$ that are not multiples $k$ and $P_{ii}^{(n)}>0$\r\nfor {some } $n$ as multiples of $k$.\r\n  In other words,\r\n$P_{ii}^{(km+1)}=P_{ii}^{(km+2)}=\\cdots=P_{ii}^{(km+k-1)}=0$\r\nfor all $m\\geq 0$ and $P_{ii}^{(kj)}>0$ for some $j \\geq 1$.\r\n\r\nThe {period} of state $i$, denoted as $d(i)$, is\r\nthe greatest common divisor (gcd) of all $n\\geq 1$\r\nsuch that $P_{ii}^{(n)}>0$, i.e.,\r\n$$d(i) = \\hbox{gcd}\\{n \\geq 1: P_{ii}^{(n)} > 0 \\}.$$\r\nIf $d(i)\\geq 2$, we say $i$ is periodic with period $d(i)$;\r\nif $d(i)=1$, we say state $i$ is {\\it aperiodic}. We\r\nset $d(i)=0$ if $P_{ii}^{(n)}=0$ for all $n\\geq 1$.\r\nStarting from state $i$, only when the $MC$ takes\r\nsome multiples of $d(i)$ steps, would $MC$ have a\r\npositive chance to return to state $i$.\r\n\r\nConsider a $MC$ with states 0, 1, 2, 3 and\r\none step transition probabilities $P_{01}=P_{12}=P_{23}=P_{30}=1$.\r\nThen, all states are periodic with period 4.\r\nSuppose the one-step\r\ntransition probabilities are such\r\nthat $P_{01}=P_{12}=P_{23}=1$, $P_{30}=1/3$ and $P_{32}=2/3$.\r\nThen, all states are periodic with period 2.\r\nIn Example 4.8, all states are aperiodic.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [3, 40], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 112, "fields": {"code": "4.2.2", "category": 2, "problem": "In the reliability example of Textbook Section 4.2.2(see figures), what fraction of time is the repair\r\nfacility idle? When a second repair facility is added, what fraction of time is\r\neach facility idle?", "problempicture1": "theall/image/exercise4-2-2-1or4.PNG", "problempicture2": "theall/image/exercise4-2-2-2.PNG", "problempicture3": "theall/image/exercise4-2-2-3.PNG", "problempicture4": "theall/image/exercise4-2-2-4.PNG", "problempicture5": "", "problempicture6": "", "choicesa": "1;1", "choicesb": "$\\frac{q^{2}}{1+p^{2}}$;$\\frac{q+p}{1+p+p^{2}}$", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "By understanding the problem,we get that the fraction of the the repair\r\nfacility idling equals to $\\pi_{0}$=$\\frac{q^{2}}{1+p^{2}}$(see the textbook p181)<br>\r\nOn the other hand,we get when the another repair facility is added, fraction of each  the repair\r\nfacility idling is equal to  $\\pi_{0}^{,}$+0.5* $\\pi_{1}^{,}$+ 0.5*$\\pi_{2}^{,}$=$\\frac{q+p}{1+p+p^{2}}$.(distrubution computing see the textbook p181)<br>", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [37, 40], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 113, "fields": {"code": "4.2.3", "category": 2, "problem": "Determine the average fraction inspected, AFI, and the average outgoing quality,\r\nAOQ, of Section 4.2.3(attached figure) for p=0.5 when r=10 and i=5?", "problempicture1": "theall/image/exercise4-2-3-1.PNG", "problempicture2": "theall/image/exercise4-2-3-2.PNG", "problempicture3": "theall/image/exercise4-2-3-3.PNG", "problempicture4": "theall/image/exercise4-2-3-4.PNG", "problempicture5": "", "problempicture6": "", "choicesa": "0.47;0.3", "choicesb": "$\\frac{32}{41}$;$\\frac{9}{82}$", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "After understanding the problem,we use the formula in the attached figure<br>\r\nAFI=$\\frac{1}{1+(r-1)(1-p)^i}$=$\\frac{32}{41}$<br>\r\nAOQ=(1-AFI)$*$p=$\\frac{9}{82}$<br>", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 114, "fields": {"code": "4.2.4", "category": 2, "problem": "Section 4.2.2 already determined the availability R of a certain computer system.R1 for one repair facility and<br>\r\nR2 for two repair facilities.\r\n p is the computer failure probability on a single day. Compute and compare\r\nR1 and R2 for p=0.3.", "problempicture1": "theall/image/exercise4-2-2-1or4_654n1aX.PNG", "problempicture2": "theall/image/exercise4-2-2-2_IvEYHBK.PNG", "problempicture3": "theall/image/exercise4-2-2-3_KSIVAJ7.PNG", "problempicture4": "theall/image/exercise4-2-2-4_1cFyAT3.PNG", "problempicture5": "", "problempicture6": "", "choicesa": "1;1", "choicesb": "0.02;0.4", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Directly use the formula on textbook\r\n  R1=$\\frac{1}{1+p^{2}}$=$\\frac{100}{109}$<br>\r\n  R2=$\\frac{1+p}{1+p+p^{2}}$=$\\frac{130}{139}$<br>", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 115, "fields": {"code": "4.2.5", "category": 2, "problem": "From purchase to purchase, a particular customer switches brands among products\r\nA,B and C according to a Markov chain whose transition probability\r\nmatrix is\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,\\,\\,A\\quad\\quad B\\quad\\quad C\\\\\r\n {\\bf P}&=& \\matrix{A \\cr B \\cr C }\r\n  \\pmatrix{\r\n  0.6& 0.2   &0.2 \\cr\r\n \r\n  0.1& 0.7  &0.2    \\cr\r\n  0.1   & 0.1      & 0.8\r\n}\r\n\\end{eqnarray*}\r\nIn the long run, what fraction of time does this customer purchase brand A?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.3", "choicesb": "0.2", "choicesc": "0.1", "choicesd": "0.4", "choicese": "none of the above", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "As the matrix is regular, we directly compute the limiting distribution by solving <br> \r\n                       $ P^{T}$$*$$\\pi$=$\\pi$<br>\r\n                        $\\sum$$\\pi_{i}$=1<br>\r\nThe solution is  $\\pi_{A}$=0.2", "linkability1": 1.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 116, "fields": {"code": "4.2.6", "category": 2, "problem": "A component of a computer has an active life, measured in discrete units, that\r\nis a random variable T where<br>\r\n$Pr(T=1)$=$0.1$,$Pr(T=2)$=$0.2$,<br>\r\n$Pr(T=3)$=$0.3$,$Pr(T=4)$=$0.4$,<br>\r\nSuppose one starts with a fresh component, and each component is replaced by\r\na new component upon failure. Determine the long run probability that a failure\r\noccurs in a given period.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\frac{5}{17}$", "choicesb": "0.2", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "the time before the failure we denote as 0,1,2,3(0 means the error already happened)<br>\r\nthen we get the probability transition matrix<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,\\,\\,\\,\\,0\\quad\\quad 1\\quad\\quad 2\\quad\\quad 3\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr1  \\cr 2\\cr3}\r\n  \\pmatrix{\r\n&  0.1& 0.2   &0.3 & 0.4\\cr\r\n  &   1& 0  &0&0    \\cr\r\n &    0& 1  &0 &0    \\cr\r\n  &  0  & 0&1      & 0\r\n}\r\n\\end{eqnarray*}<br>\r\nthe matrix is regular we directly compute the limiting distrubution by solving<br> \r\n                       $ P^{T}$$*$$\\pi$=$\\pi$<br>\r\n                        $\\sum$$\\pi_{i}$=1<br>get \r\n$\\pi_{0}$=$\\frac{5}{17}$,that means error happens at rate  of $\\pi_{0}$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 117, "fields": {"code": "4.2.7", "category": 2, "problem": "Consider a machine whose condition at any time can be observed and classified\r\nas being in one of the following three states:<br>\r\nState 1: Good operating order<br>\r\nState 2: Deteriorated operating order<br>\r\nState 3: In repair<br>\r\nWe observe the condition of the machine at the end of each period in a sequence\r\nof periods. Let Xn denote the condition of the machine at the end of period\r\nn for n =1,2..... Let X0 be the condition of the machine at the start. We\r\nassume that the sequence of machine conditions is a Markov chain with transition\r\nprobabilities(see figure)<br>\r\nand that the process starts in state X0=1.\r\nFind $Pr(X_{4}=1)$and the long run rate of repairs per unit time.", "problempicture1": "theall/image/exercise4-2-7.PNG", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1?1", "choicesb": "0.2?0.3", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "directly computing the $P^{4}_{11}$,we get $Pr(X_{4}=1)$=$\\frac{3367}{4929}$;<br>\r\nsolve the equations,we get the liming distrubution $\\pi$,and get $\\pi_{3}$=$\\frac{1}{31}$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 118, "fields": {"code": "4.2.8", "category": 2, "problem": "At the end of a month, a large retail store classifies each receivable account\r\naccording to\r\n0: Current  <br>\r\n1: 30\u201360 days overdue<br>\r\n2: 60\u201390 days overdue<br>\r\n3: Over 90 days<br>\r\nEach such account moves from state to state according to a Markov chain with\r\ntransition probability matrix\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,\\,\\,\\,\\,0\\quad\\quad 1\\quad\\quad 2\\quad\\quad 3\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr1  \\cr 2\\cr3}\r\n  \\pmatrix{\r\n&  0.95& 0.05   &0.3 & 0.4\\cr\r\n  &   0.5& 0  &0.5&0    \\cr\r\n &    0.2& 0  &0 &0.8    \\cr\r\n  &  0.1  & 0&0     & 0.9\r\n}\r\n\\end{eqnarray*}<br>\r\nIn the long run, what fraction of accounts are over 90 days overdue?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\frac{8}{51}$", "choicesb": "0.2", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "P is regular,the long time behaviour is determined by the solution of<br>\r\n  $P^{T}$*$X$=$X$<br>\r\n   $\\sum$$x_{i}$=$1$<br>\r\n get the answer $x_{3}$=$\\frac{8}{51}$.<br>\r\nSo the fraction is equal to $\\frac{8}{51}$.<br>", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 119, "fields": {"code": "4.3.1", "category": 2, "problem": "A Markov chain has a transition probability matrix(see attached figures)<br>\r\nFind the number of equivalence classes.What is the period of the Markov chain?", "problempicture1": "theall/image/exercise4-3-1.PNG", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1;1", "choicesb": "2;5", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "one can carefully check that every state is communicated(or you can compute the matrix $P^{n}$ to see that).So there are only one class.Besides,all the state share  the same period.Easily checking one(5,for eaxmple) we can find $P^{n}_{55}$$>$0,when n=8 or 5.As 8 and 5 are co-prime,we can see the period should be one.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [35, 37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 120, "fields": {"code": "4.3.2", "category": 2, "problem": "Which states are transient and which are recurrent in the Markov chain whose\r\ntransition probability matrix is attached?", "problempicture1": "theall/image/exercise4-3-2.PNG", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "2,4,5is recurrent?0,1,3 is transient", "choicesb": "2,4,5is trasient?0,1,3 is recurrent", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Carefully computing the$\\sum_{n=0}^{\\infty}$$P^{n}_{ii}$?we can find  only i=2,4,5 is infinity(easily see,there is only 1 remains).So the 2,4,5 is recurrent state,others are trasient.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 121, "fields": {"code": "4.3.3", "category": 2, "problem": "A Markov chain on states {0,1,2,3,4,5} has transition probability matrix(attached figure).\r\nFind the number of communicating classes; which states are recurrent?", "problempicture1": "theall/image/exercise4-3-3.PNG", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "3;state 1", "choicesb": "3;state 0,1,2,3", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Carefully check,we can get (1)0 and 2 (2)1 and 3 (3) 4 and 5 ;the three are communicating class.\r\nso we can only compute one of their $\\sum_{n=0}^{\\infty}$$P^{n}_{ii}$ to decide if they are recurrent.\r\nWe find when i=1,0,the value is infinity. so 0,1,2,3 is recurrent.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [35], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 122, "fields": {"code": "4.3.4", "category": 2, "problem": "Determine the communicating classes and period for each state of the Markov\r\nchain whose transition probability matrix is attached", "problempicture1": "theall/image/exercise4-3-4.PNG", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "The period of states 0, 2, 3, 4, 5 is 1; state 1 is zero", "choicesb": "The period of states 2, 3, 4 ,5 is 4; states 0, 1 is zero", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Carefully checking,we can get (1)2 3 4 5 (2)1  (3) 0 ;the three are communicating class.\r\nso we can only compute one of their $P^{n}_{ii}$ to decide their period.\r\nWe find i=5,0,the value$>$0 when n is large enough,so the period is 1 for state 0,2,3,4,5.Value equals to zero\r\nwhen i=1 and n$>$2. So the period of 1 is zero.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 123, "fields": {"code": "4.1.1", "category": 4, "problem": "Following Example 4.4,\r\nWhat is the average number of times per day that Mr C goes from Gym to\r\nHome?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.343", "choicesb": "0.274", "choicesc": "0.835", "choicesd": "0.8", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Recall that in Example 4.4, $X_n$ represent the states (whereabouts)\r\nof Mr. C. after $n$ steps of transitions. Over a large number, $n$, of steps,\r\nthe fraction of  transitions from G to H is\r\n$${ \\sum_{i=1}^n 1_{\\{X_{i-1}=G, X_i=H\\}} \\over n }\r\n\\approx {\\pi_G } \\times P(X_{n+1}=H|X_n=G)= 0.343 \\times 0.8= 0.274$$\r\nFor a large $n$ transitions, the time in hours are\r\n$$ (12 \\pi_H + 10 \\pi_O + 2 \\pi_G )n = 7.864 n,$$\r\nwhich is $ 7.864 n/24= 0.328n$ days.\r\nIn other words, there are about $ 0.274n$ transitions from G to H\r\nover $0.328n$ days. On average, it is about $ 0.274/.328= 0.835$ times\r\nfrom G to H daily.", "linkability1": 3.0, "linkability2": 2.0, "linkability3": 3.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. You may revise Chapter 4.1 - 4.2.", "messagesuccess": "You get it!", "sensitivity": 1.0, "gussingparameter": 0.25, "difficulty": 5, "calculateddifficulty": null, "linkneuron": [37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 124, "fields": {"code": "4.2.2", "category": 4, "problem": "In Example 4.5, is the process $\\{(X_{n-1}, X_{n}): n \\geq 2\\}$   a $MC$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes.", "choicesb": "No.", "choicesc": "It is uncertain.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No. It is not a MC, because,\r\n\\begin{eqnarray*} && P(X_{n+1}=2| X_n=1, X_{n-1}=1, X_{n-2}=2) =1-p  \\\\\r\n&\\not=& 0\r\n=P(X_{n+1}=2| X_n=1, X_{n-1}=1, X_{n-2}=1, X_{n-3}=2).\r\n\\end{eqnarray*}\r\n(Please contemplate why?)", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 3.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Sorry, it is not correct. Time to revise the concept of Markov chain!", "messagesuccess": "Great! You got the concept of Markov chain!", "sensitivity": 1.0, "gussingparameter": 0.333, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [3], "rightproblems": [], "wrongproblems": [2], "twinproblems": [2]}}, {"model": "mathematics.question", "pk": 125, "fields": {"code": "4.2.4", "category": 4, "problem": "Let a random variable $\\xi$\r\nfollow   exponential distribution\r\nwith density\r\n$ f(x)= e^{-x}$ for $x>0$ and $0$ for $x < 0$.\r\nIn Example 4.6, suppose every up (down) day, the rise (fall), as multiple\r\nof the previous day's closing price, has the same distribution\r\nas that of $\\xi+1$ $(1/(a \\xi+1))$, independent of everything else, where\r\n$a>0$ is a constant.\r\nIs the information above enough to calculate the long term return of the stock, particularly when\r\n$a=1$ or $a=7/4$ or $a=4/7$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes", "choicesb": "No.", "choicesc": "It is uncertain.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $\\eta_i$ be the return of day $n$, as multiple of\r\nthe closing price of day\r\n$n-1$. Then, 1 dollar will grow to\r\n$$\\prod_{i=1}^n \\eta_i\r\n= e^{n \\times (1/n) \\sum_{i=1}^n \\log \\eta_i } $$\r\nafter\r\n$n$ days of trading. Recall that the long run fraction of up (down)\r\ndays is $4/11$ ($7/11$).\r\nObserve that\r\n\\begin{eqnarray*}\r\n&& (1/n)\\sum_{i=1}^n \\log \\eta_i   \\approx 4/11 \\times E\\{\\log(\\xi+1)\\}\r\n+ 7/11 \\times E\\{-\\log(a\\xi + 1)\\}\r\n\\\\\r\n&=&\r\n {4\\over 11}  \\int_0^\\infty \\log(x+1) e^{-x} dx\r\n- {7\\over 11}  \\int_0^\\infty \\log(ax+1) e^{-x} dx\r\n\\end{eqnarray*}\r\nIf $a \\geq 1$, obviously the above quantity is negative.\r\nTherefore, in the long run the stock decreases exponentially fast.\r\nIf $a=4/7$, since $\\log(ax+1) > a \\log(x+1)$, the above quantity is\r\nstill negative, and the stock still decreases exponentially fast.\r\nThe dividing value of $a$ is 0.4635. And the stock increases\r\n(decreases) exponentially fast\r\nwhen $a<0.4635$ ($a>0.4635$).", "linkability1": 3.0, "linkability2": 3.0, "linkability3": 3.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again.", "messagesuccess": "Well done! You have mastered the concept of long run behavior.", "sensitivity": 1.0, "gussingparameter": 0.33, "difficulty": 5, "calculateddifficulty": null, "linkneuron": [37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 126, "fields": {"code": "4.3.2", "category": 4, "problem": "Can you prove Proposition 4.1 (a), (b) and (c)?\r\n<br>\r\n((b) and (c) are beyond requirement)", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes, I can!", "choicesb": "I have no idea...", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "(a). By the definition of $d(i)$, all $n$ with $P_{ii}^{(n)} >0$\r\ncontains $d(i)$ as a factor. Whence, if $k$ is not a multiple of $d(i)$,\r\n$P_{ii}^{(k)}=0$.\r\n\r\n(b). Citing a result of algebra regarding gcd: there exists\r\n$n_1, n_2, ..., n_k$ with $P_{ii}^{(n_j)}>0$ and non-zero integers $a_1,\r\n..., a_k$ such that\r\n$d(i)=a_1 n_1 + \\cdots + a_k n_k$. (Proof of this result is also elmentary;\r\nsee the following remark.)\r\nFor notational simplicity, assume $a_1,..., a_l$ are positive and $a_{l+1},..., a_k$ are\r\nnegative, for some $1 \\leq l \\leq k$.\r\nLet $m=-(a_{l+1}n_{l+1} +\\cdots + a_k n_k) >0$.\r\nFor any  $n\\geq m^2+m$, let $n^*$ be such that\r\n$n  = n^* m + r$, where $ 0 \\leq r < m$. Then,\r\n$n^* > m $ and\r\n\\begin{eqnarray*}\r\nnd(i) &=& n^*m d(i) + r d(i) = n^*m d(i) + r a_1 n_1 + \\cdots + ra_k a_k\r\n\\\\\r\n&=&\r\n -(n^*d(i) - r)(a_{l+1} n_{l+1} + \\cdots + a_k n_k)\r\n+ r a_1 n_1 \\cdots + ra_l a_l.\r\n\\end{eqnarray*}\r\nNotice that $n^*d(i) - r>0$.\r\nThen the claim of part (b) follows by\r\nobserving that, for any nonnegative integers $s_1,...,s_k$,\r\n$$P_{ii}^{(s_1 n_1+ \\cdots s_k n_k)} \\geq\r\nP_{ii}^{(s_1 n_1)} \\times \\cdots \\times P_{ii}^{(s_k n_k)}\r\n\\geq \\{ P_{ii}^{( n_1)} \\}^{s_1}\\times \\cdots \\times\r\n\\{P_{ii}^{(n_k)}\\}^{s_k} > 0.$$\r\n\r\n\r\n(c). If $i \\leftrightarrow j$, there exist $k>0$ and $l>0$ such that\r\n$P_{ij}^{(k)}>0$ and $P_{ji}^{(l)}>0$.\r\nSince, by part (b), $P_{jj}^{(n d(j))}>0$ for all large $n$, it follows\r\nthat\r\n$$ P_{ii}^{ (k+l + n d(j))} \\geq P_{ij}^{(k)} P_{jj}^{ (n d(j))} P_{ji}^{(l)}\r\n>0,$$\r\nfor all large $n$. Hence $k+l + n d(j)$ contains $d(i)$ as a factor\r\nfor all large $n$. Then the difference of\r\n$k+l + (n+1) d(j)$ and $k+l+n d(j)$ for a large $n$, which is\r\n$d(j)$ must contain $d(i)$ as a factor.\r\nAs a result, $d(j) \\geq d(i)$. Likewise, one can show\r\n$d(i) \\geq d(j)$. It then holds that\r\n$d(i)=d(j)$.\r\n\r\n<br><br>\r\nRemark. \r\n<br>\r\nSuppose ${\\cal N}$ is a set of positive integers and $J$ is\r\nthe gcd of ${\\cal N}$. Then,\r\n$$ J= \\min\\{ a_1 n_1+ \\cdots + a_k n_k > 0: \\hbox{ for\r\nall $k\\geq 1$, } n_i \\in {\\cal N} \\hbox{ and } a_i\r\n\\hbox{ are nonzero integers} \\}.$$\r\n In other words, $J$ is the smallest positive integer as a\r\n  combination of elements of ${\\cal N}$ with the combination coefficients\r\n  being integers.\r\n  Proof is a little technical but not too difficult: Suppose $J^* \\equiv a_1 n_1 + \\cdots +a_k n_k$\r\n  is indeed the smallest positive number among such combinations.\r\n  Obviously, $J^* \\leq n$ for all $n \\in {\\cal A}$.\r\n  For any $n \\in {\\cal A}$, write $n= n^* J^* + r$ where\r\n  $ 0 \\leq r < J^*$. Then, $r=0$, for otherwise\r\n  $$n-n^*(a_1 n_1+\\cdots a_k n_k)= n-n^*J^* = r$$\r\n  produces a positive number smaller than $J^*$.\r\n  Hence $J^*$ is a common divisor of ${\\cal N}$.\r\n  Suppose now all $n \\in {\\cal N}$ contain, in addition to\r\n  $J^*$, another common factor $j\\geq 1$.\r\n  Set $b_i = n_i/(J^* j)$ which are positive integers.\r\n  Then,\r\n  $$J^* = a_1 n_1+ \\cdots a_k n_k = J^* j (a_1b_1 + \\cdots + a_kb_k)$$\r\n  As a result, $1/j= (a_1 b_1 + \\cdots +a_k b_k)$, implying that\r\n  $j$ must be $1$. This proves $J^*$ is the gcd $J$.\r\n  As mentioned before, part (b) and (c) in the above Exercise\r\n  is too technical and probabilistically uninteresting and thus not required.", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 3.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Don't worry, it is not that easy. You may go through the answer and see whether you understand it!", "messagesuccess": "Excellent! You make it! I am sure now you have a better understanding of Proposition 4.1!", "sensitivity": 1.0, "gussingparameter": 0.5, "difficulty": 5, "calculateddifficulty": null, "linkneuron": [37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 127, "fields": {"code": "3.1.9", "category": 4, "problem": "A Markov chain $X_n$ ($X_0=0$) has two states 0 and 1 \r\nand transition probability matrix \r\n$$ \\hskip .41in 0 \\quad \\,\\,\r\n\\, 1 $$\r\n$$ \r\n{\\bf P}= \\matrix{0 \\cr 1} \\pmatrix{.80  \\quad .20 \\cr \r\n\t.30\\quad .70}\r\n$$\r\nThen what is $P(X_3=0, X_1 \\not=0, X_2 \\not=0 \\,\\,|\\,\\, X_0=0)$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.7", "choicesb": "0.512", "choicesc": "0.042", "choicesd": "0.8", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "\\begin{eqnarray*}\r\n&&P(X_{3}=0, X_{1} \\not=0, X_{2} \\not=0 \\,\\,|\\,\\, X_{0}=0)\r\n\\\\\r\n&=&\r\nP(X_{3}=0\\,\\,|\\,\\, X_{1}\\not=0, X_{2}\\not=0, X_{0}\\not=0)P(X_{1}\\not=0, X_{2}\\not=0\\,\\,|\\,\\,X_{0}=0)\r\n\\\\\r\n&=&\r\nP(X_{3}=0\\,\\,|\\,\\, X_{2}\\not=0)P(X_{2}\\not=0\\,\\,|\\,\\,X_{1}\\not=0, X_{0}=0)P(X_{1}\\not=0\\,\\,|\\,\\,X_{0}=0)\r\n\\\\\r\n&=&\r\nP(X_{3}=0\\,\\,|\\,\\, X_{2}\\not=0)P(X_{2}\\not=0\\,\\,|\\,\\,X_{1}\\not=0)P(X_{1}\\not=0\\,\\,|\\,\\,X_{0}=0)\r\n\\\\\r\n&=&\r\nP_{10}P_{11}P_{01}\r\n\\\\\r\n&=&\r\n0.3\\times 0.7\\times 0.2\r\n\\\\\r\n&=&\r\n0.042\r\n\\end{eqnarray*}", "linkability1": 1.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, you may try it again. Or revise the concept of Markov chain.", "messagesuccess": "Great! You have mastered the concept of Markov chain!", "sensitivity": 1.0, "gussingparameter": 0.25, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [3, 5, 6, 7], "rightproblems": [41], "wrongproblems": [40], "twinproblems": [40]}}, {"model": "mathematics.question", "pk": 128, "fields": {"code": "3.4.12", "category": 4, "problem": "A Markov chain has transition probability matrix\r\n$$ \\hskip .4in 0 \\quad \\, 1 \\quad \\,\\,2 $$\r\n$${\\bf P}= \\matrix{0 \\cr 1 \\cr 2} \\pmatrix{\r\n\t0 \\quad .5 \\quad .5 \\cr\r\n\t0 \\quad .5 \\quad .5 \\cr\r\n\t0 \\quad .5 \\quad .5  }\r\n$$\r\nWhat is the mean time to reach state 2 starting from state 0?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.", "choicesb": "1.", "choicesc": "2.", "choicesd": "3.", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $s_i$ be the mean time to reach state 2 starting from\r\nstate $i$, $including$ the beginning state if it is 2.\r\nThen,\r\n\\begin{eqnarray*}\r\n&&s_0 = 1 + 0.5\\times(s_1+s_2)\r\n\\\\\r\n&&s_1 = 1 + 0.5\\times(s_1+s_2)\r\n\\\\\r\n&&s_2 = 0\r\n\\end{eqnarray*}\r\nSolve this equation, we have\r\n$$s_0=s_1=2,\\,\\, s_2= 0$$\r\n The mean time to reach state 2 starting from state 0 is 2.", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. It is all about calculating the mean step to a state!", "messagesuccess": "Good! Now you know how to calculate the mean step to a state!", "sensitivity": 1.0, "gussingparameter": 0.2, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [9, 13], "rightproblems": [8], "wrongproblems": [95], "twinproblems": [95]}}, {"model": "mathematics.question", "pk": 129, "fields": {"code": "3.4.14", "category": 4, "problem": "A Markov chain $\\{X_n\\}$ $(X_0=0)$ has transition probability matrix\r\n $$ \\hskip .4in 0 \\quad \\, 1 \\quad \\,\\,2 $$\r\n $${\\bf P}= \\matrix{0 \\cr 1 \\cr 2} \\pmatrix{\r\n \t0  \\quad 0.6 \\quad 0.4 \\cr\r\n \t0   \\quad 0.5 \\quad 0.5 \\cr\r\n \t0   \\quad 0.5 \\quad 0.5  }\r\n $$\r\nLet $T=\\min\\{ n\\geq 0: X_n=2, X_{n+1}=2\\}$, then what is $E(T)$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1.2", "choicesb": "2.4", "choicesc": "3.6", "choicesd": "4.8", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $s_i$ be the mean time to reach 2 such that $X_{s_i} = X_{s_i + 1} = 2$, beginning from state $i$. Then,\r\n\\begin{eqnarray*}\r\n&&s_0 = 0.6s_1 + 0.4s_2 + 1\r\n\\\\\r\n&&s_1 = 0.5s_1 + 0.5s_2 + 1\r\n\\\\\r\n&&s_2 = 0.5(s_1+1) + 0.5\\times0\r\n\\end{eqnarray*}\r\nThe solutions are $s_0 = 3.6, \\,\\, s_1 = 3, \\,\\, s_2 = 2$.\r\n\r\nSince $X_0 = 0$, we have $E(T) = s_0 = 3.6$.", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. It is about calculating the mean step to a state!", "messagesuccess": "Great! Now you should be familiar with calculating the mean step to a state.", "sensitivity": 1.0, "gussingparameter": 0.2, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [13], "rightproblems": [51], "wrongproblems": [95], "twinproblems": [95]}}, {"model": "mathematics.question", "pk": 130, "fields": {"code": "3.4.16", "category": 4, "problem": "A Markov chain $\\{X_n\\}$ $(X_0=0)$ has transition probability matrix\r\n$$ \\hskip .4in 0 \\quad \\, 1 \\quad \\,\\,2 $$\r\n$${\\bf P}= \\matrix{0 \\cr 1 \\cr 2} \\pmatrix{\r\n\t.2  \\quad 0.4 \\quad 0.4 \\cr\r\n\t0   \\quad 0.5 \\quad 0.5 \\cr\r\n\t0   \\quad 0.5 \\quad 0.5  }\r\n$$\r\n\r\nWhat is the mean number of visits of state 1 before reaching state 2?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1", "choicesb": "2", "choicesc": "3", "choicesd": "4", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $w_i$ be the mean number of visits of state 1 before reaching state 2, starting from state $i$ (including the starting state).\r\nThen,\r\n\\begin{eqnarray*}\r\n&&w_0 = 0.2w_0 + 0.4w_1 + 0.4w_2\r\n\\\\\r\n&&w_1 = 1 + 0.5w_1 + 0.5w_2\r\n\\\\\r\n&&w_2 = 0\r\n\\end{eqnarray*}\r\nSolving the equations, \r\n$$w_0 = 1, \\,\\, w_1 = 2, \\,\\, w_2 = 0$$\r\nSince $X_0 = 0$, the mean number of visits of state 1 before reaching state 2 is 1.", "linkability1": 1.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. It is about calculating the mean number of visits to a state!", "messagesuccess": "Great! Now you should be familiar with calculating the mean number of visits to a state.", "sensitivity": 1.0, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [15], "rightproblems": [50], "wrongproblems": [4], "twinproblems": [4]}}, {"model": "mathematics.question", "pk": 131, "fields": {"code": "3.9.4", "category": 4, "problem": "Recall that the $n$-th generation of a branching\r\nprocess can be expressed as\r\n$$X_{n+1}= \\sum_{j=0}^{X_n} \\xi^{(n)}_j.$$\r\nwith $\\xi, \\xi^{(n)}_j, j \\geq 1$ are iid. And\r\n$X_0=1$ and $\\xi^{(n)}_0=0.$\r\nLet $\\phi(s)$ be the probability generating function\r\nof $\\xi$.\r\nSuppose $\\phi(s)=0.7+0.1s+0.1s^2+0.1s^3.$\r\nWhat is the probability of eventual extinction?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.2", "choicesb": "0.4", "choicesc": "0.6", "choicesd": "0.8", "choicese": "1", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "E", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Since\r\n$\\phi(s)=0.7+0.1s+0.1s^2+0.1s^3,$\r\n\\begin{eqnarray*}\r\n\\phi(s)-s=0 &\\Rightarrow& 0.7+0.1s+0.1s^2+0.1s^3-s=0 \\\\\r\n&\\Rightarrow&(s-1)(s^2 + 2s - 7)=0\\\\\r\n&\\Rightarrow& s=1 \\,\\, {\\rm or }\\,\\, s = -1+2\\sqrt{2} \\,\\, {\\rm or }\\,\\, s = -1-2\\sqrt{2}.\r\n\\end{eqnarray*}\r\nSince only $s=1\\in [0,1]$, we have $u_{\\infty}=1$.", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Get yourself familiar with Branching process.", "messagesuccess": "Great! You have mastered a typical type of questions about Branching process.", "sensitivity": 1.0, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [14, 20, 21, 22], "rightproblems": [55], "wrongproblems": [83, 132], "twinproblems": [83, 132]}}, {"model": "mathematics.question", "pk": 132, "fields": {"code": "3.9.6", "category": 4, "problem": "Let $\\phi(\\cdot)$ be  the probability generating\r\nfunction of $\\xi$, the number of direct next generation of any\r\nindividual of a population that evolves as a branching process.\r\nSuppose\r\n$$\\phi(s) =\r\n.5 s^2 + bs +c, \\qquad 0 \\leq s \\leq 1.$$\r\nAssume the population begins with one\r\nindividual ($X_0=1$). What is the probability of eventual\r\nextinction?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.4", "choicesb": "0.6", "choicesc": "0.8", "choicesd": "1.0", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "E", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "\\begin{eqnarray*}\r\n{\\rm Since}\\,\\,\\phi(1) = 1 &\\Rightarrow&\\phi(1) = 0.5 + b + c = 1\\\\\r\n&\\Rightarrow&c = .5 - b\\\\\r\n&\\Rightarrow&\\phi(s) = .5 s^2 + bs +.5-b \\\\\r\n\\end{eqnarray*}\r\n<br>\r\n\r\n\\begin{eqnarray*}\r\n\\phi(s)-s=0 &\\Rightarrow& .5s^2+(b-1)s+.5-b=0 \\\\\r\n&\\Rightarrow&(s-1)(.5s+b-.5)=0\\\\\r\n&\\Rightarrow&u_{\\infty} = s = \\left\\{ \\begin{array}{rl}\r\n 1-2b &\\mbox{ if $0<b<.5$} \\\\\r\n  1 &\\mbox{ otherwise.}\r\n       \\end{array} \\right.\r\n\\end{eqnarray*}", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Get yourself familiar with Branching process.", "messagesuccess": "Great! You have mastered a typical type of questions about Branching process.", "sensitivity": 1.0, "gussingparameter": 0.2, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [14, 20, 21, 22], "rightproblems": [55], "wrongproblems": [131], "twinproblems": [131]}}, {"model": "mathematics.question", "pk": 133, "fields": {"code": "4.3.1", "category": 3, "problem": "A two-state Markov chain has the transition probability matrix\r\n\\[\\mathbf{P}=\r\n\\begin{array}{c|cc|}\r\n&0&1\\\\\r\n0&1-a&a\\\\\r\n1&b&1-b\r\n\\end{array}\r\n\\]\r\nDetermine the first return probability\r\n\\begin{equation*}\r\nf^{(n)}_{00}=\\text{Pr}\\{X_1\\neq0,\\dots,X_{n-1}\\neq0,X_n=0|X_0=0\\}\r\n\\end{equation*}", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$b(1-b)^{n-1}a$", "choicesb": "$(1-b)^{n-1}a$", "choicesc": "$b(1-b)^{n-1}$", "choicesd": "$b(1-b)^{n-2}a$", "choicese": "None of the above", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "\\begin{align*}\r\nf^{(n)}_{00}&=\\text{Pr}\\{X_1\\neq0,\\dots,X_{n-1}\\neq0,X_n=0|X_0=0\\}\\\\\r\n&=\\text{Pr}\\{X_2\\neq0,\\dots,X_{n-1}\\neq0,X_n=0|X_1=1\\}\\text{Pr}\\{X_1=1|X_0=0\\}\\\\\r\n&=\\text{Pr}\\{X_2\\neq0,\\dots,X_{n-1}\\neq0,X_n=0|X_1=1\\}P_{01}\\\\\r\n&=\\text{Pr}\\{X_3\\neq0,\\dots,X_{n-1}\\neq0,X_n=0|X_2=1\\}P_{11}P_{01}\\\\\r\n&=\\text{Pr}\\{X_n=0|X_{n-1}=1\\}P_{11}^{n-2}P_{01}\\\\\r\n&=P_{10}P_{11}^{n-2}P_{01}\r\n\\end{align*}\r\n\r\nSo the probability is $b(1-b)^{n-2}a$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [3, 7, 28], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 134, "fields": {"code": "4.1.2", "category": 4, "problem": "A mouse initially stays in cell 0\r\n in the following cage of four cells.\r\n At each step, it moves to one of\r\n the two connected cells with equal chance.\r\n Let $X_n=i$ if the mouse\r\n stays at cell $i$ at step $n$.\r\n Over a very long time,\r\n what  fraction of\r\n the transitions are the transitions from cell 0 to cell 1?", "problempicture1": "theall/image/DIY-4.1.2-1.PNG", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1/8", "choicesb": "1/6", "choicesc": "1/4", "choicesd": "1/2", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The transition matrix for $X_n$ is\r\n \\begin{eqnarray*}\r\n&&  \\,\\,\\qquad  0\\quad \\,\\, 1 \\quad \\,\\, 2 \\quad\\,\r\n 3\\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0}  \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3}}\r\n\\pmatrix{\r\n0 & .5 & 0 & .5  \\cr\r\n.5   & 0   & .5 & 0  \\cr\r\n0 & .5 & 0 & .5  \\cr\r\n.5   & 0   & .5 & 0\r\n}\r\n\\end{eqnarray*}\r\n\r\nNotice that\r\n$${\\bf P}^k ={\\bf P} \\quad \\hbox {for all odd $k$}, \\qquad\r\n{\\bf P}^k= \\pmatrix{.5 & 0 & .5 & 0 \\cr 0  & .5 & 0 & .5 \\cr .5 & 0 & .5 & 0 \\cr 0  & .5 & 0 & .5 } \\qquad \\hbox{for all even $k$}.$$\r\nLet $\\pi_j$ be the long run fraction of time staying in state $j$ after an odd-number times of transitions. Solve the equations\r\n$$(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2\\,\\,\\pi_3)=(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2\\,\\,\\pi_3){\\bf P}^2$$\r\n$$\\pi_0 + \\pi_1 + \\pi_2 + \\pi_3=1$$\r\nThe solutions are $\\pi_0 = \\pi_1 = \\pi_2 = \\pi_3 = 1/4$.\r\n\r\nSimilarly, if the number of transitions is even in a long run, we also have\r\n$$(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2\\,\\,\\pi_3)=(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2\\,\\,\\pi_3){\\bf P}^2$$\r\n$$\\pi_0 + \\pi_1 + \\pi_2 + \\pi_3=1$$\r\nThus in the long run, $\\pi_0 = \\pi_1 = \\pi_2 = \\pi_3 = 1/4$.\r\n\r\nIn the long run, the fraction of the transitions from cell 0 to cell 1 is\r\n\\begin{eqnarray*}\r\n&&P(X_{n+1}=1,\\,X_n=0)\r\n\\\\\r\n&=&\r\nP(X_{n+1}=1,\\,X_n=0\\,|\\,X_{n-1}=0)+P(X_{n+1}=1,\\,X_n=0\\,|\\,X_{n-1}=1)\r\n\\\\\r\n&&\r\n+P(X_{n+1}=1,\\,X_n=0\\,|\\,X_{n-1}=2)+P(X_{n+1}=1,\\,X_n=0\\,|\\,X_{n-1}=3)\r\n\\\\\r\n&=& \r\nP_{01}P_{00}\\pi_0+P_{01}P_{10}\\pi_1+P_{01}P_{20}\\pi_2+P_{01}P_{30}\\pi_3\r\n\\\\\r\n&=&\r\n{1\\over 4} P_{01}(P_{00}+P_{10}+P_{20}+P_{30})\r\n\\\\\r\n&=&\r\n1\\over 8\r\n\\end{eqnarray*}", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Get yourself familiar with Long run behavior.", "messagesuccess": "Great! You have mastered a typical type of questions about long run behavior.", "sensitivity": 1.0, "gussingparameter": 0.2, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [35, 37, 39], "rightproblems": [103, 135], "wrongproblems": [102], "twinproblems": [102]}}, {"model": "mathematics.question", "pk": 135, "fields": {"code": "4.1.4", "category": 4, "problem": "A Markov chain $X_n, n\\geq 0$, has transition probability\r\nmatrix\r\n$$ \\hskip .4in 0 \\quad \\, 1 \\quad \\,\\,2 $$\r\n$${\\bf P}= \\matrix{0 \\cr 1 \\cr 2} \\pmatrix{\r\n\t.2 \\quad .5 \\quad .3 \\cr 0 \\quad .5 \\quad .5 \\cr .4 \\quad .5 \\quad\r\n\t.1  }\r\n$$\r\nWhat is the limit of $P(X_n=0, X_{n+1}=1, X_{n+2}=2)$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1/24", "choicesb": "1/8", "choicesc": "1/6", "choicesd": "1/4", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $\\pi_j$ be the long run fraction of time staying in state $j$. Notice that ${\\bf P}$ is regular. Solve the equations\r\n$$(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2)=(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2){\\bf P}^2$$\r\n$$\\pi_0 + \\pi_1 + \\pi_2=1$$\r\nThe solutions are $\\pi_0 = 1/6$, $\\pi_1 =1/2$, $\\pi_2 =1/3$.\r\n\\begin{eqnarray*}\r\n&&P(X_n=0, X_{n+1}=1, X_{n+2}=2)\r\n\\\\\r\n&=& \r\nP(X_{n+2}=2\\,|\\,X_{n+1}=1,\\,X_n=0)P(X_{n+1}=1\\,|\\,X_n=0)P(X_n=0)\r\n\\\\\r\n&=&\r\nP(X_{n+2}=2\\,|\\,X_{n+1}=1)P(X_{n+1}=1\\,|\\,X_n=0)P(X_n=0)\r\n\\\\\r\n&=&\r\nP_{12}P_{01}\\pi_0\r\n\\\\\r\n&=&\r\n1/2 \\times 1/2 \\times 1/6\r\n\\\\\r\n&=&\r\n1/24.\r\n\\end{eqnarray*}", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Get yourself familiar with Long run behavior.", "messagesuccess": "Great! You have mastered a typical type of questions about long run behavior.", "sensitivity": 1.0, "gussingparameter": 0.2, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [35, 37, 39], "rightproblems": [134], "wrongproblems": [102], "twinproblems": [102]}}, {"model": "mathematics.question", "pk": 136, "fields": {"code": "4.1.6", "category": 4, "problem": "The following is the transition probability matrix\r\nof a Markov chain with states 0, 1 and 2:\r\n$$ \\hskip .4in 0 \\quad \\,\\,\\,\\,\\, 1 \\quad \\,\\,\\,\\,\\, 2 $$\r\n$${\\bf P} = \\matrix{0 \\cr 1 \\cr 2}\r\n\\pmatrix{.50 \\quad .50 \\quad \\,\\, 0 \\cr\r\n\t\\,\\,.25 \\quad .50 \\quad .25 \\cr\r\n\t\\,\\,\\, 0 \\quad \\, \\,\\,\\, .50 \\quad \\, .50 }\r\n$$\r\nSomeone is paid 2 dollar for each one step transit from\r\nstate 1 to state 2 and  $-1$ dollar each one step transit from\r\nstate 2 to state 1. After 1000,000 steps, approximately how much\r\nis gained?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "50,000", "choicesb": "75,000", "choicesc": "100,000", "choicesd": "125,000", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $\\pi_j$ be the long run fraction of time staying in state $j$. Notice that ${\\bf P}$ is regular. Solve the equations\r\n$$(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2)=(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2){\\bf P}$$\r\n$$\\pi_0 + \\pi_1 + \\pi_2=1$$\r\nThe solutions are $\\pi_0 = 1/4$, \\, $\\pi_1 =1/2$,\\, $\\pi_2 =1/4$.\r\n\\begin{eqnarray*}\r\n&&P(X_{n+1}=1, X_{n+2}=2)\r\n\\\\\r\n&=& \r\nP(X_{n+1}=1, X_{n+2}=2\\,|\\,X_n=0)+P(X_{n+1}=1, X_{n+2}=2\\,|\\,X_n=1)+P(X_{n+1}=1, X_{n+2}=2\\,|\\,X_n=2)\r\n\\\\\r\n&=&\r\nP(X_{n+2}=2\\,|\\,X_{n+1}=1)P(X_{n+1}=1\\,|\\,X_n=0)P(X_n=0)\r\n\\\\\r\n&&\r\n+P(X_{n+2}=2\\,|\\,X_{n+1}=1)P(X_{n+1}=1\\,|\\,X_n=1)P(X_n=1)\r\n\\\\\r\n&&\r\n+P(X_{n+2}=2\\,|\\,X_{n+1}=1)P(X_{n+1}=1\\,|\\,X_n=2)P(X_n=2)\r\n\\\\\r\n&=&\r\nP_{12}P_{01}\\pi_0+P_{12}P_{11}\\pi_1+P_{12}P_{21}\\pi_2\r\n\\\\\r\n&=&\r\n1/8\r\n\\end{eqnarray*}\r\n\\begin{eqnarray*}\r\n&&P(X_{n+1}=2, X_{n+2}=1)\r\n\\\\\r\n&=& \r\nP(X_{n+1}=2, X_{n+2}=1\\,|\\,X_n=0)+P(X_{n+1}=2, X_{n+2}=1\\,|\\,X_n=1)+P(X_{n+1}=2, X_{n+2}=1\\,|\\,X_n=2)\r\n\\\\\r\n&=&\r\nP(X_{n+2}=1\\,|\\,X_{n+1}=2)P(X_{n+1}=2\\,|\\,X_n=0)P(X_n=0)\r\n\\\\\r\n&&\r\n+P(X_{n+2}=1\\,|\\,X_{n+1}=2)P(X_{n+1}=2\\,|\\,X_n=1)P(X_n=1)\r\n\\\\\r\n&&\r\n+P(X_{n+2}=1\\,|\\,X_{n+1}=2)P(X_{n+1}=2\\,|\\,X_n=2)P(X_n=2)\r\n\\\\\r\n&=&\r\nP_{21}P_{02}\\pi_0+P_{21}P_{12}\\pi_1+P_{21}P_{22}\\pi_2\r\n\\\\\r\n&=&\r\n1/8\r\n\\end{eqnarray*}\r\nThe approximate gain after 1000,000 steps would be\r\n$$1000,000 \\times {{1\\over 8} \\times {2} + {1\\over 8} \\times (-1)} = 125,000.$$", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Get yourself familiar with Long run behavior.", "messagesuccess": "Great! You have mastered a typical type of questions about long run behavior.", "sensitivity": 1.0, "gussingparameter": 0.2, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [35, 37, 39], "rightproblems": [], "wrongproblems": [102], "twinproblems": []}}, {"model": "mathematics.question", "pk": 137, "fields": {"code": "4.2.6", "category": 4, "problem": "A stock has only two outcomes of performance\r\n on each trading day: up (U) or down (D). Let $X_n$ be\r\n its performance outcome on trading day $n$. The stock performance\r\n on any day depends only on those of the preceding two days. Assume\r\n for all $n\\geq 1$,\r\n $P(X_{n+1}=U| X_n = U, X_{n-1}=U) = 0.9$,\r\n $P(X_{n+1}=U| X_n = U, X_{n-1}=D) = 0.7$, $P(X_{n+1}=U| X_n = D,\r\n X_{n-1}=U) = 0.4$, $P(X_{n+1}=U| X_n = D, X_{n-1}=D) = 0.2$. What\r\n is the approximate chance that the stock is up on the trading day ten years from\r\n now?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1/3", "choicesb": "1/4", "choicesc": "2/3", "choicesd": "3/4", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Set $Y_n=(X_{n-1},X_n)$. Then $Y_n$ is a $MC$\r\nwith state space $\\{({\\rm U, U}), ({\\rm D, U}),\r\n({\\rm U, D}),   ({\\rm D, D})\\}$ and transition probability\r\nmatrix\r\n$$ \\hskip .8 in 0 ({\\rm U, U}) \\quad \\,\\,\\,\\,\\, 1 ({\\rm D, U}) \\quad \\,\\,\\,\\,\\, 2 ({\\rm U, D})\\quad \\,\\,\\,\\,\\, 3 ({\\rm D, D}) $$\r\n$${\\bf P} = \\matrix{0 ({\\rm U, U}) \\cr 1 ({\\rm D, U}) \\cr 2 ({\\rm U, D})\\cr 3 ({\\rm D, D})}\r\n\\pmatrix{.9\\quad\\quad \\quad\\quad\\quad 0 \\quad\\quad\\quad\\quad\\quad .1 \\quad\\quad\\quad\\quad\\quad 0\\cr\r\n\t.7\\quad\\quad \\quad\\quad\\quad 0 \\quad\\quad\\quad\\quad\\quad .3 \\quad\\quad\\quad\\quad\\quad 0\\cr\r\n\t0\\quad\\quad \\quad\\quad\\quad .4 \\quad\\quad\\quad\\quad\\quad 0 \\quad\\quad\\quad\\quad\\quad .6\\cr\r\n      0\\quad\\quad \\quad\\quad\\quad .2 \\quad\\quad\\quad\\quad\\quad 0 \\quad\\quad\\quad\\quad\\quad .8}\r\n$$\r\nSolve the equations\r\n$$(\\pi_0\\,\\, \\pi_1 \\,\\,\\pi_2\\,\\,\\pi_3)=(\\pi_0\\,\\, \\pi_1 \\,\\,\\pi_2\\,\\,\\pi_3){\\bf P}$$\r\n$$\\pi_0+\\pi_1+\\pi_2+\\pi_3=1$$\r\nThe solutions are $\\pi_0=7/12,\\,\\, \\pi_1=\\pi_2=1/12, \\,\\,\\pi_3=1/4$.\r\n<br>\r\nIn the long run, we have\r\n\\begin{eqnarray*}\r\n&&P(X_n=U)\r\n\\\\\r\n&=&\r\nP(X_n=U, X_{n-1}=D)+P(X_n=U, X_{n-1}=U)\r\n\\\\\r\n&=&\r\nP(Y_n=(D,U))+P(Y_n=(U,U))\r\n\\\\\r\n&=&\r\n\\pi_1 + \\pi_0\r\n\\\\\r\n&=&\r\n1/2+7/12\r\n\\\\\r\n&=&\r\n2/3.\r\n\\end{eqnarray*}", "linkability1": 3.0, "linkability2": 2.0, "linkability3": 3.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Get yourself familiar with Long run behavior.", "messagesuccess": "Great! You have mastered a typical type of questions about long run behavior.", "sensitivity": 1.0, "gussingparameter": 0.2, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [35, 37, 39], "rightproblems": [108], "wrongproblems": [109], "twinproblems": [109]}}, {"model": "mathematics.question", "pk": 138, "fields": {"code": "4.3.4", "category": 4, "problem": "A Markov chain has transition probability matrix\r\n $$ \\hskip .4in \\, 0 \\quad \\, 1 \\quad \\,\\,2 $$\r\n $${\\bf P}= \\matrix{0 \\cr 1 \\cr 2} \\pmatrix{\r\n \t0 \\quad .6 \\quad .4 \\cr\r\n \t0 \\quad .7 \\quad .3 \\cr\r\n \t0 \\quad .5 \\quad .5  }\r\n $$\r\nWhich states are transient states?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "State 0.", "choicesb": "State 1.", "choicesc": "State 2.", "choicesd": "Both State 1 and State 2.", "choicese": "Both State 2 and State 3.", "choicesf": "Both State 1 and State 3.", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "For $n>0$, starting from state 0, the $MC$ will never return to state 0 at any step $n$.\r\n$f_{00}^{(0)}=0$, $f_{00}=0$. Therefore, state 0 is transient.", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Get yourself familiar with Long run behavior.", "messagesuccess": "Great! You have mastered a typical type of questions about long run behavior.", "sensitivity": 1.0, "gussingparameter": 0.167, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 139, "fields": {"code": "4.2.1", "category": 4, "problem": "For a nonnegative random variable $Y$,\r\n$E(Y) = \\int_0^\\infty P(Y> x) dx$. \r\nIf $Y$ is nonnegative integer-valued,\r\n$E(Y) =\\sum_{k=0}^\\infty P(Y>k)$ and \r\n$E(Y) = \\sum_{k=1}^\\infty P(Y \\geq k)$\r\nHow many of the above equations are correct?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0", "choicesb": "1", "choicesc": "2", "choicesd": "3", "choicese": "none of the above", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "(This is purely a problem of basic probability.)\r\nBy definition,\r\n\\begin{eqnarray*}\r\nE(Y) &=&  \\int_0^\\infty y dP(Y \\leq y) = - \\lim_{a \\uparrow \\infty}\r\n\\int_0^a  y d P(Y > y) =\r\n  \\lim_{a \\uparrow \\infty} \\{ -aP(Y>a)+\r\n\\int_0^a    P(Y > y) dy \\}\r\n\\\\\r\n&=&\r\n\\lim_{a \\uparrow \\infty}\r\n\\int_0^a    P(a\\geq  Y > y) dy\r\n=\\int_0^\\infty   P(  Y > y) dy .\r\n\\end{eqnarray*}\r\n If $Y$ is integer valued,\r\n$$E(Y)= \\int_0^\\infty   P(  Y > y) dy\r\n= \\sum_{k=0}^\\infty \\int_k^{k+1}P(Y>y) dy\r\n= \\sum_{k=0}^\\infty P(Y> k)=\\sum_{k=1}^\\infty P(Y\\geq k).$$", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [23], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 140, "fields": {"code": "4.2.3", "category": 4, "problem": "In Example 4.6 of lecture notes, change the conditional probability of\r\nU given (D, D) to 0.2 and D given (D,D) to 0.8. Find out\r\nthe long run fraction of up days.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.25", "choicesb": "0.5", "choicesc": "0.75", "choicesd": "1", "choicese": "0", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "With this change, the transition probabilities\r\nare symmetric with D and U. Therefore, our intuition is that\r\nthe long run fractions of up days and down days should be\r\nthe same, which must be 1/2.\r\nIn fact, solving the equations, you will find that\r\n$\\pi_{DD}=\\pi_{UU}$ and $\\pi_{DH}=\\pi_{HD}$\r\n(Please check.)\r\nAnd you find that the long run fraction of up days is\r\n$\\pi_{DD} +\\pi_{DH} = 1/2$.", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [46], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 141, "fields": {"code": "4.3.1", "category": 4, "problem": "A regular {\\it MC} is  ?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "irreducible", "choicesb": "aperiodic", "choicesc": "irreducible and aperiodic", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "If regular, there exits a $k>0$ such that\r\nthe $k$-step transition matrix has all positive entries.\r\ni.e, $P_{ij}^{(k)}>0$ for all states $i, j$.\r\nAs $\\sum_{l=0}^\\infty P_{il} = 1$, there must be an $l$ such\r\nthat $P_{il} > 0$. Hence,\r\n $P_{ij}^{(k+1)}= \\sum_{l=0}^\\infty P_{il} P_{lj}^{(k)} >0$,\r\n for all states $i, j$.\r\n By induction, we know, for all $n \\geq k$,  $n$-step transition matrix has\r\n all positive entries. Therefore, the {\\it MC} is\r\n irreducible and aperiodic.", "linkability1": 1.0, "linkability2": 0.0, "linkability3": 1.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [35, 52, 53], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 142, "fields": {"code": "4.3.3", "category": 4, "problem": "Consider the transition matrix  in Example 4.8. Suppose $X_0=1$.\r\n(a). Compute $f_{11}^{(n)}$ for all $n \\geq  0$.\r\n(b). Compute $E(M)$, the mean number of visits of state 1 using\r\nTry to use first step analysis to verify whether your calculation in\r\n(b) is correct.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$(1/2)^n$,  1", "choicesb": "$(1/2)^n$,  1/2", "choicesc": "$(1/2)^(n-1)$,  1", "choicesd": "$(1/2)^(n-1)$,   1/2", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "(a). $f_{11}^{(0)}=0$, $f_{11}^{(1)}=0$, and for $n\\geq 2$, the only path\r\nto come back at step $n$ for the first time is going to state 1 at step 1, staying there\r\nall the time and then coming back to state 1 at step n. Therefore, for $n\\geq 2$,\r\n\\begin{eqnarray*}\r\n f_{11}^n &=& P(X_n=1, X_{n-1}\\not=1, ..., X_1 \\not=1 |X_0=1)\r\n\\\\\r\n&=& P(X_n=1, X_{n-1}=0, ..., X_1 =0 |X_0=1)\r\n=\r\n P_{10}P_{00} \\cdots P_{00}P_{01}\r\n \\\\&=& (1/2)^n.\r\n \\end{eqnarray*}\r\n(b). Since\r\n$$f_{11} = \\sum_{n=1}^\\infty f_{11}^{(n)}= \\sum_{n=2}^\\infty 2^{-n} = 1/2.$$\r\nIt follows from (4.4) that $E(M) = f_{11}/(1-f_{11}) = 1$.\r\n\r\n Let $s_i$ be mean number of visits of state 1, beginning from state $i$ and\r\nexcluding the beginning state if it is 1. Then,\r\n$$ s_1 = 1/2 s_0 \\quad \\hbox{and} \\quad s_0 = 1/2 s_0 + 1/2 (1+s_1)$$\r\nSolving the equation, we have $s_1 = 1 $, which is the same as $E(M)$.", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [56, 57], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 143, "fields": {"code": "3.4.11", "category": 4, "problem": "A mouse initially stays in cell 0\r\nin the following cage of four cells.\r\nAt each step, it moves to one of\r\nthe two connected cells with equal chance.\r\nWhat is the\r\nthe mean number of steps the mouse passes by\r\ncell 1 before it reaches cell 3?", "problempicture1": "theall/image/diy3.4.11.png", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0", "choicesb": "1", "choicesc": "2", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Set $w_{i}$ the mean number of steps the mouse passes by cell 1 before it reaches cell 3 starting from cell $i$. Then obviously $w_{3} = 0$. By symmetry, $w_{0} = w_{2}$.\r\nMoreover,\r\n\\begin{align*}\r\nw_{0} &= \\frac{1}{2}(w_{1}+w_{3})\\\\\r\nw_{1} &= 1 + \\frac{1}{2}(w_{0} + w_{2})\\\\\r\nw_{2} &= w_{0}\\\\\r\nw_{3} &= 0\r\n\\end{align*}\r\nSolving the above equations,\r\n$w_{0} = w_{2} = 1, w_{1} = 2, w_{3} = 0$.\\\\\r\nStarting from cell 0, the mean number of steps the mouse passes by cell 1 before 3 is 1.", "linkability1": 1.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [15], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 144, "fields": {"code": "3.4.13", "category": 4, "problem": "A mouse travels randomly in between the 4 points in the\r\n  following figure. It starts from point A. From any point,  it\r\n  moves to each of the surrounding connected points with equal\r\n  probability. (For example, for point C, it moves B or D with 1/2\r\n  probability; from point B,  it moves to A, C or D with 1/3\r\n  probability; etc.) What is the probability that it reaches point D\r\n  before reaching point C?", "problempicture1": "theall/image/diy3.4.13.png", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0", "choicesb": "0.6", "choicesc": "0.8", "choicesd": "1", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "First, number A, B, C, D as 1, 2, 3, 4.\r\nSet $p_{i}$ = P(the mouse reaches point 4 before reaching 3 $|$ starting from point $i$).\r\nThen,\r\n\\begin{align*}\r\np_{1} &= P_{12}p_{2} + P_{14}p_{4} = \\frac{1}{2}(p_{2}+p_{4})\\\\\r\np_{2} &= \\frac{1}{3}(p_{1} + p_{3}+p_{4})\\\\\r\np_{3} &= 0\\\\\r\np_{4} &= 1\r\n\\end{align*}\r\nSolving the above equations,\r\n$p_{1} =\\frac{4}{5}, p_{2} = \\frac{3}{5}, p_{3} = 0, p_{4}=1$.\r\nStarting from point A, the probability that the mouse reaches point D before C is $\\frac{4}{5}$.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [9, 12], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 145, "fields": {"code": "3.4.15", "category": 4, "problem": "The following is the transition probability matrix\r\nof a Markov chain with states 0, 1 and 2:\r\n$$ \\hskip .4in 0 \\quad \\,\\,\\,\\,\\, 1 \\quad \\,\\,\\,\\,\\, 2 $$\r\n$${\\bf P} = \\matrix{0 \\cr 1 \\cr 2}\r\n\\pmatrix{.50 \\quad .50 \\quad \\,\\, 0 \\cr\r\n\t\\,\\,.30 \\quad .50 \\quad .20 \\cr\r\n\t\\,\\,\\, 0 \\quad \\, \\,\\,\\, .50 \\quad \\, .50 }\r\n$$\r\nWhat is the approximate probability of\r\n$P(X_{n-1}=X_n = X_{n+1}=0)$ for a large $n$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.3", "choicesb": "0.15", "choicesc": "0.075", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "t is easily verified that $P^{2}$ has all entries positive and therefore $P$ is regular.\r\nThen we have,\r\n\\begin{align*}\r\n\\pi_{0} &= 0.5\\pi_{0} + 0.3\\pi_{1} \\\\\r\n\\pi_{1} &= 0.5\\pi_{0} +0.5\\pi_{1} + 0.5\\pi_{2}\\\\\r\n\\pi_{2} &= 0.2\\pi_{1}+0.5\\pi_{2}\\\\\r\n\\pi_{0} &+\\pi_{1}+\\pi_{2} = 1\r\n\\end{align*}\r\nSolving the above equations, $\\pi_{0} = 0.3, \\pi_{1} = 0.5, \\pi_{2} = 0.2$\\\\\r\nTherefore, $P(X_{n-1} = 0) = 0.3$ for large n.\r\n\\begin{align*}\r\nP(X_{n-1} = X_{n} = X_{n+1} = 0) &= P(X_{n+1} = 0 | X_{n} = X_{n-1} = 0) P(X_{n} = 0 | X_{n-1} = 0)P(X_{n-1} = 0) \\\\\r\n&= P(X_{n+1} = 0 | X_{n} = 0) P(X_{n} = 0 |X_{n-1} = 0) P(X_{n-1} = 0)\\\\\r\n&= P_{00} \\times  P_{00}  \\times \\pi_{0}\\\\\r\n&= 0.075 \r\n\\end{align*}", "linkability1": 1.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [25, 37, 39], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 147, "fields": {"code": "3.4.17", "category": 4, "problem": "A Markov chain $\\{X_n\\}$ $(X_0=0)$ has transition probability matrix\r\n$$ \\hskip .4in 0 \\quad \\, 1 \\quad \\,\\,2 $$\r\n$${\\bf P}= \\matrix{0 \\cr 1 \\cr 2} \\pmatrix{\r\n\t.2  \\quad 0.4 \\quad 0.4 \\cr\r\n\t0   \\quad 0.5 \\quad 0.5 \\cr\r\n\t0   \\quad 0.5 \\quad 0.5  }\r\n$$\r\nCompute $E(T)$ where $T=\\min\\{ n\\geq 0: X_n=2, X_{n+1}=2\\}$.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "3", "choicesb": "5", "choicesc": "5.25", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Set $w_{i}$ the mean number of steps before the first two consecutive state 2 occur (including the step firstly reaches state 2 of the consecutive states starting from state 0).\r\nThen,\r\n\\begin{align*}\r\nw_{0} &= (0.2w_{0} + 0.4w_{1} + 0.4w_{2}) + 1\\\\\r\nw_{1} &= (0.5w_{1} + 0.5w_{2}) + 1\\\\\r\nw_{2} &= 0.5(1+w_{1}) + 0.5\\times 0\r\n\\end{align*}\r\nSolving the equations, $w_{0} = 5.25, w_{1}=5, w_{2} =  3$.\r\nTherefore, $E(T) = w_{0} = 5.25$", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [9, 12], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 148, "fields": {"code": "3.9.5", "category": 4, "problem": "Let $\\phi(\\cdot)$ be  the probability generating\r\nfunction of $\\xi$, the number of direct next generation of any\r\nindividual of a population that evolves as a branching process.\r\nSuppose\r\n$$\\phi(s) =\r\n.5 s^2 + bs +c, \\qquad 0 \\leq s \\leq 1.$$\r\n\r\nSpecify the conditions on the constants\r\n$b$ and $c$.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "b+c=1", "choicesb": "b+c=0.5", "choicesc": "$0\\leq b \\leq 0.5, 0 \\leq c \\leq 0.5$", "choicesd": "b and c", "choicese": "a and c", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "E", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Note that,\r\n\\begin{align*}\r\n\\phi(s) = E(s^{\\xi}) = \\Sigma_{k=0}^{\\infty}s^{k}p_{k} &= p_{0} + p_{1}s + p_{2}s^{2} + p_{3}s^{3} \\dots\\\\\r\n&= \\phi(s) = 0.5s^{2} + bs + c\r\n\\end{align*}\r\nAnd $\\Sigma_{i=0}^{\\infty} p_{i} = 1, p_{i} \\geq 0$ for all $i$.\r\nThen,  {$0.5 + b + c = 1, b\\geq 0, c\\geq 0$}\r\ni.e. {$b + c = 0.5, 0\\leq b \\leq 0.5, 0 \\leq c \\leq 0.5$}", "linkability1": 1.0, "linkability2": 1.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [20], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 149, "fields": {"code": "3.9.7", "category": 4, "problem": "Consider a branching process $\\{X_n: n \\geq 0\\}$ with $X_0={\\bf 2}$. \r\nAssume the distribution of $\\xi$, the size of next generation fathered by one individual in \r\nthe population, is \r\n$$P(\\xi=0)= 0.2, \\quad  P(\\xi=1)=0.5, \\quad P(\\xi=2)=0.2,\\quad P(\\xi=3)=0.1.$$   \r\nCompute the probability of eventual extinction of the population.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1", "choicesb": "0", "choicesc": "$\\frac{-3+\\sqrt{17}}{2}$", "choicesd": "$\\frac{-3-\\sqrt{17}}{2}$", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "For $n \\geq 1, X_{0} = 1$,\r\n\\begin{equation*}\r\nu_{n} = \\Sigma_{k=0}^{3}u_{n-1}^{k}p_{k} = 0.2 + 0.5u_{n-1} + 0.2u_{n-1}^{2} + 0.1u_{n-1}^{3}\r\n\\end{equation*}\r\nLet $n \\rightarrow \\infty$, $u_{n} = 1$ or $\\frac{-3+\\sqrt{17}}{2}$ or $\\frac{-3-\\sqrt{17}}{2}$.\r\nThen the probability of eventual extinction is $\\frac{-3+\\sqrt{17}}{2}$.", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [21], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 150, "fields": {"code": "4.1.3", "category": 4, "problem": "The following is the transition probability matrix\r\n of a Markov chain with states 0, 1 and 2:\r\n $$ \\hskip .4in 0 \\quad \\,\\,\\,\\,\\, 1 \\quad \\,\\,\\,\\,\\, 2 $$\r\n $${\\bf P} = \\matrix{0 \\cr 1 \\cr 2} \r\n \\pmatrix{.50 \\quad .50 \\quad \\,\\, 0 \\cr\r\n \t\\,\\,.25 \\quad .50 \\quad .25 \\cr\r\n \t\\,\\,\\, 0 \\quad \\, \\,\\,\\, .50 \\quad \\, .50 }\r\n $$\r\n In the long run, what is the mean fraction of\r\n time the Markov chain stays at state 0?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.25", "choicesb": "0.5", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "It is easily verified that $P^{2}$ has all entries positive and therefore $P$ is regular.\r\nThen we have, ($\\pi_{0}$ $\\pi_{1}$ $\\pi_{2}$) = ($\\pi_{0}$ $\\pi_{1}$ $\\pi_{2}$)$P$, $\\pi_{0}+ \\pi_{1}+\\pi_{2} = 1$.\r\ni.e. \r\n\\begin{align*}\r\n\\pi_{0} &= 0.5\\pi_{0} + 0.25\\pi_{1}\\\\\r\n\\pi_{1} &= 0.5 \\pi_{0} + 0.5\\pi_{1} + 0.5\\pi_{2}\\\\\r\n\\pi_{2} &= 0.25\\pi_{1} + 0.5\\pi_{2}\\\\\r\n\\pi_{0}&+ \\pi_{1}+\\pi_{2} = 1\r\n\\end{align*}\r\nSolving the equations, $\\pi_{0} = 0.25, \\pi_{1} = 0.5, \\pi_{2} = 0.25$.\r\nIn the long run, the mean fraction of time the Markov Chain stays at state 0 is 0.25.", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [39], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 151, "fields": {"code": "4.1.5", "category": 4, "problem": "A Markov chain $\\{X_n\\}$ has transition probability matrix\r\n$$ \\hskip .4in 0 \\quad \\, 1 \\quad \\,\\,2 $$\r\n$${\\bf P}= \\matrix{0 \\cr 1 \\cr 2} \\pmatrix{\r\n\t0 \\quad .6 \\quad .4 \\cr\r\n\t0 \\quad .6 \\quad .4 \\cr\r\n\t0 \\quad .3 \\quad .7  }\r\n$$\r\nIf $n$ is very large, what is the approximate distribution of $X_n$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$P(X_{n} = 1) = \\frac{3}{7}, P(X_{n} = 2) = \\frac{4}{7}$", "choicesb": "$P(X_{n} = 1) = \\frac{4}{7}, P(X_{n} = 2) = \\frac{3}{7}$", "choicesc": "$P(X_{n} = 1) = \\frac{1}{2}, P(X_{n} = 2) = \\frac{1}{2}$", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Note that from the matrix, state 0 can only be the initial state and cannot be reached from any state. Therefore $X_{n} \\neq 0$ for any $n > 0$. In the long run, state 0 is redundant and we can discard it and the new matrix is $P'$.\r\nThen we have, ($\\pi_{1}$ $\\pi_{2}$) = ($\\pi_{1}$ $\\pi_{2}$)$P'$, i.e. $\\pi_{1} + \\pi_{2} = 1$.\r\n\\begin{align*}\r\n\\pi_{1} &= 0.6\\pi_{1} + 0.3\\pi_{2}\\\\\r\n\\pi_{2} &= 0.4\\pi_{1} + 0.7\\pi_{2}\\\\\r\n\\pi_{1} &+ \\pi_{2} = 1\r\n\\end{align*}\r\nSolving the equations, {$\\pi_{1} = \\frac{3}{7}, \\pi_{2} = \\frac{4}{7}$.}\r\nIf n is very large, the approximate distribution of $X_{n}$ is, {$P(X_{n} = 1) = \\frac{3}{7}, P(X_{n} = 2) = \\frac{4}{7}$.}", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [37, 39], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 152, "fields": {"code": "4.2.5", "category": 4, "problem": "Let $X_n$ denote the whether of day $n$, which\r\n could only be sunny (S) or cloudy (C). Suppose whether of any day\r\n depends only on the previous two days' whether in the following\r\n way:    $P(X_n=S| X_{n-1}=S, X_{n-2}=S ) =0.9 $, $P(X_n=S|\r\n X_{n-1}=S, X_{n-2}=C ) = P(X_n=S| X_{n-1}=C, X_{n-2}=S ) =0.5 $\r\n and $P(X_n=S| X_{n-1}=C, X_{n-2}=C ) =0.2 $.\r\n \r\n What is an approximate value of $P(X_{n+1}=S, X_n= C,\r\n X_{n-1}=S)$ for a very large $n$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$1/19$", "choicesb": "$2/19$", "choicesc": "$10/19$", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "theall/image/diy4.2.5.png", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Set $Y_{n} = (X_{n-1}, X_{n})$. Then $\\{Y_{n}\\}$ is a MC with state space $\\{ (S,S),(S,C),(C,S),(C,C)\\}$ and the transition probability matrix is displayed as in the picture.\r\nLet 0, 1, 2 and 3 denote $(S,S),(S,C),(C,S),(C,C)$ respectively. Then,\r\n ($\\pi_{0}$ $\\pi_{1}$ $\\pi_{2}$ $\\pi_{3}$) = ($\\pi_{0}$ $\\pi_{1}$ $\\pi_{2}$ $\\pi_{3}$)$P$, $\\pi_{0}+ \\pi_{1}+\\pi_{2} +\\pi_{3} = 1$.\r\n i.e.\r\n \\begin{align*}\r\n \\pi_{0} &= 0.9\\pi_{0} + 0.5\\pi_{2}\\\\\r\n\\pi_{1} &= 0.1 \\pi_{0} + 0.5\\pi_{2}\\\\\r\n\\pi_{2} &= 0.5\\pi_{1} + 0.2\\pi_{3}\\\\\r\n\\pi_{3} &= 0.5\\pi_{1} + 0.8\\pi_{3}\\\\\r\n\\pi_{0}&+ \\pi_{1}+\\pi_{2} + \\pi_{3}= 1\r\n \\end{align*}\r\n Solving the equations, {$\\pi_{0} = \\frac{10}{19}, \\pi_{1} = \\pi_{2} = \\frac{2}{19}, \\pi_{3} = \\frac{5}{19}$.}\r\nFor large $n$,\r\n\\begin{align*}\r\nP(X_{n+1} = S, X_{n} = C, X_{n-1} = S) &= P(X_{n+1} = S | X_{n}= C, X_{n-1} = S) P(X_{n} = C, X_{n-1} = S)\\\\\r\n&= P_{12} \\times \\pi_{1} = \\frac{1}{19}\r\n\\end{align*}", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [37, 39, 42], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 153, "fields": {"code": "4.2.7", "category": 4, "problem": "A Markov chain $\\{X_n: n \\geq 0\\}$  has two states 0 and\r\n 1 and transition probability matrix\r\n $$ \\hskip .41in 0 \\quad \\,\\,\r\n \\, 1 $$\r\n $$\r\n {\\bf P}= \\matrix{0 \\cr 1} \\pmatrix{0.2  \\quad 0.8 \\cr 0.7\\quad\r\n \t0.3}\r\n $$\r\n What is the  long run fraction of transitions from\r\n state 0 to state 1?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$28/75$", "choicesb": "$7/15$", "choicesc": "$32/75$", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "We have, ($\\pi_{0}$ $\\pi_{1}$) = ($\\pi_{0}$ $\\pi_{1}$)$P$, $\\pi_{0} + \\pi_{1} = 1$.\r\ni.e.\r\n\\begin{align*}\r\n\\pi_{0} &= 0.2\\pi_{0} + 0.7\\pi_{1}\\\\\r\n\\pi_{1} &= 0.8\\pi_{0} + 0.3\\pi_{1}\\\\\r\n\\pi_{0} &+ \\pi_{1} = 1\r\n\\end{align*}\r\n Solving the equations, {$\\pi_{0} = \\frac{7}{15}, \\pi_{1}= \\frac{8}{15}, \\pi_{0} \\times P_{01} = \\frac{7}{15} \\times 0.8 = \\frac{28}{75}$}\r\n The long run fraction of transitions from state 0 to 1 is $\\frac{28}{75}$.", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 154, "fields": {"code": "4.3.5", "category": 4, "problem": "A Markov chain has transition probability matrix\r\n $$ \\hskip .4in 0 \\quad \\, 1 \\quad \\,\\,2 $$\r\n $${\\bf P}= \\matrix{0 \\cr 1 \\cr 2} \\pmatrix{\r\n \t.5 \\quad .2 \\quad .3 \\cr 0 \\quad .5 \\quad .5 \\cr 0 \\quad .5 \\quad\r\n \t.5  }\r\n $$\r\n Point out which of the three states are recurrent and\r\n which are transient.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "All are recurrent.", "choicesb": "Only state 1 and 2 are recurrent and state 0 is transient.", "choicesc": "Only state 0 is recurrent and state 1 and 2 are transient.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Note that $\\Sigma_{n=1}^{\\infty} P_{00}^{(n)} < 1$, thus state 0 is transient.\r\nNote that state 1 communicates with state 2 and the MC will return to visit state 1 or 2 infinite number of times.\r\nThus State 1 and 2 are recurrent.", "linkability1": 2.0, "linkability2": 0.0, "linkability3": 1.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [56, 58], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 155, "fields": {"code": "4.1.2", "category": 3, "problem": "Five balls are distributed between two urns, labeled A and B. Each period, one of the five balls is selected at random, and whichever urn it\u2019s in, it is moved to the other urn. In the long run, what fraction of time is urn A empty?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. 1/32", "choicesb": "B. 5/32", "choicesc": "C. 5/16", "choicesd": "D. 1/2", "choicese": "E. 1/4", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $X_n$ be the number of balls in urn A at the end of period $n$. Then, $\\{X_n\\}$ is a Markov chain with state $ \\{0, 1, 2, 3, 4, 5\\} $. <br> <br>\r\n\r\nThe long-run fraction of time that urn A contain $i$ balls is defined as $\\pi_i$, expressed in the following matrix equation for $i = 0, 1, 2, 3, 4, 5$: \r\n\\begin{align*}\r\n\\left[\\pi_0 \\ \\pi_1 \\ \\pi_2 \\ \\pi_3 \\ \\pi_4 \\ \\pi_5 \\right] = \\mathbf{\\pi}\r\n&= \\mathbf{\\pi} \\cdot \\mathbf{P} = \\mathbf{\\pi} \\cdot \\begin{bmatrix}\r\n0 & 1 & 0 & 0 & 0 & 0\\\\\r\n1/5 & 0 & 4/5 & 0 & 0 & 0\\\\\r\n0 & 2/5 & 0 & 3/5 & 0 & 0\\\\\r\n0 & 0 & 3/5 & 0 & 2/5 & 0\\\\\r\n0 & 0 & 0 & 4/5 & 0 & 1/5\\\\\r\n0 & 0 & 0 & 0 & 1 & 0\\\\\r\n\\end{bmatrix} \\\\\r\n\\mathbf{\\pi}^T\r\n&= \\mathbf{P}^T \\cdot \\mathbf{\\pi}^T \\\\\r\n\\left(\\mathbf{P}^T - \\mathbf{I}_3 \\right) \\cdot \\mathbf{\\pi}^T \r\n&= \\mathbf{0} \\hspace{0.5cm} \r\n\\end{align*}\r\ntogether with $\\pi_0 + \\pi_1 + \\pi_2 + \\pi_3 + \\pi_4 + \\pi_5 = 1$ to solve the solution of $\\pi$, we have:\r\n\\begin{align*}\r\n\\left[\r\n\\begin{array}{c}\r\n\\begin{array}{ccc}\r\n& & \\\\\r\n& \\mathbf{P}^T - \\mathbf{I}_6 & \\\\\r\n& & \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{cccccc}\r\n\\hline\r\n1 & 1 & 1 & 1 & 1 & 1\r\n\\end{array}\r\n\\end{array} \\right] \\cdot  \\mathbf{\\pi}^T \r\n&= \\left[ \\begin{array}{c}\r\n0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 1\\\\\r\n\\end{array} \\right]\r\n\\end{align*}\r\n\r\nwhere $\\mathbf{I}_6$ is the 6 by 6 identity matrix. <br> <br>\r\nLet \r\n\\[\r\n\\mathbf{A} =\r\n\\left[\r\n\\begin{array}{c}\r\n\\begin{array}{ccc}\r\n& & \\\\\r\n& \\mathbf{P}^T - \\mathbf{I}_6 & \\\\\r\n& & \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{cccccc}\r\n\\hline\r\n1 & 1 & 1 & 1 & 1 & 1\r\n\\end{array}\r\n\\end{array} \\right]\r\n\\ , \\\r\n\\mathbf{b} = \\left[ \\begin{array}{c}\r\n0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 1\\\\\r\n\\end{array} \\right]\r\n\\] \\\\\r\nBy least square method, the optimal solution of $\\pi$ is given as:\r\n\\begin{align*}\r\n\\therefore \\ \\mathbf{A} \\mathbf{\\pi}^T &= \\mathbf{b}\\\\\r\n\\mathbf{\\pi}^T &= \\left(\\mathbf{A}^T \\mathbf{A} \\right)^{-1} \\mathbf{A}^T \\mathbf{b} \\\r\n\\end{align*}\r\nFinally, we have:\r\n\\[\r\n\\underline{\\pi_0 = \\frac{1}{32}} \\ , \\ \\pi_1 = \\frac{5}{32} \\ , \\ \\pi_2 = \\frac{5}{16} \\ , \\ \\pi_3 = \\frac{5}{16} \\ , \\ \\pi_4 = \\frac{5}{32} \\ \\ \\text{and} \\ \\ \\pi_5 = \\frac{1}{32}.\r\n\\] \r\nand the desired long run fraction is $\\underline{\\pi_0 = 1/32 \\approx 0.0312}$.", "linkability1": 2.0, "linkability2": 3.0, "linkability3": 1.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 156, "fields": {"code": "4.1.5", "category": 3, "problem": "The four towns A,B,C, and D are connected by railroad lines as shown in the following diagram (Figure. 4.1.5). <br>\r\nEach day, in whichever town it is in, a train chooses one of the lines out of that town at random and traverses it to the next town, where the process repeats on the next day. In the long run, what is the probability of finding the train in town D?", "problempicture1": "theall/image/4.1.5_WnCN0JZ.png", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. 1/8", "choicesb": "B. 1/4", "choicesc": "C. 3/8", "choicesd": "D. 5/8", "choicese": "E. 3/4", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $X_n$ be the town that the train stops at the end of day $n$. Then, $\\{X_n\\}$ is a Markov chain with state $ \\{A, B, C, D\\} = \\{0, 1, 2, 3\\} $. <br> <br>\r\nThe long-run fraction of time that urn A contain $i$ balls is defined as $\\pi_i$, expressed in the following matrix equation for $i = 0, 1, 2, 3$: \r\n\\begin{align*}\r\n\\left[\\pi_0 \\ \\pi_1 \\ \\pi_2 \\ \\pi_3 \\ \\right] = \\mathbf{\\pi}\r\n&= \\mathbf{\\pi} \\cdot \\mathbf{P} = \\mathbf{\\pi} \\cdot \\begin{bmatrix}\r\n0 & 1/2 & 0 & 1/2 \\\\\r\n1/3 & 0 & 1/3 & 1/3 \\\\\r\n0 & 1 & 0 & 0\\\\\r\n1/2 & 1/2 & 0 & 0 \\\\\r\n\\end{bmatrix} \\\\\r\n\\mathbf{\\pi}^T\r\n&= \\mathbf{P}^T \\cdot \\mathbf{\\pi}^T \\\\\r\n\\left(\\mathbf{P}^T - \\mathbf{I}_4 \\right) \\cdot \\mathbf{\\pi}^T \r\n&= \\mathbf{0} \\hspace{0.5cm} \r\n\\end{align*}\r\ntogether with $\\pi_0 + \\pi_1 + \\pi_2 + \\pi_3 = 1$ to solve the solution of $\\pi$, we have:\r\n\\begin{align*}\r\n\\left[\r\n\\begin{array}{c}\r\n\\begin{array}{ccc}\r\n& & \\\\\r\n& \\mathbf{P}^T - \\mathbf{I}_4 & \\\\\r\n& & \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{cccc}\r\n\\hline\r\n1 & 1 & 1 & 1 \r\n\\end{array}\r\n\\end{array} \\right] \\cdot  \\mathbf{\\pi}^T \r\n&= \\left[ \\begin{array}{c}\r\n0\\\\ 0\\\\ 0\\\\ 0\\\\ 1\\\\\r\n\\end{array} \\right]\r\n\\end{align*}\r\n\r\nwhere $\\mathbf{I}_4$ is the 4 by 4 identity matrix. <br> <br>\r\nLet \r\n\\[\r\n\\mathbf{A} =\r\n\\left[\r\n\\begin{array}{c}\r\n\\begin{array}{ccc}\r\n& & \\\\\r\n& \\mathbf{P}^T - \\mathbf{I}_4 & \\\\\r\n& & \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{cccc}\r\n\\hline\r\n1 & 1 & 1 & 1 \r\n\\end{array}\r\n\\end{array} \\right]\r\n\\ , \\\r\n\\mathbf{b} = \\left[ \\begin{array}{c}\r\n0\\\\ 0\\\\ 0\\\\ 0\\\\ 1\\\\\r\n\\end{array} \\right]\r\n\\] \r\nBy least square method, the optimal solution of $\\pi$ is given as:\r\n\\begin{align*}\r\n\\therefore \\ \\mathbf{A} \\mathbf{\\pi}^T &= \\mathbf{b}\\\\\r\n\\mathbf{\\pi}^T &= \\left(\\mathbf{A}^T \\mathbf{A} \\right)^{-1} \\mathbf{A}^T \\mathbf{b} \\\r\n\\end{align*}\r\nFinally, we have:\r\n\\[\r\n\\pi_0 = \\frac{1}{4} \\ , \\ \\pi_1 = \\frac{3}{8} \\ , \\ \\pi_2 = \\frac{1}{8} \\ \\text{and} \\ \\underline{\\pi_3 = \\frac{1}{4}}.\r\n\\]\r\nand the desired long run fraction is $\\underline{\\pi_3 = 1/4 = 0.25}$.", "linkability1": 2.0, "linkability2": 3.0, "linkability3": 1.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 157, "fields": {"code": "4.1.8", "category": 3, "problem": "A transition probability matrix is given as\r\n \\[\r\n \\mathbf{P} = \r\n \\begin{array}{l}\r\n \\begin{array}{llllll}\r\n \\hspace{0.125cm} &   \\ \\ \\ 0 \\ & \\hspace{0.125cm} \\ \\ 1 \\ \\  & \\hspace{0.1cm}  \\  2 \\ \\ & \\hspace{0.1cm}  \\   3 \\ \\ \\ & \\hspace{0.1cm}   4 \\  \\ \\ \\ \\\\\r\n \\end{array}\\\\\r\n \\begin{array}{c}\r\n 0\\\\\r\n 1\\\\\r\n 2\\\\\r\n 3\\\\\r\n 4\\\\\r\n \\end{array}\r\n \\begin{Vmatrix}\r\n 0    & 1/2\t& 1/2 \t& 0 \t& 0 \t    \\\\ \r\n 1/2  & 0   & 1/2 \t& 0 \t& 0 \t     \\\\ \r\n 1/3  & 1/3 & 0 \t& 1/3 \t& 0 \t  \\\\ \r\n 0\t  & 0 \t& 1/2 \t& 0 \t& 1/2 \t  \\\\ \r\n 1/2  & 0 \t& 0 \t& 1/2 \t& 0 \t \\\\ \r\n \\end{Vmatrix}\r\n \\end{array}\r\n \\] \r\n Compute the limiting distribution.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. $\\pi_0 = \\frac{5}{38} \\ , \\ \\pi_1 = \\frac{3}{38} \\ , \\ \\pi_2 = \\frac{9}{19} \\ , \\ \\pi_3 = \\frac{4}{19} \\ \\text{and} \\ \\pi_4 = \\frac{2}{19}$", "choicesb": "B. $\\pi_0 = \\frac{18}{43} \\ , \\ \\pi_1 = \\frac{16}{43} \\ , \\ \\pi_2 = \\frac{9}{86} \\ , \\ \\pi_3 = \\frac{5}{86} \\ \\text{and} \\ \\pi_4 = \\frac{2}{43}$", "choicesc": "C. $\\pi_0 = \\frac{22}{87} \\ , \\ \\pi_1 = \\frac{20}{87} \\ , \\ \\pi_2 = \\frac{9}{29} \\ , \\ \\pi_3 = \\frac{4}{29} \\ \\text{and} \\ \\pi_4 = \\frac{2}{29}$", "choicesd": "D. $\\pi_0 = \\frac{22}{73} \\ , \\ \\pi_1 = \\frac{20}{73} \\ , \\ \\pi_2 = \\frac{16}{73} \\ , \\ \\pi_3 = \\frac{9}{73} \\ \\text{and} \\ \\pi_4 = \\frac{6}{73}$", "choicese": "E. $\\pi_0 = \\frac{22}{69} \\ , \\ \\pi_1 = \\frac{20}{69} \\ , \\ \\pi_2 = \\frac{5}{23} \\ , \\ \\pi_3 = \\frac{3}{23} \\ \\text{and} \\ \\pi_4 = \\frac{1}{23}$", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The long-run fraction of time that urn A contain $i$ balls is defined as $\\pi_i$, expressed in the following matrix equation for $i = 0, 1, 2, 3, 4$: \r\n\\begin{align*}\r\n\\left[\\pi_0 \\ \\pi_1 \\ \\pi_2 \\ \\pi_3 \\ \\pi_4 \\right] = \\mathbf{\\pi}\r\n&= \\mathbf{\\pi} \\cdot \\mathbf{P} = \\mathbf{\\pi} \\cdot \\begin{bmatrix}\r\n 0    & 1/2\t& 1/2 \t& 0 \t& 0 \t    \\\\ \r\n1/2  & 0   & 1/2 \t& 0 \t& 0 \t     \\\\ \r\n1/3  & 1/3 & 0 \t& 1/3 \t& 0 \t  \\\\ \r\n0\t  & 0 \t& 1/2 \t& 0 \t& 1/2 \t  \\\\ \r\n1/2  & 0 \t& 0 \t& 1/2 \t& 0 \t \\\\ \r\n\\end{bmatrix} \\\\\r\n\\mathbf{\\pi}^T\r\n&= \\mathbf{P}^T \\cdot \\mathbf{\\pi}^T \\\\\r\n\\left(\\mathbf{P}^T - \\mathbf{I}_5 \\right) \\cdot \\mathbf{\\pi}^T \r\n&= \\mathbf{0} \\hspace{0.5cm} \r\n\\end{align*}\r\ntogether with $\\pi_0 + \\pi_1 + \\pi_2 + \\pi_3 + \\pi_4 = 1$ to solve the solution of $\\pi$, we have:\r\n\\begin{align*}\r\n\\left[\r\n\\begin{array}{c}\r\n\\begin{array}{ccc}\r\n& & \\\\\r\n& \\mathbf{P}^T - \\mathbf{I}_5 & \\\\\r\n& & \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{ccccc}\r\n\\hline\r\n1 & 1 & 1 & 1 & 1\r\n\\end{array}\r\n\\end{array} \\right] \\cdot  \\mathbf{\\pi}^T \r\n&= \\left[ \\begin{array}{c}\r\n0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 1\\\\\r\n\\end{array} \\right]\r\n\\end{align*}\r\n\r\nwhere $\\mathbf{I}_5$ is the 5 by 5 identity matrix. <br> <br>\r\nLet \r\n\\[\r\n\\mathbf{A} =\r\n\\left[\r\n\\begin{array}{c}\r\n\\begin{array}{ccc}\r\n& & \\\\\r\n& \\mathbf{P}^T - \\mathbf{I}_5 & \\\\\r\n& & \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{ccccc}\r\n\\hline\r\n1 & 1 & 1 & 1 & 1\r\n\\end{array}\r\n\\end{array} \\right]\r\n\\ , \\\r\n\\mathbf{b} = \\left[ \\begin{array}{c}\r\n0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 1\\\\\r\n\\end{array} \\right]\r\n\\] \r\nBy least square method, the optimal solution of $\\pi$ is given as:\r\n\\begin{align*}\r\n\\therefore \\ \\mathbf{A} \\mathbf{\\pi}^T &= \\mathbf{b}\\\\\r\n\\mathbf{\\pi}^T &= \\left(\\mathbf{A}^T \\mathbf{A} \\right)^{-1} \\mathbf{A}^T \\mathbf{b} \\\r\n\\end{align*}\r\nFinally, we have:\r\n\\[\r\n\\pi_0 = \\frac{22}{87} \\ , \\ \\pi_1 = \\frac{20}{87} \\ , \\ \\pi_2 = \\frac{9}{29} \\ , \\ \\pi_3 = \\frac{4}{29} \\ \\text{and} \\ \\pi_4 = \\frac{2}{29}.\r\n\\]\r\nis the desired limiting distribution.", "linkability1": 2.0, "linkability2": 3.0, "linkability3": 1.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 158, "fields": {"code": "4.1.11", "category": 3, "problem": "Suppose that a production process changes state according to a Markov process whose transition probability matrix is given by\r\n\\[\r\n\\mathbf{P} = \r\n\\begin{array}{l}\r\n\\begin{array}{lllll}\r\n\\hspace{0.075cm} &   \\ \\ \\ 0 \\ & \\hspace{0.15cm} \\ 1 \\ \\  & \\hspace{0.1cm}   2 \\ \\ & \\hspace{0.125cm}    \r\n 3 \\ \\ \\ \\  \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n3\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n0.3    & 0.5\t& 0 \t& 0.2 \\\\ \r\n0.5  & 0.2   & 0.2 \t& 0.1 \\\\ \r\n0.2  & 0.3 & 0.4 \t& 0.1\t\\\\ \r\n0.1\t  & 0.2 \t& 0.4 \t& 0.3  \\\\ \r\n\\end{Vmatrix}\r\n\\end{array}\r\n\\] <br>\r\n(a) Suppose that states 0 and 1 are \u201cIn-Control\u201d while states 2 and 3 are deemed \u201cOut-of-Control.\u201d In the long run, what fraction of time is the process Out-of-Control? <br>\r\n(b) In the long run, what fraction of transitions are from an In-Control state to an Out-of-Control state?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. (a). 81/379 (b). 81/379", "choicesb": "B. (a). 62/379 (b). 62/758", "choicesc": "C. (a). 81/379 (b). 81/758", "choicesd": "D. (a). 143/379 (b). 143/758", "choicese": "E. (a). 143/379 (b). 143/379", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "E", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "(a). <br>\r\nThe long-run fraction of time that urn A contain $i$ balls is defined as $\\pi_i$, expressed in the following matrix equation for $i = 0, 1, 2, 3$: \r\n\\begin{align*}\r\n\\left[\\pi_0 \\ \\pi_1 \\ \\pi_2 \\ \\pi_3 \\ \\right] = \\mathbf{\\pi}\r\n&= \\mathbf{\\pi} \\cdot \\mathbf{P} = \\mathbf{\\pi} \\cdot \\begin{bmatrix}\r\n0.3    & 0.5\t& 0 \t& 0.2 \\\\ \r\n0.5  & 0.2   & 0.2 \t& 0.1 \\\\ \r\n0.2  & 0.3 & 0.4 \t& 0.1\t\\\\ \r\n0.1\t  & 0.2 \t& 0.4 \t& 0.3  \\\\ \r\n\\end{bmatrix} \\\\\r\n\\mathbf{\\pi}^T\r\n&= \\mathbf{P}^T \\cdot \\mathbf{\\pi}^T \\\\\r\n\\left(\\mathbf{P}^T - \\mathbf{I}_4 \\right) \\cdot \\mathbf{\\pi}^T \r\n&= \\mathbf{0} \\hspace{0.5cm} \r\n\\end{align*}\r\ntogether with $\\pi_0 + \\pi_1 + \\pi_2 + \\pi_3 = 1$ to solve the solution of $\\pi$, we have:\r\n\\begin{align*}\r\n\\left[\r\n\\begin{array}{c}\r\n\\begin{array}{ccc}\r\n& & \\\\\r\n& \\mathbf{P}^T - \\mathbf{I}_4 & \\\\\r\n& & \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{cccc}\r\n\\hline\r\n1 & 1 & 1 & 1 \r\n\\end{array}\r\n\\end{array} \\right] \\cdot  \\mathbf{\\pi}^T \r\n&= \\left[ \\begin{array}{c}\r\n0\\\\ 0\\\\ 0\\\\ 0\\\\ 1\\\\\r\n\\end{array} \\right]\r\n\\end{align*}\r\n\r\nwhere $\\mathbf{I}_4$ is the 4 by 4 identity matrix. <br> <br>\r\nLet \r\n\\[\r\n\\mathbf{A} =\r\n\\left[\r\n\\begin{array}{c}\r\n\\begin{array}{ccc}\r\n& & \\\\\r\n& \\mathbf{P}^T - \\mathbf{I}_4 & \\\\\r\n& & \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{cccc}\r\n\\hline\r\n1 & 1 & 1 & 1 \r\n\\end{array}\r\n\\end{array} \\right]\r\n\\ , \\\r\n\\mathbf{b} = \\left[ \\begin{array}{c}\r\n0\\\\ 0\\\\ 0\\\\ 0\\\\ 1\\\\\r\n\\end{array} \\right]\r\n\\] \r\nBy least square method, the optimal solution of $\\pi$ is given as:\r\n\\begin{align*}\r\n\\therefore \\ \\mathbf{A} \\mathbf{\\pi}^T &= \\mathbf{b}\\\\\r\n\\mathbf{\\pi}^T &= \\left(\\mathbf{A}^T \\mathbf{A} \\right)^{-1} \\mathbf{A}^T \\mathbf{b} \\\r\n\\end{align*}\r\nFinally, we have:\r\n\\[\r\n\\pi_0 = \\frac{117}{379} \\ , \\ \\pi_1 = \\frac{119}{379} \\ , \\ \\pi_2 = \\frac{81}{379} \\ \\text{and} \\ \\pi_3 = \\frac{62}{379}.\r\n\\]\r\nand the desired long run fraction is $\\underline{\\pi_2 + \\pi_3 = 81/379 + 62/379 = 143/379 \\approx 0.3773}$. <br> \r\n<br>\r\n(b). <br>\r\n The fraction of transitions are from an In-Control state to an Out-of-Control state is:\r\n\\begin{align*}\r\n&\\lim\\limits_{n \\longrightarrow \\infty} P(X_{n+1} = 2 \\cup X_{n+1} = 3 \\ | \\ X_n = 0 \\cup X_n = 1)\\\\\r\n&= \\lim\\limits_{n \\longrightarrow \\infty} P(X_{n+1} = 2 \\cup X_{n+1} = 3 )\\\\\r\n&= \\pi_2 + \\pi_3 \\\\\r\n&= 81/379 + 62/379 = \\underline{143/379}\r\n\\end{align*}", "linkability1": 2.0, "linkability2": 3.0, "linkability3": 1.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 159, "fields": {"code": "4.2.1", "category": 3, "problem": "Consider a discrete-time periodic review inventory model (see Chapter 3, Section 3.3.1),and let $ n $ be the total demand in period $ n $. Let $X_n$ be the inventory quantity on hand at the end of period n. Instead of following an ($s,S$) policy, a ($q,Q$) policy will be used: If the stock level at the end of a period is less than or equal to $q = 2$ units, then $Q = 2$ additional units will be ordered and will be available at the beginning of the next period. Otherwise, no ordering will take place. This is a ($q,Q$) policy with $q = 2$ and $Q = 2$. Assume that demand that is not filled in a period is lost (no back ordering). <br>\r\n\r\nSuppose that $\\xi_1, \\xi_2, ...$ are independent random variables, each having the probability distribution where\r\n\\[\r\n\\begin{array}{ccccccc}\r\n\\hline \r\nk & = & 0 & 1 & 2 & 3 & 4 \\\\\r\n\\Pr\\{\\xi = k \\} & = & 0.1 & 0.3 & 0.3 & 0.2 & 0.1 \\\\\r\n\\hline\r\n\\end{array}\r\n\\]\r\nThen, $X_0, X_1, ...$ is a Markov chain.  In the long run, during what fraction of periods are orders placed?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. 3/8", "choicesb": "B. 189/680", "choicesc": "C. 15/68", "choicesd": "D. 43/340", "choicese": "E. 297/340", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "E", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The relationship between $X_{n+1}, X_{n}$ and $\\xi_{n+1}$ according to the problem description is given as\r\n\r\n\\[\r\nX_{n+1} =\r\n\\begin{cases}\r\nX_n + 2 - \\xi_{n+1} & \\ \\ \\text{if} \\ \\ X_n \\leq 2 \\\\\r\nX_n - \\xi_{n+1} & \\ \\ \\text{if} \\ \\ X_n > 2 \\\\\r\n0 & \\ \\ \\text{if} \\ \\ X_n < 0 \\\\\r\n\\end{cases}\r\n\\] \r\n\r\nTherefore, the transition probability is given as:\r\n\\[\r\n\\mathbf{P} = \r\n\\begin{array}{l}\r\n\t\\begin{array}{llllll}\r\n\t\t\\hspace{0.125cm} &   \\ \\ 0 \\ & \\hspace{0.15cm} \\ 1 \\ \\  & \\hspace{0.125cm}  2 \\ \\ & \\hspace{0.125cm}   3 \\ \\  & \\hspace{0.125cm}  4 \\  \\ \\ \\ \\\\\r\n\t\\end{array}\\\\\r\n\t\\begin{array}{c}\r\n\t\t0\\\\\r\n\t\t1\\\\\r\n\t\t2\\\\\r\n\t\t3\\\\\r\n\t\t4\\\\\r\n\t\\end{array}\r\n\t\\begin{Vmatrix}\r\n\t\t0.6  & 0.3\t& 0.1 \t& 0 \t& 0 \t    \\\\ \r\n\t\t0.3  & 0.3  & 0.3 \t& 0.1 \t& 0 \t \\\\ \r\n\t\t0.1  & 0.2  & 0.3 \t& 0.3 \t& 0.1 \t  \\\\ \r\n\t\t0.3  & 0.3  & 0.3 \t& 0.1 \t& 0 \t \\\\ \r\n\t\t0.6  & 0.3\t& 0.1 \t& 0 \t& 0 \t    \\\\ \r\n\t\\end{Vmatrix}\r\n\\end{array}\r\n\\]\r\n\r\nThe long-run fraction of time that urn A contain $i$ balls is defined as $\\pi_i$, expressed in the following matrix equation for $i = 0, 1, 2, 3, 4$:\r\n\r\n\\begin{align*}\r\n\\left[\\pi_0 \\ \\pi_1 \\ \\pi_2 \\ \\pi_3 \\ \\pi_4 \\right] = \\mathbf{\\pi}\r\n&= \\mathbf{\\pi} \\cdot \\mathbf{P} = \\mathbf{\\pi} \\cdot \\begin{bmatrix}\r\n0.6  & 0.3\t& 0.1 \t& 0 \t& 0 \t    \\\\ \r\n0.3  & 0.3  & 0.3 \t& 0.1 \t& 0 \t \\\\ \r\n0.1  & 0.2  & 0.3 \t& 0.3 \t& 0.1 \t  \\\\ \r\n0.3  & 0.3  & 0.3 \t& 0.1 \t& 0 \t \\\\ \r\n0.6  & 0.3\t& 0.1 \t& 0 \t& 0 \t    \\\\ \r\n\\end{bmatrix} \\\\\r\n\\mathbf{\\pi}^T\r\n&= \\mathbf{P}^T \\cdot \\mathbf{\\pi}^T \\\\\r\n\\left(\\mathbf{P}^T - \\mathbf{I}_5 \\right) \\cdot \\mathbf{\\pi}^T \r\n&= \\mathbf{0} \\hspace{0.5cm} \r\n\\end{align*}\r\ntogether with $\\pi_0 + \\pi_1 + \\pi_2 + \\pi_3 + \\pi_4 = 1$ to solve the solution of $\\pi$, we have:\r\n\\begin{align*}\r\n\\left[\r\n\\begin{array}{c}\r\n\\begin{array}{ccc}\r\n& & \\\\\r\n& \\mathbf{P}^T - \\mathbf{I}_5 & \\\\\r\n& & \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{ccccc}\r\n\\hline\r\n1 & 1 & 1 & 1 & 1\r\n\\end{array}\r\n\\end{array} \\right] \\cdot  \\mathbf{\\pi}^T \r\n&= \\left[ \\begin{array}{c}\r\n0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 1\\\\\r\n\\end{array} \\right]\r\n\\end{align*}\r\n\r\nwhere $\\mathbf{I}_5$ is the 5 by 5 identity matrix. <br> <br>\r\nLet \r\n\\[\r\n\\mathbf{A} =\r\n\\left[\r\n\\begin{array}{c}\r\n\\begin{array}{ccc}\r\n& & \\\\\r\n& \\mathbf{P}^T - \\mathbf{I}_5 & \\\\\r\n& & \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{ccccc}\r\n\\hline\r\n1 & 1 & 1 & 1 & 1\r\n\\end{array}\r\n\\end{array} \\right]\r\n\\ , \\\r\n\\mathbf{b} = \\left[ \\begin{array}{c}\r\n0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 1\\\\\r\n\\end{array} \\right]\r\n\\] \r\nBy least square method, the optimal solution of $\\pi$ is given as:\r\n\\begin{align*}\r\n\\therefore \\ \\mathbf{A} \\mathbf{\\pi}^T &= \\mathbf{b}\\\\\r\n\\mathbf{\\pi}^T &= \\left(\\mathbf{A}^T \\mathbf{A} \\right)^{-1} \\mathbf{A}^T \\mathbf{b} \\\r\n\\end{align*}\r\nFinally, we have:\r\n\\[\r\n\\pi_0 = \\frac{3}{8} \\ , \\ \\pi_1 = \\frac{189}{680} \\ , \\ \\pi_2 = \\frac{15}{68} \\ , \\ \\pi_3 = \\frac{71}{680} \\ \\text{and} \\ \\pi_4 = \\frac{3}{136}.\r\n\\]\r\nand the desired long run fraction is $\\underline{\\pi_0 + \\pi_1 + \\pi_2 = 3/8 + 189/680 + 15/68 = 297/340 \\approx 0.8735}$.", "linkability1": 2.0, "linkability2": 3.0, "linkability3": 3.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 160, "fields": {"code": "4.2.4", "category": 3, "problem": "A component of a computer has an active life, measured in discrete units, that is a random variable $\\xi$, where <br>\r\n\\[\r\n\\begin{array}{cccccc}\r\n\\hline \r\nk & =  & 1 & 2 & 3 & 4 \\\\\r\n\\Pr\\{\\xi = k \\} & = & 0.1 & 0.3 & 0.2 & 0.4 \\\\\r\n\\hline\r\n\\end{array}\r\n\\] <br>\r\nSuppose that one starts with a fresh component, and each component is replaced by a new component upon failure. Let $X_n$ be the remaining life of the component in service at the end of period $n$. When $X_n = 0$, a new item is placed into service at the start of the next period. <br> <br>\r\n(a). Determine the long run probability that the item in service at the end of a period has no remaining life and therefore will be replaced.<br>\r\n(b). Determine the mean life of a component.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. (a). 10/39; (b). 62/29", "choicesb": "B. (a). 10/39; (b). 62/39", "choicesc": "C. (a). 29/39; (b). 62/29", "choicesd": "D. (a). 29/39; (b). 62/39", "choicese": "E. (a). 4/39; (b). 62/29", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The transition probability is given as:\r\n\\[\r\n\\mathbf{P} = \r\n\\begin{array}{l}\r\n\\begin{array}{llllll}\r\n\\hspace{0.125cm} & \\    0 \\ & \\hspace{0.1cm}  1 \\ \\  & \\hspace{0.075cm}   2 \\ \\ & \\hspace{0.125cm}   3 \\ \\ & \\hspace{0.125cm}  4 \\  \\ \\ \\ \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n3\\\\\r\n4\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n0\t& 0.1\t& 0.3 \t& 0.2 \t& 0.4 \t    \\\\ \r\n1   & 0\t\t& 0 \t& 0 \t& 0 \t \\\\ \r\n0   & 1 \t& 0 \t& 0 \t& 0 \t  \\\\ \r\n0   & 0  \t& 1 \t& 0 \t& 0 \t \\\\ \r\n0   & 0\t\t& 0 \t& 1 \t& 0 \t    \\\\ \r\n\\end{Vmatrix}\r\n\\end{array}\r\n\\]\r\n\r\nThe long-run fraction of time that urn A contain $i$ balls is defined as $\\pi_i$, expressed in the following matrix equation for $i = 0, 1, 2, 3, 4$:\r\n\r\n\\begin{align*}\r\n\\left[\\pi_0 \\ \\pi_1 \\ \\pi_2 \\ \\pi_3 \\ \\pi_4 \\right] = \\mathbf{\\pi}\r\n&= \\mathbf{\\pi} \\cdot \\mathbf{P} = \\mathbf{\\pi} \\cdot \\begin{bmatrix}\r\n0\t& 0.1\t& 0.3 \t& 0.2 \t& 0.4 \t    \\\\ \r\n1   & 0\t\t& 0 \t& 0 \t& 0 \t \\\\ \r\n0   & 1 \t& 0 \t& 0 \t& 0 \t  \\\\ \r\n0   & 0  \t& 1 \t& 0 \t& 0 \t \\\\ \r\n0   & 0\t\t& 0 \t& 1 \t& 0 \t    \\\\ \r\n\\end{bmatrix} \\\\\r\n\\mathbf{\\pi}^T\r\n&= \\mathbf{P}^T \\cdot \\mathbf{\\pi}^T \\\\\r\n\\left(\\mathbf{P}^T - \\mathbf{I}_5 \\right) \\cdot \\mathbf{\\pi}^T \r\n&= \\mathbf{0} \\hspace{0.5cm} \r\n\\end{align*}\r\ntogether with $\\pi_0 + \\pi_1 + \\pi_2 + \\pi_3 + \\pi_4 = 1$ to solve the solution of $\\pi$, we have:\r\n\\begin{align*}\r\n\\left[\r\n\\begin{array}{c}\r\n\\begin{array}{ccc}\r\n& & \\\\\r\n& \\mathbf{P}^T - \\mathbf{I}_5 & \\\\\r\n& & \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{ccccc}\r\n\\hline\r\n1 & 1 & 1 & 1 & 1\r\n\\end{array}\r\n\\end{array} \\right] \\cdot  \\mathbf{\\pi}^T \r\n&= \\left[ \\begin{array}{c}\r\n0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 1\\\\\r\n\\end{array} \\right]\r\n\\end{align*}\r\n\r\nwhere $\\mathbf{I}_5$ is the 5 by 5 identity matrix. <br> <br>\r\nLet \r\n\\[\r\n\\mathbf{A} =\r\n\\left[\r\n\\begin{array}{c}\r\n\\begin{array}{ccc}\r\n& & \\\\\r\n& \\mathbf{P}^T - \\mathbf{I}_5 & \\\\\r\n& & \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{ccccc}\r\n\\hline\r\n1 & 1 & 1 & 1 & 1\r\n\\end{array}\r\n\\end{array} \\right]\r\n\\ , \\\r\n\\mathbf{b} = \\left[ \\begin{array}{c}\r\n0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 1\\\\\r\n\\end{array} \\right]\r\n\\] \r\nBy least square method, the optimal solution of $\\pi$ is given as:\r\n\\begin{align*}\r\n\\therefore \\ \\mathbf{A} \\mathbf{\\pi}^T &= \\mathbf{b}\\\\\r\n\\mathbf{\\pi}^T &= \\left(\\mathbf{A}^T \\mathbf{A} \\right)^{-1} \\mathbf{A}^T \\mathbf{b} \\\r\n\\end{align*}\r\nFinally, we have:\r\n\\[\r\n\\pi_0 = \\frac{10}{39} \\ , \\ \\pi_1 = \\frac{10}{39} \\ , \\ \\pi_2 = \\frac{3}{13} \\ , \\ \\pi_3 = \\frac{2}{13} \\ \\text{and} \\ \\pi_4 = \\frac{4}{39}.\r\n\\]\r\nand the desired long run fraction is $\\underline{\\pi_0 = 10/39 \\approx 0.2564}$. <br>\r\n\r\nThe mean life time of a component is:\r\n\r\n\\[\r\nE(\\xi) = \\sum_{k = 1}^{4} k \\cdot \\left(\\frac{\\pi_k}{\\sum_{k = 1}^{4} \\pi_k}\\right) =  \\left( \\frac{1 \\cdot \\pi_1 + 2 \\cdot \\pi_2 + 3 \\cdot \\pi_3 + 4 \\cdot \\pi_4}{\\pi_1 + \\pi_2 + \\pi_3 + \\pi_4} \\right) = \\underline{62/29} \\approx 2.1379\r\n\\]", "linkability1": 3.0, "linkability2": 3.0, "linkability3": 1.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 161, "fields": {"code": "4.2.7", "category": 3, "problem": "Customers arrive for service and take their place in a waiting line. There is a single service facility, and a customer undergoing service at the beginning of a period will complete service and depart at the end of the period with probability $\\beta$ and will continue service into the next period with probability $\\alpha = 1 - \\beta$, and then the process repeats. This description implies that the service time $\\xi$ of an individual is a random variable with the geometric distribution, <br>\r\n\\[\r\n\\Pr\\{\\xi = k \\} = \\beta \\alpha^{k-1} \\ \\ \\text{for} \\ k = 1, 2, ...,\r\n\\]\r\n<br>\r\n and the service times of distinct customers are independent random variables. <br>\r\n At most a single customer can arrive during a period. We suppose that the actual number of arrivals during the nth period is a random variable $ \\xi_n $ taking on the values 0 or 1 according to <br>\r\n \\[\r\n \\Pr\\{\\xi_n = 0 \\} = p\r\n \\]\r\n and <br>\r\n \\[\r\n \\Pr\\{\\xi_n = 1 \\} = q = 1 - p \\ \\ \\text{for} \\ n = 0, 1, ...\r\n \\]\r\n<br>\r\n  The state $ X_n $ of the system at the start of period $ n $ is defined to be the number of customers in the system, either waiting or being served. Then, $ \\{X_n \\} $ is a Markov chain. Specify the following transition probabilities in terms of $\\alpha, \\beta, p $ and $ q $ : $ P_{00},P_{01},P_{02}, P_{10}, P_{11}, $ and $ P_{12} $.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. \r\n\\[\\begin{array}{l}\r\n\\begin{array}{lllll}\r\n\\hspace{0.125cm} & \\   \\ 0 \\ & \\hspace{0.375cm} \\ \\ \\ 1 \\ \\  & \\hspace{0.425cm}  \\ \\ 2 \\ \\ & \\hspace{0.05cm}  \\cdots \\ \\ \\ \\  \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n\\vdots\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\np \\beta\t& q \\beta + \\alpha\t& 0 \t& \\cdots    \\\\ \r\np \\beta   & q \\beta + p \\alpha\t\t& q \\beta \\alpha \t& \\cdots \t \t \\\\ \r\n\\vdots   & \\vdots \t& \\vdots \t& \\cdots \t \t  \\\\ \r\n\\vdots   & \\vdots  \t& \\vdots \t& \\cdots \t \t \\\\ \r\n\\end{Vmatrix}\r\n\\end{array}\\]", "choicesb": "B. \r\n\\[\\begin{array}{l}\r\n\t\\begin{array}{lllll}\r\n\\hspace{0.125cm} & \\   \\ \\ \\ \\ 0 \\ & \\hspace{0.675cm} \\ \\ \\ 1 \\ \\  & \\hspace{0.425cm}  \\ \\ \\ 2 \\ \\ & \\hspace{0.05cm}  \\cdots \\ \\ \\ \\  \\\\\r\n\t\\end{array}\\\\\r\n\t\\begin{array}{c}\r\n\t\t0\\\\\r\n\t\t1\\\\\r\n\t\t2\\\\\r\n\t\t\\vdots\\\\\r\n\t\\end{array}\r\n\t\\begin{Vmatrix}\r\n\t\tp + q \\beta\t& q \\beta \\alpha\t& 0 \t& \\cdots    \\\\ \r\n\t\tp \\beta   & q \\beta + p \\alpha\t\t& q \\beta \\alpha \t& \\cdots \t \t \\\\ \r\n\t\t\\vdots   & \\vdots \t& \\vdots \t& \\cdots \t \t  \\\\ \r\n\t\t\\vdots   & \\vdots  \t& \\vdots \t& \\cdots \t \t \\\\ \r\n\t\\end{Vmatrix}\r\n\\end{array}\\]", "choicesc": "C. \\[\\begin{array}{l}\r\n\\begin{array}{lllll}\r\n\\hspace{0.125cm} & \\   \\ 0 \\ & \\hspace{0.375cm} \\ \\ \\ 1 \\ \\  & \\hspace{0.425cm}  \\ \\ 2 \\ \\ & \\hspace{0.05cm}  \\cdots \\ \\ \\ \\  \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n\\vdots\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n q \\beta\t& q \\alpha + p\t& 0 \t& \\cdots    \\\\ \r\np \\beta   & q \\beta + p \\alpha\t\t& q \\beta \\alpha \t& \\cdots \t \t \\\\ \r\n\\vdots   & \\vdots \t& \\vdots \t& \\cdots \t \t  \\\\ \r\n\\vdots   & \\vdots  \t& \\vdots \t& \\cdots \t \t \\\\ \r\n\\end{Vmatrix}\r\n\\end{array}\\]", "choicesd": "D. \\[\\begin{array}{l}\r\n\\begin{array}{lllll}\r\n\\hspace{0.125cm} & \\   \\ 0 \\ & \\hspace{0.375cm} \\ \\ \\ 1 \\ \\  & \\hspace{0.425cm}  \\ \\ 2 \\ \\ & \\hspace{0.05cm}  \\cdots \\ \\ \\ \\  \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n\\vdots\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\np \t& q \\beta\t& q  \\alpha \t& \\cdots    \\\\ \r\np \\beta   & q \\beta + p \\alpha\t\t& q \\beta \\alpha \t& \\cdots \t \t \\\\ \r\n\\vdots   & \\vdots \t& \\vdots \t& \\cdots \t \t  \\\\ \r\n\\vdots   & \\vdots  \t& \\vdots \t& \\cdots \t \t \\\\ \r\n\\end{Vmatrix}\r\n\\end{array}\\]", "choicese": "E. \\[\\begin{array}{l}\r\n\\begin{array}{lllll}\r\n\\hspace{0.125cm} & \\   \\ \\ \\ \\ 0 \\ & \\hspace{0.475cm} \\ \\ 1 \\ \\  & \\hspace{0.15cm} 2 \\ & \\hspace{0.05cm}  \\cdots \\ \\ \\ \\  \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n\\vdots\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\np + q \\beta\t& q \\beta \\alpha\t& 0 \t& \\cdots    \\\\ \r\np \t& q \\beta\t& q  \\alpha \t& \\cdots \t \t \\\\ \r\n\\vdots   & \\vdots \t& \\vdots \t& \\cdots \t \t  \\\\ \r\n\\vdots   & \\vdots  \t& \\vdots \t& \\cdots \t \t \\\\ \r\n\\end{Vmatrix}\r\n\\end{array}\\]\\\\", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Assume $\\xi_1, \\xi_2, ...$ are independent identically distributed (iid) random variables, the transition probability matrix is given as:\r\n\r\n\\begin{align*}\r\n\\mathbf{P} \r\n&= \r\n\\begin{array}{l}\r\n\\begin{array}{lllll}\r\n\\hspace{0.125cm} &   \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ 0 \\ & \\hspace{1.75cm} \\ \\ \\ \\ \\ \\ \\ \\ \\ 1 \\ \\  & \\hspace{2.025cm}  \\ \\ \\ \\ \\ \\ \\ \\ 2 \\ \\ & \\hspace{0.725cm}   \\ \\ \\ \\cdots \\ \\ \\ \\  \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n\\vdots\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\np + (1-p) \\beta\t& (1-p) \\beta (1 - \\beta)\t& 0 \t& \\cdots    \\\\ \r\np \\beta   & (1-p) \\beta + p (1 - \\beta)\t\t& (1-p)\\beta(1-\\beta) \t& \\cdots \t \t \\\\ \r\n\\vdots   & \\vdots \t& \\vdots \t& \\cdots \t \t  \\\\ \r\n\\vdots   & \\vdots  \t& \\vdots \t& \\cdots \t \t \\\\ \r\n\\end{Vmatrix}\r\n\\end{array} \\\\\r\n&= \r\n\\begin{array}{l}\r\n\\begin{array}{lllll}\r\n\\hspace{0.125cm} & \\   \\ \\ \\ \\ 0 \\ & \\hspace{0.675cm} \\ \\ \\ 1 \\ \\  & \\hspace{0.425cm}  \\ \\ \\ 2 \\ \\ & \\hspace{0.05cm}  \\cdots \\ \\ \\ \\  \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n\\vdots\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\np + q \\beta\t& q \\beta \\alpha\t& 0 \t& \\cdots    \\\\ \r\np \\beta   & q \\beta + p \\alpha\t\t& q \\beta \\alpha \t& \\cdots \t \t \\\\ \r\n\\vdots   & \\vdots \t& \\vdots \t& \\cdots \t \t  \\\\ \r\n\\vdots   & \\vdots  \t& \\vdots \t& \\cdots \t \t \\\\ \r\n\\end{Vmatrix}\r\n\\end{array}\r\n\\end{align*}", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 3.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 162, "fields": {"code": "4.3.2", "category": 3, "problem": "Which of the following property/properties that a finite-state aperiodic irreducible Markov chain has:", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "A. Regular", "choicesb": "B. Recurrent", "choicesc": "C. Both A and B", "choicesd": "D. Neither A and B", "choicese": "E. It cannot be determined.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "An aperiodic irreducible Markov chain is regular since the states communicate each other will have a well-mixed transition probability matrix as long as it is not periodic. \\\\\r\n\r\nAs the Markov chain is regular, there exists a limiting distribution such that all diagonal entires are positive. The condition of a recurrent Markov chain is $\\sum_{n=1}^{\\infty} P_{ii}^{(n)} = \\infty$, which is also satisfied.\\\\\r\n\r\nSo an aperiodic irreducible Markov chain is regular and recurrent.", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 3.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": 0.2, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [35, 52, 53, 56], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 163, "fields": {"code": "7.1.1", "category": 3, "problem": "Determine whether the following equivalences for the age and the excess life in a renewal process $ N(t) $ are correct or not: (Assume $ t > x $)\r\n \\begin{eqnarray*}\r\n Pr\\{\\delta_t\\geq x,\\gamma_t > y\\}&=& Pr\\{N(t-x)=N(t+y)\\}\\\\\r\n &=& \\sum_{k=0}^{\\infty}Pr\\{W_k < t-x,W_{k+1 } > t+y\\}\\\\\r\n &=& [1-F(t+y)]+\\sum_{k=1}^{\\infty}\\int_{0}^{t-x}[1-F(t+y-z)]dF_k(z)\r\n \\end{eqnarray*}\r\nIf correct, carry out the evaluation when the interoccurrence times are exponentially distributed with parameter $ \\lambda $, so that $ dF_k(z) $ is gamma density\r\n$$ dF_k(z) = \\frac{\\lambda^kz^{k-1}}{(k-1)!}e^{-\\lambda z}dz ~~~ for ~z > 0.$$", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Not true.", "choicesb": "True, and the evaluation above is $ \\lambda e^{-\\lambda x - \\lambda y} $.", "choicesc": "True, and the evaluation above is $  e^{-\\lambda x - \\lambda y} $.", "choicesd": "True, and the evaluation above is $  e^{\\lambda x + \\lambda y} $.", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "First, we prove the statement is true.\r\n With the definition of age and excess life, we have\r\n \\begin{eqnarray*}\r\n Pr\\{\\delta_t\\geq x,\\gamma_t > y\\}&=& Pr(W_{N(t)}\\leq t-x, W_{N(t)+1}>t+y)\\\\\r\n &=& Pr(N(t-x)\\geq N(t), N(t+y)<N(t)+1)\\\\\r\n &=& Pr (N(t-x)=N(t)=N(t+y))\\\\\r\n &=& Pr(N(t-x)=N(t+y))\\\\\r\n &=& \\sum_{k=0}^{\\infty}Pr(W_k<t-x, W_{k+1}>t+y)\\\\\r\n &=& Pr(W_0<t-x,W_1>t+y)+\\sum_{k=1}^{\\infty}Pr(W_k<t-x,W_{k+1}>t+y)\\\\ \r\n &=&[1-Pr(W_1\\leq t+y)]+\\sum_{k=1}^{\\infty}[Pr(W_k<t-x)-Pr(W_k<t-x,W_{k+1}=W_k+X_{k+1}\\leq t+y)]\\\\\r\n &=& [1-F(t+y)]+\\sum_{k=1}^{\\infty}[\\int_{0}^{t-x}dF_k(z)-\\int_{0}^{t-x}\\int_{0}^{t+y-z}dF(\\xi)dF_k(z)]\\\\\r\n &=& [1-F(t+y)]+\\sum_{k=1}^{\\infty}\\int_{0}^{t-x}[1-F(t+y-z)]dF_k(z) \r\n \\end{eqnarray*}\r\nSo we finish the proof of the statement. Now we assume that $$ dF_k(z) = \\frac{\\lambda^kz^{k-1}}{(k-1)!}e^{-\\lambda z}dz ~~~ for ~z>0.$$\r\nSo we have $ F(z)=1-e^{-\\lambda z} $, then\r\n\\begin{eqnarray*}\r\n Pr\\{\\delta_t\\geq x,\\gamma_t > y\\}&=& e^{-\\lambda t-\\lambda y}+\\sum_{k=1}^{\\infty}\\int_{0}^{t-x}\\frac{\\lambda^k z^{k-1}}{(k-1)!}e^{-\\lambda t-\\lambda y}dz\\\\\r\n &=& e^{-\\lambda t-\\lambda y} +\\sum_{k=1}^{\\infty} \\frac{\\lambda^k (t-x)^{k}}{k!}e^{-\\lambda t-\\lambda y}\\\\\r\n &=& \\sum_{k=0}^{\\infty} \\frac{\\lambda^k (t-x)^{k}}{k!}e^{-\\lambda t-\\lambda y}\\\\\r\n &=& e^{-\\lambda t-\\lambda y}\\times e^{\\lambda t-\\lambda x}\\\\\r\n &=& e^{-\\lambda x-\\lambda y}\r\n\\end{eqnarray*}\r\nSo the correct answer is c.", "linkability1": 3.0, "linkability2": 3.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Please check whether there are some mistakes in your computation.", "messagesuccess": "Congratulations! You have mastered the definition of the renewal process\r\nand known how to use it.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [76, 83], "rightproblems": [], "wrongproblems": [164, 166, 169], "twinproblems": [164, 166]}}, {"model": "mathematics.question", "pk": 164, "fields": {"code": "7.1.2", "category": 3, "problem": "For $ k\\geq 1 $, determine whether the following statement is true:\r\n \\begin{eqnarray*}\r\n Pr\\{N(t) = k\\} &=& Pr\\{W_k\\leq t < W_{k+1}\\}\\\\\r\n &=& \\int_{0}^{t} [1-F(t-x)]dF_k(x)\r\n \\end{eqnarray*}\r\nIf true, carry out the evaluation when the interoccurrence times are exponentially distributed with parameter $ \\lambda $, so that $ dF_k(z) $ is gamma density\r\n$$ dF_k(z) = \\frac{\\lambda^kz^{k-1}}{(k-1)!}e^{-\\lambda z}dz ~~~ for ~z>0.$$", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Not true.", "choicesb": "True, and the evaluation above is $ \\frac{(\\lambda t)^k}{k!}e^{-\\lambda t} $.", "choicesc": "True, and the evaluation above is $ e^{-\\lambda kt} $.", "choicesd": "True, and the evaluation above is $  \\frac{(\\lambda t)^k}{(k-1)!}e^{-\\lambda kt} $.", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "First, we prove the statement is true.\r\n With the definition of age and excess life, we have\r\n \\begin{eqnarray*}\r\nPr\\{N(t) = k\\} &=&Pr\\{N(t)\\geq k\\}-Pr\\{N(t)\\geq k+1\\}\\\\\r\n&=& Pr\\{W_k\\leq t\\}-Pr\\{  W_{k+1}\\leq t\\}\\\\\r\n&=& Pr\\{W_k\\leq t < W_{k+1}\\}\\\\\r\n&=& \\int_{0}^{t}\\int_{t-z}^{\\infty} dF(\\xi)dF_k(z)\\\\\r\n&=&\\int_{0}^{t} [1-F(t-z)]dF_k(z)\r\n \\end{eqnarray*}\r\nSo we finish the proof of the statement. Now we assume that $$ dF_k(z) = \\frac{\\lambda^kz^{k-1}}{(k-1)!}e^{-\\lambda z}dz ~~~ for ~z>0.$$\r\nSo we have $ F(z)=1-e^{-\\lambda z} $, then\r\n\\begin{eqnarray*}\r\n Pr\\{N(t) = k\\} &=& \\int_{0}^{t} e^{-\\lambda t+ \\lambda z}\\frac{\\lambda^k z^{k-1}}{(k-1)!}e^{-\\lambda z} dz\\\\\r\n &=& \\frac{(\\lambda t)^k}{k!}e^{-\\lambda t}\r\n\\end{eqnarray*}\r\nSo the correct answer is c.", "linkability1": 3.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Please check whether there are some mistakes in your computation.", "messagesuccess": "Congratulations! You have mastered the definition of the renewal process\r\nand known how to use it.", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [76], "rightproblems": [], "wrongproblems": [163], "twinproblems": [163]}}, {"model": "mathematics.question", "pk": 165, "fields": {"code": "7.1.3", "category": 3, "problem": "A fundamental identity involving the renewal function, vaild for all renewal processes, is\r\n $$ E[W_{N(t)+1}] = E[X_1](M(t)+1) ,$$\r\n where $ M(t) = E[N(t)]$. Using this identity, compute the mean excess life $ E[\\gamma_t] .$ The  $ E[\\gamma_t] $ is:", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$ E[X_1](1+M(t)) -t $", "choicesb": "$ E[X_1](1+M(t))  $", "choicesc": "$ E[X_1](1+M(t)) +t $", "choicesd": "$ E[X_1]M(t) +1 $", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Since $ \\gamma_t = W_{N(t)+1} -t $, with the identity above, we have:\r\n \\begin{eqnarray*}\r\nE[\\gamma_t]&=& E[W_{N(t)+1} -t]\\\\\r\n&=& E[W_{N(t)+1}] -t\\\\\r\n&=& E[X_1](1+M(t)) -t\r\n \\end{eqnarray*}", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Please check the definition of excess life in renewal process.", "messagesuccess": "Congratulations! You have mastered the definition of the renewal process\r\nand known how to use it.", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [80, 81, 83], "rightproblems": [], "wrongproblems": [167, 168], "twinproblems": [167, 168]}}, {"model": "mathematics.question", "pk": 166, "fields": {"code": "7.1.4", "category": 3, "problem": "Let $ \\gamma_t $ be the excess life and $ \\delta_t $ the age in a renewal process having interoccurrence distribution function $ F(x) $. Determine the conditional probability $ Pr(\\gamma_t>y|\\delta_t = x) $ and the conditional mean $ E[\\gamma_t|\\delta_t =x] $.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$ 1-F(x+y) $, $ \\int_{0}^{\\infty}1-F(x+y)dy  $", "choicesb": "$ 1-F(x-y) $, $ \\int_{0}^{\\infty}1-F(x-y)dy  $", "choicesc": "$ 1-F(x+\\frac{1}{2}y) $, $ \\int_{0}^{\\infty}1-F(x+\\frac{1}{2}y)dy  $", "choicesd": "$ 1-F(x-\\frac{1}{2}y) $, $ \\int_{0}^{\\infty}1-F(x-\\frac{1}{2}y)dy  $", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "With the defintion, we have:\r\n \\begin{eqnarray*}\r\n Pr(\\gamma_t>y|\\delta_t = x) &= & Pr(W_{N(t)+1}>t+y|t = W_{N(t)}+x)\\\\\r\n &=& Pr(W_{N(t)+1}-W_{N(t)}>x+y| W_{N(t)}=t-x)\\\\\r\n &=& Pr(X_{N(t)+1}>x+y|W_{N(t)}=t-x)\\\\\r\n &=& Pr(X_{N(t)+1}>x+y)\\\\\r\n &=& 1 - F(x+y)\r\n \\end{eqnarray*}\r\nAnd \r\n$$ E[\\gamma_t|\\delta_t=x] = \\int_{0}^{\\infty}  Pr(\\gamma_t>y|\\delta_t = x) =\\int_{0}^{\\infty}1-F(x+y)dy $$\r\nSo the right answer is (a).", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Please check the definition of the age and excess life in renewal process.", "messagesuccess": "Congratulations! You have mastered the definition of the renewal process\r\nand known how to use it.", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [76, 83], "rightproblems": [], "wrongproblems": [163], "twinproblems": [163]}}, {"model": "mathematics.question", "pk": 167, "fields": {"code": "7.3.1", "category": 3, "problem": "In another form of sum quota sampling, a sequence of nonnegative independent and identically distributed random variables $ X_1, X_2, \\ldots $ is observed, the sampling continuing until the first time that the sum of the observations exceeds the quota $ t $. In renewal process terminology, the sample size is $ N(t)+1 $. The sample mean is  \r\n$$ \\frac{W_{N(t)+1}}{N(t)+1}=\\frac{X_1+\\cdots+X_{N(t)+1}}{N(t)+1}.$$\r\nAn important question in statistical theory is whether or not this sample mean is unbiased. That is, how does the expected value of this sample mean relate to the expected value of, say, $ X_1 $? Assume that the individual $ X $ summands are exponentially distributed with parameter $ \\lambda $, so that $ N(t) $ is a Poisson process. Evaluate $ E[\\frac{W_{N(t)+1}}{N(t)+1}] $:", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$ \\frac{1}{\\lambda t}[1-e^{-\\lambda t}](1+\\frac{1}{\\lambda t}) $", "choicesb": "$ [1-e^{-\\lambda t}](1+\\frac{1}{\\lambda t}) $", "choicesc": "$ [1-e^{-\\lambda t}] $", "choicesd": "$ \\frac{1}{\\lambda}[1-e^{-\\lambda t}](1+\\frac{1}{\\lambda t}) $", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "With the result of erercise 7.3.3. We have $  W_{N(t)+1}$ and $ N(t) $ are independent. So we just need to compute $ E[W_{N(t)+1}] $ and $ E[\\frac{1}{N(t)+1}] $ separately.\r\n $$ E[W_{N(t)+1}] = E[X_1](E[N(t)+1]) = \\frac{1}{\\lambda}(\\lambda t+1)$$and\r\n \\begin{eqnarray*}\r\n E[\\frac{1}{N(t)+1}] &=& \\sum_{k=0}^{\\infty} \\frac{(\\lambda t)^k e^{-\\lambda t}}{k!}\\frac{1}{k+1}\\\\\r\n &=& (\\sum_{k=0}^{\\infty} \\frac{(\\lambda t)^k e^{-\\lambda t}}{k!}-e^{-\\lambda t})\\frac{1}{\\lambda t}\\\\\r\n &=& (1-e^{-\\lambda t})\\frac{1}{\\lambda t}\r\n \\end{eqnarray*}\r\nSo $$ E[\\frac{W_{N(t)+1}}{N(t)+1}] = E[W_{N(t)+1}]E[\\frac{1}{N(t)+1}] = \\frac{1}{\\lambda}[1-e^{-\\lambda t}](1+\\frac{1}{\\lambda t})$$\r\nSo the right answer is (d).", "linkability1": 3.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Please check the result of exercise 7.3.3.", "messagesuccess": "Congratulations! You have mastered the definition of the renewal process\r\nand known how to use it.", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [77, 80, 81], "rightproblems": [], "wrongproblems": [165], "twinproblems": [165]}}, {"model": "mathematics.question", "pk": 168, "fields": {"code": "7.3.2", "category": 3, "problem": "A fundamental identity involving the renewal function, vaild for all renewal processes, is \r\n$$E[W_{N(t)+1}] = E[X_1](M(t)+1).$$\r\nEvaluate the left side the renewal counting process is a Poisson process. $E[W_{N(t)+1}]$ is equal to:", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$ \\lambda t+1 $", "choicesb": "$  t+\\frac{1}{\\lambda} $", "choicesc": "$ \\lambda t $", "choicesd": "$ e^{-\\lambda t} $", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "With the defintion of the poisson process, we have:\r\n \\begin{eqnarray*}\r\n E[W_{N(t)+1}] &=& \\int_{0}^{\\infty} P(W_{N(t)+1}>x)dx\\\\\r\n &=& \\int_{0}^{\\infty} P(N(x) < N(t)+1)dx\\\\\r\n &=& t+\\int_{t}^{\\infty}P(N(t-x) < 1)dx\\\\\r\n &=& t+\\int_{t}^{\\infty}e^{-\\lambda (x-t)}dx\\\\\r\n &=& t+\\frac{1}{\\lambda}\r\n \\end{eqnarray*}\r\nSo the right answer is (b).", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Please check the knowledge of this part.", "messagesuccess": "Congratulations! You have mastered the knowledge of this part\r\nand known how to use it.", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [75, 77, 81], "rightproblems": [169], "wrongproblems": [165], "twinproblems": [165]}}, {"model": "mathematics.question", "pk": 169, "fields": {"code": "7.3.3", "category": 3, "problem": "Pulses arrive at a counter according to a Poisson process of rate $ \\lambda $. All physically realizable counters are imperfect, incapable of detecting all signals that enter their detection chambers. After a particle or signal arrives, a counter must recuperate, or renew itself, in preparation for the nest arrival. Signals arriving during the readjustment period, called dead time or locked time, are lost. We must distinguish between the arriving particles and the recorded particles. The experimenter observes only the particles recorded; from this observation he desires to infer the properties of the arrival process.\r\n<br>\r\nSuppose the each arriving pulse locks the counter for a fixed time $ \\tau $. Assume that $ t>\\tau $, determine the probability $ p(t) $ that the counter is free at time $ t $", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$ e^{-2\\lambda \\tau} $", "choicesb": "$  e^{-\\lambda \\tau}$", "choicesc": "$ e^{-2\\lambda t} $", "choicesd": "$ e^{-\\lambda t} $", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "With the defintion of the current life $ \\delta_t $, we know that\r\n $$ p(t) = Pr\\{\\delta_t>\\tau\\}$$\r\n For Poisson process, we know that when $ t>\\tau $,\r\n $$ Pr\\{\\delta_t>\\tau\\} = e^{-\\lambda \\tau} $$\r\nSo the right answer is (b).", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Please check the knowledge of this part.", "messagesuccess": "Congratulations! You have mastered the knowledge of this part\r\nand known how to use it.", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [77, 83], "rightproblems": [168], "wrongproblems": [163, 171], "twinproblems": [171]}}, {"model": "mathematics.question", "pk": 170, "fields": {"code": "7.3.4", "category": 3, "problem": "This problem is designed to aid in the understanding of length-biased sampling. Let $ X $ be a uniformly distributed random variable on $ [0,1] $. Then, $ X $ divides $ [0,1] $ into the subintervals $ [0,X] $ and $ (X,1] $. By symmetry, each subinterval has mean length $ \\frac{1}{2} $. Now pick one of these subintervals at random in the following way: Let $  Y $ be independent of $ X $ and uniformly distributed on $ [0,1] $, and pick the subinterval $ [0,X] $ or $ (X,1] $ that $ Y $ falls in. Let $ L $ be the length of the subinterval so chosen. Formally,\r\n\\[  L= \\left\\{\r\n\\begin{array}{ll}\r\nX, & Y\\leq X \\\\\r\n1-X, & Y > X \\\\\r\n\r\n\\end{array} \r\n\\right. \\]\r\nDetermine the mean of $ L $.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1/2", "choicesb": "2/3", "choicesc": "3/4", "choicesd": "4/5", "choicese": "5/6", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "We need to use the tower property of conditional expectation, which is \r\n $$E(L) = E(E(L|X))$$\r\n so \r\n \\begin{eqnarray*}\r\n  E(L) &=& E[E[XPr(Y\\leq X)+(1-X)Pr(Y>X)|X]]\\\\\r\n  &=& E(X^2 +(1-X)^2)\\\\\r\n  &=& 2/3\r\n \\end{eqnarray*}\r\n\r\nSo the right answer is (b).\r\n<br>\r\nIf you are not familiar with conditional expectation, you can compute the expectation directly.", "linkability1": 1.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! You can check the knowledge of conditional expectation to solve this problem.", "messagesuccess": "Congratulations! You have mastered the knowledge of this part\r\nand known how to use it.", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [84], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 171, "fields": {"code": "7.3.5", "category": 3, "problem": "Birds are perched along a wire as shown according to a Poisson process of rate $ \\lambda $ per unit distance. At a fixed point t along the wire, let $ D(t) $ be the random distance to the nearest bird. What is the mean value of $ D(t) $? What is the probability density function $ f_t(x) $ for $ D(t) $ when $ 0 < x < t $ ?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$E[D(t)] = 1-\\frac{1}{2\\lambda}e^{-2\\lambda t} $ , $  f_t(x) = 1 - e^{-2\\lambda x}$", "choicesb": "$E[D(t)] = 1-\\frac{1}{\\lambda}e^{-\\lambda t} $ , $  f_t(x) = 1 - e^{-\\lambda x}$", "choicesc": "$E[D(t)] = 1-\\frac{2}{\\lambda}e^{-\\frac{1}{2}\\lambda t} $ , $  f_t(x) = 1 - e^{-\\frac{1}{2}\\lambda x}$", "choicesd": "$E[D(t)] = 1-e^{-\\lambda t} $ , $  f_t(x) = 1 - e^{-\\lambda x}$", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "We need to realize that $ D(t) = \\min\\{\\delta_t, \\gamma_t\\} $. So when $ 0 < x < t $, the density funtion of $ D(t) $ is:\r\n<br>\r\n \\begin{eqnarray*}\r\nP(D(t)\\leq x)&=& 1-P(D(t) >  x)\\\\\r\n&=& 1-P(\\min\\{\\delta_t, \\gamma_t\\}> x)\\\\\r\n&=& 1-P(\\delta_t>x, \\gamma_t> x)\\\\\r\n&=& 1-e^{-2\\lambda x}\r\n \\end{eqnarray*}\r\nAfter knowing the density function, the expectation is easy to compute that  $ E[D(t)] = 1-\\frac{1}{2\\lambda}e^{-2\\lambda t} $.\r\n \r\n\r\nSo the right answer is (a).", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Please check the knowledge of this part.", "messagesuccess": "Congratulations! You have mastered the knowledge of this part\r\nand known how to use it.", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [77, 83], "rightproblems": [], "wrongproblems": [169], "twinproblems": [169]}}, {"model": "mathematics.question", "pk": 172, "fields": {"code": "7.4.1", "category": 3, "problem": "Suppose that a renewal function has form $ M(t) = t +[1-\\exp(-at)] $. Determine the mean and variance of the interoccurrence distribution.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$ \\mu = 1, \\sigma^2 = 1 $", "choicesb": "$ \\mu = 1, \\sigma^2 = 3 $", "choicesc": "$ \\mu = 2, \\sigma^2 = 1 $", "choicesd": "$ \\mu = 2, \\sigma^2 = 3 $", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "With the use of the elementary renewal theorem and refined renewal theorem, this question is very easy.\r\n The theorem tells us that:\r\n $$\\lim_{t\\rightarrow \\infty}\\frac{M(t)}{t} = \\frac{1}{\\mu}$$\r\n and\r\n $$ \\lim_{t\\rightarrow \\infty}[M(t)-\\frac{t}{\\mu}] = \\frac{\\sigma^2-\\mu^2}{2\\mu^2}$$\r\n With the $ M(t) = t +[1-\\exp(-at)] $, we know that \r\n $$ \\frac{1}{\\mu} = 1 $$ and $$  \\frac{\\sigma^2-\\mu^2}{2\\mu^2} = t+1-\\frac{t}{\\mu}.$$\r\n So  $ \\mu = 1, \\sigma^2 = 3 $. So the right answer is (b).", "linkability1": 3.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Please see the elementary renewal theorem and refined renewal theorem.", "messagesuccess": "Congratulations! You have already known about  the knowledge of asymptotic behavior of renewal process.", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [85], "rightproblems": [], "wrongproblems": [173, 174, 175], "twinproblems": [174]}}, {"model": "mathematics.question", "pk": 173, "fields": {"code": "7.4.2", "category": 3, "problem": "A system is subject to failures. Each failure requires a repair time that is exponentially distributed with rate parameter $ \\alpha $. The operating time of the system until the next failure is exponentially distributed with rate parameter $ \\beta $. The repair times and the operating times are all statistically independent. Suppose that the system is operating at time 0. Using refined renewal theorem, determine an approximate expression for the mean number of failures up to time $ t $, the approximation holding for $ t\\gg 0 $.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$ \\frac{t\\alpha\\beta}{\\alpha+\\beta}-\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2} $", "choicesb": "$ \\frac{t\\alpha\\beta+\\alpha}{\\alpha+\\beta}-\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2} $", "choicesc": "$ \\frac{t\\alpha\\beta+\\beta}{\\alpha+\\beta}-\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2} $", "choicesd": "$ \\frac{t\\alpha\\beta}{\\alpha+\\beta}-\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2}+1 $", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "This is a tough question. We use two steps to solve this question: first, with the use of refined renewal theorem, we can compute the mean number of the complement of repair up to time $ t $. Here, we use $ N(t) $ to stand for it. In other word, we account the time of finishing repair machine. With the refined renewal theorem,\r\n $$ \\lim_{t\\rightarrow \\infty}[M(t)-\\frac{t}{\\mu}] = \\frac{\\sigma^2-\\mu^2}{2\\mu^2}$$\r\n SInce our $ X_1 $ here is operating time plus repair time. We know they follow exponential distribution and they are independent. So we have:\r\n $$ \\mu = \\frac{1}{\\alpha}+\\frac{1}{\\beta}, \\sigma^2 = \\frac{1}{\\alpha^2}+\\frac{1}{\\beta^2} $$\r\n So \r\n $$ E[N(t)] = M(t) = \\frac{t\\alpha\\beta}{\\alpha+\\beta}-\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2}  $$\r\n However, please remember we just know the mean number of time of finishing repair machine $ M(t) $, which is not equal to the mean number of failures, which we use $ \\tilde{M}(t) $ to stand for. Our second step is just to know relationship between them. It is not difficult to see that, if at time $ t $, it is the repair time. $ M(t)+1 =\\tilde{M}(t) $. If at time $ t $, it is the operating time. $ M(t) =\\tilde{M}(t) $. We use the event $ A $ to stand that time $ t $  is the repair time. So the $ A^c $ is that time $ t $  is the operating time. So $$ \\tilde{M}(t) = (M(t)+1)P(A)+M(t)P(A^c)$$.\r\n It is easy to compute that $$ P(A) = \\frac{1/\\alpha}{1/\\alpha+1/\\beta}= \\frac{\\beta}{\\alpha+\\beta}, P(A^c) = \\frac{\\alpha}{\\alpha+\\beta}.  $$\r\n So $$\\tilde{M}(t) = \\frac{t\\alpha\\beta+\\beta}{\\alpha+\\beta}-\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2} $$\r\n So the right answer is (c).", "linkability1": 3.0, "linkability2": 3.0, "linkability3": 2.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Please see the refined renewal theorem.", "messagesuccess": "Congratulations! You have mastered the knowledge of this part.", "sensitivity": null, "gussingparameter": null, "difficulty": 5, "calculateddifficulty": null, "linkneuron": [83, 85], "rightproblems": [], "wrongproblems": [172], "twinproblems": [175]}}, {"model": "mathematics.question", "pk": 174, "fields": {"code": "7.4.3", "category": 3, "problem": "Suppose that the life of a lightbulb is a random variable $ X $ with hazard rate $ h(x) = \\theta x$ for $ x>0 $. Each failed lightbulb is immediately replaced with a new one. Determine an asymptotic expression for the mean age of the lightbulb in service at time $ t $, vaild for $ t\\gg 0 $.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$ \\sqrt{\\frac{2}{\\pi\\theta}}$", "choicesb": "$ \\sqrt{\\frac{1}{\\pi\\theta}} $", "choicesc": "$ 2\\sqrt{\\frac{1}{\\pi\\theta}} $", "choicesd": "$ 2\\sqrt{\\frac{2}{\\pi\\theta}} $", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "It is easy to see that this question just ask us to compute that $ E(\\beta_t) $, which is equal to $ E(\\delta_t) + E(\\gamma_t)$. With $ h(x) =\\theta x $, we know that $ H(X) = \\frac{1}{2}\\theta x^2 $, which is equal to $ -\\ln(1-F(x)) $. So \r\n$$1-F(X) = e^{-\\frac{1}{2}\\theta x^2}$$\r\nSince $$ E(\\delta_t)= E(\\gamma_t) = \\frac{\\sigma^2+\\mu^2}{2\\mu} $$\r\nWith $ 1-F(X) = e^{-\\frac{1}{2}\\theta x^2} $, it is easy to compute that $ \\mu = \\sqrt{\\frac{\\pi}{2\\theta}}, \\mu^2+\\sigma^2 = \\frac{2}{\\theta}$.\r\nSo $ E(\\beta_t) =  2\\sqrt{\\frac{2}{\\pi\\theta}} $. So the right answer is (d).", "linkability1": 3.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Please see the asymptotic behavior of Age and Excess life.", "messagesuccess": "Congratulations! You have already known about  the knowledge of asymptotic behavior of renewal process.", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [83, 85], "rightproblems": [], "wrongproblems": [172, 176], "twinproblems": [172]}}, {"model": "mathematics.question", "pk": 175, "fields": {"code": "7.4.4", "category": 3, "problem": "A developing country is attempting to control its population growth by placing restrictions on the number of children each family can have. This society places a high premium on female children, and it is felt that any policy that ignores the desire to have female children will fail. The proposed policy is to allow any married couple to have children up to the first female baby, at which point they must cease having children. Assume that male and female children are equally likely. The number of children in any family is a random variable $ N $. In the population as a whole, what fraction of children are female? Use the elementary renewal theorem to compute it.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1/2", "choicesb": "2/3", "choicesc": "3/4", "choicesd": "4/5", "choicese": "5/6", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "There is a renewal process inside question where our $ X_1 $ follow geometric distribution with $ p = q = 1/2 $.\r\n This question just ask to compute $ \\frac{M(t)}{t} $. With the use of elementary renewal theorem:\r\n $$\\lim_{t\\rightarrow \\infty}\\frac{M(t)}{t} = \\frac{1}{\\mu}=\\frac{1}{2}$$", "linkability1": 1.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Please see the elementary renewal theorem.", "messagesuccess": "Congratulations! You have already known about  the knowledge of asymptotic behavior of renewal process.", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [85], "rightproblems": [], "wrongproblems": [172], "twinproblems": [173]}}, {"model": "mathematics.question", "pk": 176, "fields": {"code": "7.4.5", "category": 3, "problem": "A Markov chain $ X_0, X_1, X_1,\\ldots $ has the transition probability matrix\r\n $$ \\qquad  \\,\\, \\,\\,\\, 0\\qquad 1\\qquad 2  $$\r\n$${ P} =\\matrix{ 0 \\cr 1 \\cr 2    }\r\n\\pmatrix{\r\n\t0.3 &  0.7  &  0              \\cr\r\n\t0.6\t&   0  &  0.4      \\cr\r\n\t0\t &   0.5  &   0.5          \\cr\r\n}\r\n$$\r\nA sojourn in a state is an uninterrupted sequence of consecutive visits to that state. Determine the mean duration of a typical sojourn in state 0, which we use $ x $ to stand for. And determine the long run fraction of time that the process is in state 1, which we use $ y $ to stand for. Compute the value of $ x $ and $ y $.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$ x = 10/3, y = 35/93$", "choicesb": "$ x = 10/7, y=35/93$", "choicesc": "$ x= 10/3, y = 58/93 $", "choicesd": "$ x = 10/3, y = 58/93 $", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "$ x $ is easy to compute. We can regard the number of duration of sojourn in state 0 as a geometric distribution with $ p = 0.3, q = 0.7 $. So the $ x = 10/7 $. With the knowledge of renewal theory, we know $y= \\pi_1$ where $ \\pi $ satisfy that:\r\n$$ \\pi P = \\pi $$\r\nSo $ \\pi_0 = 10/31, \\pi_1 = 35/93, \\pi_2 = 28/93 $. So our answer is (b).", "linkability1": 3.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops! Please review the renewal theorem.", "messagesuccess": "Congratulations! You have already known about  the knowledge of asymptotic behavior of renewal process.", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [85], "rightproblems": [], "wrongproblems": [174], "twinproblems": []}}, {"model": "mathematics.question", "pk": 177, "fields": {"code": "4.1.1", "category": 1, "problem": "Let $\\{X_n\\}$ be a $MC$ with two states 0 and 1, and transition\r\nmatrix:\r\n$$\\bf P = \\pmatrix{0.33 & 0.67 \\cr 0.75 & 0.25}.$$\r\nThen,\r\n\\begin{eqnarray*}\r\n&&  \\bf P^2 = \\pmatrix{0.611 & 0.389 \\cr 0.438 & 0.562}, \\quad\r\n\\bf P^5 = \\pmatrix{0.524 & 0.476 \\cr 0.536 & 0.464}, \\\\\r\n&& \\bf P^7 = \\pmatrix{0.528 & 0.472 \\cr 0.530 & 0.469}, \\quad .... \\quad\r\n\\bf P^{16} = \\pmatrix{0.5294 & 0.4706 \\cr 0.5294 & 0.4706} \\quad ....\r\n\\end{eqnarray*}\r\nRecall that $\\bf P^{(n)}=\\bf P^n$.\r\nWe get the impression from this example that\r\nthe $n$-step transition matrix converge, actually quite fast in this example,\r\nand that limits in the same columns are the same.\r\nThe question is: is this a general phenomenon or only pertained to\r\nthis $MC$? Note that all the above transition matrices, one-step or many\r\nstep, have all positive entries.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No solution for an example", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [35], "rightproblems": [], "wrongproblems": [178], "twinproblems": []}}, {"model": "mathematics.question", "pk": 178, "fields": {"code": "4.1.2", "category": 1, "problem": "Let $\\{X_n\\}$ be a $MC$ with two states 0 and 1, and transition\r\nmatrix:\r\n$$\\bf P = \\pmatrix{0 & 1 \\cr 1 & 0}.$$\r\nIn other words, from state 1, the $MC$ sure goes to 0 the next step; and\r\nfrom state 0, it sure goes to 1 the next step. Then\r\n$$\\bf P^k =\\bf P \\quad \\hbox {for all odd $k$}, \\qquad\r\n\\bf P^k= \\pmatrix{1 & 0 \\cr 0  & 1 } \\qquad \\hbox{for all even $k$}.$$\r\nClearly, $\\bf P^n$ does not converge as $n \\to \\infty$.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No solution for an example", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [35], "rightproblems": [], "wrongproblems": [177, 179], "twinproblems": []}}, {"model": "mathematics.question", "pk": 179, "fields": {"code": "4.1.3", "category": 1, "problem": "According to an old Chinese saying: Fu Bu Guo San Dai\r\n(wealthiness does not last beyond third generation---a word-by-word\r\ntranslation). <br>\r\nIndeed, suppose every family in a population assumes one of $N+1$ financial classes/states:\r\nState 0 is the poorest, 1 the second poorest, ...,  $N$ the richest.\r\nLet $X_n$  the state of the $n$-th generation of a family. It is reasonable to assume a transition matrix with all $P_{ij} > 0$, $i, j=0, 1,..., N$. Such transition probability matrix and the $MC$ are so called $regular$, to be defined below.\r\n<br>\r\nIt will be shown that, in the long run,the family, no matter it started rich or poor, will turn out ordinary after many generations. The same is true for, other than wealth, height or IQ or nearly all of the biological traits. The hundredth generation of Yao, Ming, the star basketball player, and that of Pan, Changjiang, a  Chinese comedy star known for being short and funny, will have about same distribution of height.  The former can very well be shorter than the latter, and, even if taller, it's largely by randomness.\r\n<br>\r\nIndeed, we shall show a large class of $MC$s, called regular $MC$s,\r\nthe transition matrices do converge. Example 4.1 is one of those and\r\nExample 4.2 is not.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No solution for an example", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [35], "rightproblems": [], "wrongproblems": [178, 180], "twinproblems": []}}, {"model": "mathematics.question", "pk": 180, "fields": {"code": "4.1.4", "category": 1, "problem": "Mr. C  spends his life time   only in three places Home, Office and\r\nGym. Assume every time he reaches H, O and G, he spends 12, 10 and 2 hours, respectively,\r\n and\r\nthen transits to another location according to the following transition\r\nprobability matrix\r\n\\begin{eqnarray*}\r\n&&\\qquad \\,\\, \\, {\\rm H} \\quad \\,\\, {\\rm O} \\quad \\,\\, {\\rm G} \\\\\r\n\\bf P\r\n&=& \\matrix{{\\rm H} \\cr {\\rm O} \\cr {\\rm G}}\r\n\\pmatrix{ 0   & 0.9 & .1 \\cr .1 & 0& .9 \\cr 0.8 & 0.2 & 0}\r\n\\end{eqnarray*}\r\nIt is easily verified that $\\bf P^2$ has all entries positive and therefor\r\n$\\bf P$ is regular, which is perhaps easier to see from the this figure.\r\n\r\nSolving equation (4.2)-(4.3), we have\r\n$$\\pi_H= 0.309 \\qquad \\pi_O=0.347\\qquad \\pi_G=0.343.$$\r\nLet $X_n$ be Mr C's whereabout after $n$ transitions.\r\n$\\{X_n: n\\geq 0\\}$ is a $MC$ with state space\r\n$\\{ {\\rm H, O, G} \\}$ and  transition matrix $\\bf P$.\r\nLet $n_H$ be the number of times he stays in state H\r\n in a long period with (a large) $n$ transitions. Likewise\r\n define\r\n $n_O$ and $n_G$.\r\nThe long  run fraction of his life time spent in office is\r\n \\begin{eqnarray*}\r\n && {n_O \\times 10 \\over n_H \\times 12 + n_O \\times 10 + n_G \\times 2}\r\n ={10 n_O/n \\over 12 n_H/n + 10 n_O/n + 2 n_G/n} \\\\\r\n& {   \\approx } &\r\n {10 \\pi_O \\over 12 \\pi_H + 10 \\pi_O + 2 \\pi_G }\r\n = 0.441.\r\n  \\end{eqnarray*}\r\nTranslating into daily hours, it is $0.441*24 =10.58$ hours in office per day.\r\nLikewise, the long run fractions of his life time spent in Home and\r\nGym are, respectively, $0.472$ and $0.087$, which are\r\n$11.32$ and $2.09$ in daily hours.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No solution for an example", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [24], "rightproblems": [], "wrongproblems": [179], "twinproblems": []}}, {"model": "mathematics.question", "pk": 181, "fields": {"code": "5.1.4", "category": 1, "problem": "Suppose coin $A$ and coin $B$ each respectively has chances\r\n$q$ and $p$ to turn up head, with $q$ being very small.\r\n  Suppose coins A and B are tossed\r\n$together$     a large number of $n$ times, and\r\n  $nq$ is close to $\\mu>0$. \r\n<br>\r\nLet $X$ be the number of times $A$ turning up\r\nhead and $Y$ be the number of times both $A$ and $B$  turning\r\nup heads.\r\nIt is clear that\r\n$X \\sim Bin(n, q)$, which is approximately ${\\cal P}(\\mu)$.\r\n$Y|\\{X=k\\}$ is the number of heads of coin B on those $k$ tosses with\r\n coin A being heads.\r\nThen, $Y|\\{X=k\\} \\sim Bin(k, p)$, as the  two coins are tossed\r\nindependently.\r\nOn the other hand,\r\n$Y\\sim Bin(n, qp)$, which is approximately ${\\cal P}(\\mu p)$.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No solution for an example", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [68], "rightproblems": [], "wrongproblems": [182], "twinproblems": []}}, {"model": "mathematics.question", "pk": 182, "fields": {"code": "5.1.5", "category": 1, "problem": "Suppose, in addition to the assumptions (1) and (2) in Example 5.2,\r\nthe chance, denoted by $p$, for  Chelsea to score a goal\r\nwithin any given one minute is equal. Let $X(t)$\r\nbe the number of the Chelsea goals from the beginning of the\r\nmatch till time $t$, with the unit of the time being minute.\r\nThen, $\\{X(t): t\\in [0, 90]\\}$ is $approximately$ a Poisson\r\nprocess with rate $p$, the mean number of goals per minute, the time unit.\r\n(A match has 90 minutes.)\r\n\r\nIt is important to understand Poisson processes through binomial approximation as,\r\nfor example, follows.\r\nHeuristically, let $\\Delta t$ be a very very small time unit. Cut up the half line\r\n$(0, \\infty)$ into $(0, \\Delta t], (\\Delta t, 2\\Delta t], (2 \\Delta t, 3 \\Delta t], ...$\r\nSuppose events occur along time satisfying the following assumptions.\r\nFirst, corresponding to (b) or (5.1'),\r\n$$\\xi_i =\\cases{ 1 &  with probabability $\\lambda \\Delta t$, if an event occur in interval\r\n$((i-1)\\Delta t, i \\Delta t]$ \\cr\r\n0 & otherwise.}\r\n$$\r\nSecond, corresponding to (a), assume $\\xi_i, i \\geq 1$ are independent.\r\nLet $X(t)$ be the number of events happening up to time $t$. Then,\r\n$X(t), t\\geq 0$ is approximately a Poisson process with intensity $\\lambda$.\r\nIn fact, for any fixed $t>0$,\r\n$X(t) \\approx \\sum_{i=1}^n \\xi_i$ where $n $ is the integer part of $t/\\Delta t$.\r\nSince, by binomial approximation,\r\n$$\\sum_{i=1}^n \\xi_i \\sim Bin(n,  \\lambda \\Delta t) \\approx {\\cal P}( n \\lambda \\Delta t)\r\n\\approx {\\cal P}(\\lambda t),$$\r\nWe conclude $X(t)$ follows approximately ${\\cal P}(\\lambda t)$. Likewise argument\r\nimplies $X(t+s)-X(s)$ follows approximately ${\\cal P}(\\lambda t )$. The independent\r\nincrement of $\\{X(t)\\}$ is ensured approximately by the independence of $\\xi_i, i \\geq 1$.\r\n\r\n\r\nThe Poisson process can be viewed as tallying\r\nmicro-rare events over time.\r\n Therefore it is widely used to model number of events which happen\r\nrandomly along time. Here $\\lambda$ measures the intensity of the\r\nevents happening. It measures\r\nthe average number of events per time unit.\r\n The larger (smaller) the $\\lambda$, the more (less) likely the\r\nevents.\r\nThe most essential ingredient of the Poisson process is\r\nthat {\\it over different non-overlapping time intervals, event occurrences\r\n are entirely independent}. More general version of Poisson processes\r\n allows the intensity vary over time, which is referred to as\r\n non-homogeneous Poisson process.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No solution for an example", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [68], "rightproblems": [], "wrongproblems": [181], "twinproblems": []}}, {"model": "mathematics.question", "pk": 183, "fields": {"code": "6.1.4", "category": 1, "problem": "In a partly (or mostly) fictitious portray of\r\n the battle of Thermopylae in 480 BC,  300 Greek Spartans, led by King Leonidas,\r\nwere   allegedly  able  to block for three days the advance of a Persian army\r\nof King Xerxes numbered in the hundreds of thousands, and were later all killed.\r\nSuppose the ever-brave King Leonidas  only counted the number of surviving soldiers\r\nincluding himself, at any time $t$; whilst the ever-optimistic King Xerxes\r\n only counted the number of kills\r\nthey had inflicted upon the Spartans at any time $t$. Let\r\n$X(t)$ and $Y(t)$ be the counting process in the minds of Leonidas and Xerxes, respectively.\r\nThen $X(0)=300$ and $Y(0)=0$. Clearly $X(t)=300-Y(t)$.\r\nThen,  if $\\{ Y(t): t \\in [0, \\infty) \\}$ is a pure birth process, then\r\n$\\{ X(t): t\\in [ \\infty)\\}$ is a pure death process.\r\nIf the later is a pure death process, then the former is a pure birth process.}\r\n<br>\r\n In fact, it can be seen from the definitions of pure birth and pure death processes,\r\n especially through postulate 3 via sojourn times, that there is a mirror\r\n relation between pure birth and pure death process as illustrated above\r\n and reiterated as follows.\r\n For any pure death process $X(t)$, beginning with $X(0)=N$, with state space $\\{0, ..., N\\}$\r\n and death rates $\\mu_N, ..., \\mu_1$,\r\nthe process $Y(t) = N-X(t)$, which counts the cumulative number of\r\ndeaths, is a pure birth process with birth rates\r\n$$\\lambda_0 = \\mu_N,\r\n\\lambda_1 = \\mu_{N-1}, ..., \\lambda_{N-1} = \\mu_1\r\n\\quad  {\\rm and }  \\quad \\lambda_N = 0.$$\r\nNote also that the sojourn time $S_k$ for the pure death process\r\n$X(\\cdot)$ is then the sojourn time $S_{N-k} $ for\r\nthe pure birth process $Y(\\cdot) = N-X(\\cdot)$.\r\n<br>\r\n  (continued)  Assume\r\nthe   life times of the $N=300$  Spartan soldiers are\r\niid following a common\r\n exponential distribution with parameter\r\n$\\alpha$,\r\nthen the process of the number of\r\nsurviving solders by time $t$,   $\\{X(t)\\}$, is a pure death\r\nprocess with rates $\\mu_k = K\\alpha$.\r\nThe following is a proof.\r\nLet $\\xi_i, i=1,..., N$ be the life times of the $N$ members of\r\nthe population. Then, $\\xi_i$ are iid following exponential distribution\r\nwith parameter $\\alpha$.\r\n\\begin{eqnarray*}\r\n&& P(X(t+h) - X(t) =-1 | X(t)= k)\r\n\\cr &=& P(\r\n\\hbox{one of the $k$ surviving soldiers by $t$ dies within $(t, t+h)$}\r\n \\, | \\,\r\n\\hbox{\r\n$k$ soldiers survive up to time $t$})\r\n\\cr &=& k\r\nP(\\xi_1 \\in (t, t+h), \\xi_2 \\geq t+h, ..., \\xi_k \\geq t+h\r\n\\, | \\, \\xi_1 \\geq t, ..., \\xi_k \\geq t)\r\n\\cr &=&\r\nk  P(\\xi_1 \\in (t, t+h)|\\xi_1 \\geq t) \\prod_{j=2}^k P(\\xi_j \\geq t+h | \\xi_j\\geq t)\r\n\\cr &=&\r\nk  [ e^{-\\alpha t} - e^{-\\alpha (t+h) } ]/\r\n e^{-\\alpha t} \\prod_{j=2}^k e^{-\\alpha (t+h) }/ e^{-\\alpha t}\r\n\\cr &=&\r\nk [1- e^{-\\alpha h}] e^{-(k-1) \\alpha h}\r\n\\cr &=&\r\nk \\alpha h +o(h).\r\n\\end{eqnarray*}\r\n Similarly, one can show (please DIY)\r\n$$P(X(t+h) - X(t) = 0 |X(t) = k) = 1-k\\alpha h + o(h).$$\r\nThen, $X(\\cdot)$ is a pure death process with linear rates\r\n$\\mu_k = k\\alpha$.\r\n\r\nTo compute the marginal distribution of $X(t)$, write\r\n\\begin{eqnarray*}  P(X(t)=k)\r\n  &=&\r\nP(\\hbox{$k$ of the $n$ iid r.v.s $\\xi_1,..., \\xi_n$ are $\\geq t$\r\nand $n-k$ of them are $< t$})\r\n\\cr &=&\r\n{N \\choose k} P(\\xi_i > t)^k\r\nP(\\xi_i \\leq t)^{N-k} = {N \\choose k} e^{-\\alpha t k}\r\n(1- e^{-\\alpha t})^{N-k}\r\n\\end{eqnarray*}\r\nfor $k=N, N-1, ..., 0$.\r\n\r\nRecall that $W_k = S_N + S_{N-1} + ... +S_{N-k+1}$,\r\n $ k=1,...N$,\r\ndenote  the waiting times. Set $W_0 = 0$.\r\nThen, $W_N = S_N + ...+S_1$ is the time of the death\r\nof the last member of the population. In other words,\r\n$W_N$ is {\\it the time to extinction}.\r\n$$P(W_N < t) = P(X(t) =0 ) = {N \\choose 0} e^{-\\alpha t\r\n\\times 0} (1- e^{-\\alpha t})^{N-0}\r\n= (1-e^{-\\alpha t})^N.$$\r\nAlternatively,\r\n$$P(W_N < t) = P(\\xi_1 < t, ..., \\xi_N < t) =\r\n(1- e^{-\\alpha t})^N.$$", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No solution for an example", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [96], "rightproblems": [], "wrongproblems": [], "twinproblems": [194]}}, {"model": "mathematics.question", "pk": 184, "fields": {"code": "6.1.5", "category": 1, "problem": "Australian rabbits, since they were first introduced by the Europeans in 18th century, experienced\r\nexplosive growth in the ensuing decades and caused devastating\r\necological and environmental disaster, accounting for about $70\\%$\r\nof the loss of plant species and land erosion.\r\nBecause of lack of significant natural enemies (perhaps excluding humans), the deciding factor, aside from randomness, of population growth/decrease is precisely\r\npopulation size itself. Namely, small (large) population implies abundant (scarce)\r\nenvironmental food supply per rabbit. Such a process of\r\npopulation along time can be properly modeled by birth and death process.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No solution for an example", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 185, "fields": {"code": "7.1.4", "category": 1, "problem": "Suppose the inter-occurrence times $X_1,X_2,...$ are iid $\\sim$ $F$, with density\r\n$f(x) = \\theta - x \\theta^2/2$ for $0 \\leq x \\leq 2/\\theta$; where $\\theta >0$.\r\nThen,\r\n$$F(x) = \\int_0^x f(s)ds = \\theta x - x^2\\theta^2/4; \\qquad 0 \\leq x \\leq 2/\\theta.$$\r\nAnd $\\mu = E(X_1) = 2/(3\\theta) $ and $\\sigma^2= 2/(9\\theta^2)$.\r\n\r\n$N(t)$ is a renewal process based on $X_1,X_2,...$.\r\nThen, as $ t \\to \\infty$,\r\n\r\n(1). (Elementary renewal theorem)\r\n$$M(t)/t \\to 1/\\mu = 3\\theta/2;$$\r\n\r\n(2). (Refined renewal theorem),  moreover,\r\n$$M(t) = t/\\mu + { \\sigma^2  - \\mu^2 \\over 2 \\mu^2} + o(1)\r\n= 3 \\theta t / 2 + {2/9 - 4/9 \\over 2\\times 4/9} + o(1)\r\n= 3 \\theta t/2 - 1/4 +o(1);$$\r\n\r\n(3). (Central limit theorem for the renewal process)\r\n$${N(t) - t/\\mu \\over  \\sqrt{ t\\sigma^2/\\mu^3}  }= {N(t) - 3\\theta t/ 2 \\over\r\n \\sqrt{ 3t\\theta/4}  }\\to N(0, 1)$$\r\n\r\n\r\n(4). (Asymptotic distribution for $\\gamma_t$ and $\\delta_t$)\r\n$$ P(\\gamma_t < x) \\to\r\n{1 \\over \\mu} \\int_0^x (1-F(s))ds\r\n= {3\\theta \\over 2} \\int_0^x [1-\\theta s + {s^2\\theta^2 \\over 4}] ds\r\n= {3\\theta \\over 2}\\{ x- { \\theta x^2 \\over 2} - { \\theta^2 x^3 \\over 12} \\}$$\r\nfor $ 0 \\leq x \\leq 2/\\theta$.\r\nAs we know, $\\delta_t$ has the same limiting distribution as that of $ \\gamma_t$.\r\nFurthermore,\r\n$$\r\nE(\\beta_t) / \\mu \\to 1+ {\\sigma^2 \\over \\mu^2}\r\n\\to 1+ { 2/(9 \\theta^2) \\over 4/(9\\theta^2) } = 3/2.$$\r\nThis limit does not have to do with $\\theta$.\r\nThe same is true when $F(\\cdot)$ is the uniform distribution\r\non $[0, \\theta]$. However, if $F(\\cdot)$ is the uniform\r\ndistribution on $[\\theta, \\theta+1]$ for\r\nThen, larger the $\\theta$, the smaller the upward bias\r\nof the total life time relative to the ordinary life time.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No solution for an example", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [80], "rightproblems": [], "wrongproblems": [186], "twinproblems": []}}, {"model": "mathematics.question", "pk": 186, "fields": {"code": "7.1.5", "category": 1, "problem": "Suppose $Y_t; t=0, 1, 2,...$ is a  Markov chain with state space being\r\n$\\{0, 1\\}$ and transition matrix:\r\n$${\\bf P}= \\pmatrix{P_{00} & P_{01} \\cr\r\nP_{10} & P_{11} } = \\pmatrix{0.4 & 0.6 \\cr 0.7 & 0.3}. $$\r\n$Y_0=0$.\r\nThe limiting probabilities $\\pi_0, \\pi_1$ satisfy\r\n$$\\cases{ ( \\pi_0  \\,\\,\\, \\pi_1) = (\\pi_0 \\,\\,\\, \\pi_1) {\\bf P} \\cr\r\n\\pi_0 + \\pi_1 =1}$$\r\nAnd the solutions are $\\pi_0 = 7/13 $ and $\\pi_1=6/13$.\r\nThen,\r\n$$\\lim_{t \\to\\infty} P(Y_t = 0) = 7/13 \\qquad\\hbox{and} \\qquad\r\n\\lim_{t \\to\\infty} P(Y_t = 1) = 6/13.$$\r\nMoreover, our interpretation of the long run behavior of Markov chains\r\nalso implies that, as $n \\to \\infty$,\r\n$$ { \\# \\# \\{i \\leq n: Y_i = 0 \\} \\over n } \\to\r\n\\lim_{t \\to\\infty} P(Y_t = 0) = 7/13. .......(7.11)$$\r\nIn addition,\r\n$$ { \\#\\#\r\n\\{i \\leq n, Y_i=0, Y_{i-1}=1 \\} \\over n }\r\n\\to \\lim_{t \\to \\infty} P(Y_{t-1}=1, Y_t = 0)\r\n=P_{1 0} \\lim_{t \\to \\infty} P(Y_{t-1}=1)\r\n= 0.7   \\pi_1 = 21/65\r\n......(7.12)\r\n$$\r\n\r\n\r\n\r\n(1).\r\nSet $N(t) = \\#\\#\\{ 0 < i \\leq t , Y_i =0\\} $ for $ t \\geq 1$ and $N(0)=0$.\r\nThen, $N(t)$ is the total number of times the Markov Chain $\\{Y_1, ..., Y_t\\}$ stays\r\nat state $0$.\r\nThe above interpretation implies\r\n$$N(t)/t \\to 7/13.$$\r\nWe now try to approach a similar result from the perspective of a renewal process.\r\nFirst, we verify that $N(\\cdot)$ is indeed a renewal process.\r\nSet $W_0=0$, and\r\n$W_1= \\min\\{t > 0: Y_t=0\\}$,\r\n$W_2= \\min\\{t > W_1: Y_t=0\\}$, ...\r\n$W_{k+1}= \\min\\{t > W_k: Y_t=0\\}$,...\r\nAnd let\r\n$X_k= W_k-W_{k-1}$ for $k=1,2,...$.\r\nThen,\r\n$W_k$ is the $k$-th time that the Markov chain $Y_t$ visits\r\nstate $0$, counting from time 1 (not time 0).\r\nAnd $X_k$ is, certainly, the time period between the $k-1$-th\r\nand $k$-th visits of state 0.\r\nWith some technicalities it can be shown $X_k$ are iid positive\r\nrandom variables with distribution:\r\n$$ P(X_k=i) = \\cases{  0.4 & $i=1$ \\cr\r\n0.6 \\times 0.7 & $i=2$ \\cr\r\n\\vdots & $\\vdots$ \\cr\r\n0.6 \\times 0.3^{n-2} \\times 0.7 & $i=n$ , for  $n \\geq 3$\r\n }$$\r\n And $\\mu= E(X_k) = 13/7$.\r\n By renewal theorem,\r\n we have\r\n $$ E(N(t))/t \\to {1/\\mu}= 7/13.$$\r\n $$ {N(t)/t - 7/13 \\over \\sqrt{ \\sigma^2/(t \\mu^3)} } \\to N(0,1)$$\r\n which implies (7.11)\r\n\r\n\r\n(2). Set\r\n$$N(t)= \\sum_{i=1}^t I(Y_i=0, Y_{i-1}=1).$$\r\nThen $N(t)$ is   the number of times the Markov chian $\\{Y_t\\}$ returns to state $0$\r\nfrom state 1 over times $\\{1, 2, ..., t\\}$.\r\nTo understand why $N(\\cdot)$ is a renewal process. We set\r\n$W_0=0$.\r\n$W_1= \\min\\{ k \\geq 2: Y_k=0, Y_{k-1}=1\\},$\r\n$W_2= \\min\\{ k >W_1 : Y_k=0, Y_{k-1}=1\\},$...\r\n$W_{n+1}= \\min\\{ k > W_n : Y_k=0, Y_{k-1}=1\\},$...\r\nAnd let $X_n = W_n-W_{n-1}$ for $n \\geq 1$.\r\nThen $X_n$ are iid with\r\n$$P(X_n=k) = \\sum_{i=0}^{k-2}  0.4^i\\times 0.6\\times 0.3^{k-2-i} \\times 0.7,\r\n\\qquad k\\geq 2.$$\r\nAnd\r\n$$\\mu  = E(X_n) = 65/21.$$\r\nBy the renewal theorem,\r\nwe have, as $t \\to \\infty$,\r\n$$E(N(t))/t \\to 1/\\mu = 21/65.$$\r\n$$ { N(t)/t - 21/65 \\over \\sqrt{ \\sigma^2/(t \\mu^3)} } \\to N(0,1),$$\r\nwhich implies (7.12).", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No solution for an example", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [80], "rightproblems": [], "wrongproblems": [185], "twinproblems": []}}, {"model": "mathematics.question", "pk": 187, "fields": {"code": "5.1.1", "category": 4, "problem": "Can we use the moment generating function to derive the mean and variance of\r\na Poisson random variable?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes.", "choicesb": "No.", "choicesc": "Perhaps.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Suppose $X\\sim {\\cal P}(\\lambda)$. Its moment generating function\r\nis\r\n$$\\psi(t)\\equiv E(e^{tX})= \\sum_{n=0}^\\infty e^{tn} \\lambda^n e^{-\\lambda}/n! = e^{-\\lambda}\r\n\\sum_{n=0}^\\infty (e^{t} \\lambda)^n /n!\r\n=e^{-\\lambda + \\lambda e^t}.$$\r\nThen,\r\n$$ E(X)= {\\partial \\over \\partial t} \\psi(t)\\Bigl|_{t=0}\r\n=\\lambda, \\quad \\hbox{and} \\quad\r\nE(X^2)= {\\partial^2 \\over \\partial t^2}\\psi(t)\\Bigl|_{t=0}\r\n=\\lambda + \\lambda^2$$\r\nTherefore, $$var(X)= E(X^2) - (E(X))^2 = \\lambda + \\lambda^2 - \\lambda^2 = \\lambda.$$", "linkability1": 1.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Get yourself familiar with the probability of Poisson random variable and moment generating function!", "messagesuccess": "Great! You have mastered the probability of Poisson random variable and an application of moment generating function!", "sensitivity": 1.0, "gussingparameter": 0.333, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [62, 63, 64], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 188, "fields": {"code": "5.2.1", "category": 4, "problem": "Can we use binomial approximation to interpret the fact that the mean and\r\nvariance are equal for a Poisson random variable?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes.", "choicesb": "No.", "choicesc": "Perhaps.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "A Poisson distribution ${\\cal P}(\\lambda)$ is the limit\r\nof binomial distribution $Bin(n, \\lambda/n)$ as $n \\to \\infty$.\r\n$$\\qquad {\\textcolor[rgb]{1,0,0}{\\hbox{  (Binomial approximation)}} }$$\r\nFor binomial distribution, the mean is $n \\lambda/n = \\lambda$ and\r\nthe variance is $n (\\lambda/n)(1-\\lambda/n) \\to \\lambda$ as\r\n$n \\to \\infty$. It is therefore understandable that\r\n the Poisson distribution has equal mean and variance.\r\n (This is not a rigorous mathematical proof though.)", "linkability1": 1.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. It is related to Binomial approximation.", "messagesuccess": "Great! You have mastered the concept of Binomial approximation!", "sensitivity": 1.0, "gussingparameter": 0.333, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [64, 67], "rightproblems": [], "wrongproblems": [], "twinproblems": [189]}}, {"model": "mathematics.question", "pk": 189, "fields": {"code": "5.3.2", "category": 4, "problem": "Can you prove Proposition 5.2 (as shown in the following)?\r\n<br>\r\n<br>\r\nProposition 5.2 [Extended binomial approximation] \r\n<br>\r\nSuppose (a). $\\xi_{n, i}$ takes\r\nvalue $1 $ and $0$ with probability $p_{n,i} $ and $1-p_{n,i}$; (b).\r\n$\\xi_{n,1} , ...,\\xi_{n, n}$ are independent; (c). $\\max\\{p_{n, i}: 1 \\leq i \\leq n\\}\r\n\\to 0$ and $\\sum_{i=1}^n p_{n,i} \\to \\lambda$\r\nas $n \\to \\infty$ where $0< \\lambda < \\infty$.\r\nThen, as $n \\to \\infty$,\r\n$$\\sum_{i=1}^n \\xi_{n,i}  \\to {\\cal P}(\\lambda) \\qquad \\hbox{in distribution.}$$", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes.", "choicesb": "No.", "choicesc": "Perhaps.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "As with the famous central limit theorem,\r\nthe commonly adopted proof is via $\\textcolor[rgb]{1,0,0}{\\hbox{characteristic function}}$, which\r\nis the same as moment generating function except now $t$ is replaced\r\nby $\\textcolor[rgb]{1,0,0}{\\hbox{$s=\\sqrt{-1}t$}}$, where $\\sqrt{-1}$ is complex but $t$ is real.\r\nThe characteristic function of\r\n${\\cal P}(\\lambda)$ is $ e^{-\\lambda + e^s \\lambda}$.\r\nObserve that\r\n$$ E( e^{s \\sum_{j=1}^n \\xi_{n,j} }) =\r\n\\prod_{j=1}^n E(e^{s \\xi_{n,j}})= \\prod_{j=1}^n (1-p_{n, j} +p_{n,j} e^s)\r\n= e^{\\sum_{j=1}^n \\log(1+ p_{n, j}(e^s-1))}.$$\r\nFor small $x$,  $|\\log(1+x)- x | \\leq |x|^2$.\r\nHence, as $n \\to \\infty$, \r\n$$ E( e^{s \\sum_{j=1}^n \\xi_{n,j} }) =  e^{ (e^s-1) \\sum_{j=1}^n p_{n, j} }\r\n+o(1) = e^{ (e^s-1)\\lambda} +o(1).$$\r\nIn summary, the characteristic function of $\\sum_{j=1}^n \\xi_{n, j}$\r\ntends to that of ${\\cal P}(\\lambda)$. Then the distribution\r\nof $\\sum_{j=1}^n \\xi_{n, j}$ also converges to that\r\n${\\cal P} (\\lambda)$. \r\n<br>\r\n(This is citing a theorem in probability theory: a sequence\r\nof distributions converges to a limit distribution if and only if\r\ntheir characteristic functions converge to that of the limit distribution.)", "linkability1": 3.0, "linkability2": 3.0, "linkability3": 3.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. It is related to Binomial approximation and characteristic function.", "messagesuccess": "Great! You have mastered Binomial approximation and the application of characteristic function!", "sensitivity": 1.0, "gussingparameter": 0.333, "difficulty": 5, "calculateddifficulty": null, "linkneuron": [65, 67], "rightproblems": [], "wrongproblems": [], "twinproblems": [188]}}, {"model": "mathematics.question", "pk": 190, "fields": {"code": "6.1.1", "category": 4, "problem": "Does Postulate 1 (as shown in the following) for Poisson Process imply independent increment?\r\n<br>\r\n<br>\r\n$Postulate 1$. (via infinitesimal probabilities)\r\n<br>\r\n(i) $P(X(t+h)-X(t) =1 | X(t) = k) = \\lambda h + o(h)$\r\n<br>\r\n(ii) $P(X(t+h)-X(t) =0 | X(t) = k) = 1- \\lambda h + o(h)$\r\n<br>\r\n(iii) $X(0)=0$.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes.", "choicesb": "No.", "choicesc": "I have no idea.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Yes, but it must be coupled with $\\textcolor[rgb]{1,0,0}{\\hbox{the requirement that it be a $MC$}}$.\r\nNote that, with the Markovian property,  $Postulate 1$ implies\r\n\\begin{eqnarray*}\r\n&& P(X(t+\\Delta t) -X(t)=1|X(s) , 0 \\leq s \\leq t)=\\lambda h + o(h),\\\\\r\n\\quad \\hbox{and}\r\n\\quad && P(X(t+\\Delta t) -X(t)=0|X(s) , 0 \\leq s \\leq t)=1-\\lambda h +o(h).\r\n\\end{eqnarray*}\r\nDivide the interval $(s, t]$ into\r\nsubintervals $(s, s+h], (s+h, s+2h], ..., (s+(n-1)h, s+nh]$ where\r\n$h=(t-s)/n$ and $n$ is a {\\it large } integer.\r\nFor notational simplicity, let $Y_l= X(s+lh)-X(s+(l-1)h)$.\r\nBy Postulate 1,\r\nwe have, for any $0 \\leq s< t <\\infty$ and any nonnegative integers $i$ and $ j $,\r\n\\begin{eqnarray*}\r\n&& P(X(t)-X(s)=j| X(a), 0\\leq  a \\leq t) \\\\\r\n&=&\r\n \\sum_{0 \\leq k_1 <\\cdots < k_j \\leq n} k_1\r\n P( Y_1=0,...,Y_{k_1-1}=0, Y_{k_1}=1, Y_{k_1+1}=0, ...,\r\n Y_{k_2-1}=0, Y_{k_2}=1, Y_{k_2+1}=0, \\\\\r\n && \\qquad ..., Y_{k_j-1}=0, Y_{k_j=1},\r\n Y_{k_j+1}=0,.., Y_n=0|X(a), 0\\leq  a \\leq t) +o(h)\r\n \\\\\r\n &=& \\sum_{0 \\leq k_1 <\\cdots < k_j \\leq n} (1-\\lambda h)\\cdots (1-\\lambda h) \\lambda h\r\n (1-\\lambda h) \\cdots\r\n \\\\\r\n && \\qquad (1-\\lambda h) \\lambda h\r\n (1-\\lambda h) \\cdots (1-\\lambda h) \\lambda h (1-\\lambda h) \\cdots (1-\\lambda h) +o(h)\r\n \\\\\r\n &=& {n \\choose j}\r\n (1-\\lambda h)^{n-j} (\\lambda h)^j +o(h)\r\n \\\\\r\n &\\to & (\\lambda (t-s))^j e^{- \\lambda(t-s)}/j! \\qquad \\hbox{as $n \\to \\infty$}\r\n .\r\n\\end{eqnarray*}\r\nThe conditional distribution of $X(t)-X(s)$ is independent of\r\n$X(a), 0 \\leq a \\leq s$, and, therefore, is independent of any\r\nincrements on interval $[0, s]$.", "linkability1": 3.0, "linkability2": 3.0, "linkability3": 3.0, "linkability4": 3.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, you may try it again.", "messagesuccess": "Great! You have mastered Postulate 1 of Poisson Process!", "sensitivity": 1.0, "gussingparameter": 0.333, "difficulty": 5, "calculateddifficulty": null, "linkneuron": [95], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 191, "fields": {"code": "6.1.3", "category": 4, "problem": "Does $P_n(t), n \\geq 1$ satisfying (6.1) also satisfy the\r\ndifferential equation (6.3)?\r\n\r\n$$ P_n(t) = \\lambda_{n-1} e^{-\\lambda_n t} \\int_0^t e^{\\lambda_n\r\ns} P_{n-1}(s) ds  \\qquad\\qquad \\qquad  (6.1)$$\r\n$$\\cases{ P_0'(t) = -\\lambda_0 P_0(t) \\cr\r\nP_n'(t) = - \\lambda_n P_n(t) + \\lambda_{n-1} P_{n-1}(t)  \\qquad n\r\n\\geq 1 } \\qquad (6.3)$$", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes.", "choicesb": "No.", "choicesc": "I have no idea.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "$\\textcolor[rgb]{1,0,0}{\\hbox{Rewrite (6.1)}}$ as\r\n$$    e^{\\lambda_n t} P_n (t)  = \\lambda_{n-1} \\int_0^t\r\ne^{\\lambda_n s} P_{n-1}(s) d s , \\qquad n \\geq 1, $$\r\nand $\\textcolor[rgb]{1,0,0}{\\hbox{differentiate  both sides}}$. It follows that\r\n$$\\lambda_{n }   e^{ \\lambda_n t}P_n(t) +  e^{ \\lambda_n t}P'_n(t)\r\n= \\lambda_{n-1} e^{\\lambda_n t} P_{n-1}(t)\r\n$$\r\nwhich is the same as   (6.3) for $n \\geq 1$.", "linkability1": 1.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try again. Here's a hint: you may first rewrite (6.1).", "messagesuccess": "Great! Now you may have a better understanding of Postulate 2 of Pure birth process!", "sensitivity": 1.0, "gussingparameter": 0.333, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [89, 110], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 192, "fields": {"code": "6.1.5", "category": 4, "problem": "In Example 6.1, can you show $\\lambda_k = (N-k)k \\alpha/(N-1)$ for $k=1,..., N$?\r\n<br>\r\n(Please refer to $\\backslash$Lecture notes$\\backslash$Chapter6 Part1)", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Yes.", "choicesb": "No.", "choicesc": "I have no idea.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "$X(t)=k$ means there are $k$ PPs and    $N-k$ normal people\r\non earth at time $t$.\r\nAny one ``lethal\" contact by a PP is to a normal people with chance\r\n$(N-k)/(N-1)$ and to PP with chance\r\n$(k-1)/(N-1)$. Here $N-1$ refers to the size of the entire earth population except\r\nfor the PP making the contact.\r\nTherefore an increment of the size of PPs by one due to a\r\nparticular PP within time $(t, t+\\Delta t)$ has probability\r\n$(N-k)/(N-1) \\alpha \\Delta t + o(\\Delta t)$. Write\r\n\\begin{eqnarray*}\r\n&& P(X(t+\\Delta t)-X(t)=1 | X(t)=k)\r\n\\\\\r\n&=& \\sum_{j=1}^k P(\\hbox{ One additional PP within\r\n$(t, t+\\Delta t]$  due to\r\nthe j-th existing PP } |X(t)=k) \\qquad \\textcolor[rgb]{1,0,0}{\\hbox{Slicely universe}}\r\n\\\\\r\n&=&  k (N-k)/(N-1) \\alpha \\Delta t  + o(\\Delta t).\r\n\\end{eqnarray*}\r\nHence, $\\lambda_k = k (N-k)\\alpha/(N-1)$.", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 2.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try it again.", "messagesuccess": "Great! You should have a better understanding of Pure birth process!", "sensitivity": 1.0, "gussingparameter": 0.333, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [86, 88], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 193, "fields": {"code": "6.2.2", "category": 4, "problem": "Suppose\r\n$\\{X(t): t \\in [0,\\infty)\\}$ is pure birth process with\r\n$X(0)=0$ and birth rates $\\lambda_0, \\lambda_1, ....$. $T$ is an\r\nexponentially distributed random variable with parameter\r\n$\\theta$, independent of the process $X(\\cdot)$. For any integer $K \\geq 1$,\r\ndescribe $P(X(T) \\geq K)$ by $K, i, \\lambda_i$, and $\\theta$.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\prod_{j=0}^{K}{ \\lambda_j \\over \\lambda_j + \\theta}$", "choicesb": "$\\prod_{j=0}^{K}{ \\lambda_j \\over \\lambda_j - \\theta}$", "choicesc": "$\\prod_{j=0}^{K-1}{ \\lambda_j \\over \\lambda_j + \\theta}$", "choicesd": "$\\prod_{j=0}^{K-1}{ \\lambda_j \\over \\lambda_j - \\theta}$", "choicese": "None of the above is correct.", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "\\begin{eqnarray*}\r\n&& P(X(T)\\geq K) = P(W_K \\leq T)\r\n\\\\&=&  \\int_0^\\infty P(T \\geq s|W_K=s) dP(W_k \\leq  s)\r\n= \\int_0^\\infty e^{-\\theta s } dP(W_k \\leq s)\r\n= E(e^{-\\theta W_K})\r\n\\\\&=&  E(e^{-\\theta (S_0+\\cdots+ S_{K-1})})\r\n=\\prod_{j=0}^{K-1} E(e^{-\\theta S_j})\r\n= \\prod_{j=0}^{K-1} \\int_0^\\infty e ^{-\\theta x} \\lambda_j e^{-\\lambda_j x} dx\r\n\\\\&=& \\prod_{j=0}^{K-1}{ \\lambda_j \\over \\lambda_j + \\theta}\r\n\\end{eqnarray*}", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "Please refer to Prob 6.2.1.", "messagefailure": "Oops, try it again. Hint: Use the concept of waiting time to convert the probability.", "messagesuccess": "Great! You should have a better understanding of Pure death process!", "sensitivity": 1.0, "gussingparameter": 0.2, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [96, 100, 101], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 194, "fields": {"code": "6.2.4", "category": 4, "problem": "In Expl 6.1.4, assume the life times of the $N$ Spartan soldiers are\r\niid. Let $X(t)$ be the number of surviving soldiers\r\nby time $t$, which statement below is correct?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Even if $X(t)$ is a pure death process, the common distribution of the soldier's life time is not exponential.", "choicesb": "$X(t)$ is a pure death process if and only if their common distribution is exponential.", "choicesc": "Even if the common distribution of the soldier's life time is exponential, $X(t)$ is not a pure death process.", "choicesd": "None of the above is correct.", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "If they are iid exponential, then $\\{X(t)\\}$ is known to be, not only\r\npure death, a linear pure death process. So the $\\textcolor[rgb]{1,0,0}{\\hbox{sufficiency}}$ is already shown in\r\nthe lecture notes. (Please refer to Expl 6.1.4.)\r\n\r\nLet $Y_1,..., Y_N$ be the life times of the $N$ Spartans which are iid with common\r\ncdf $F$.\r\n  If $\\{X(t)\\}$\r\nis pure death, then $S_0$ is exponential with parameter $\\mu_N$.\r\nThen, for any $t>0$,\r\n$$(1-F(t))^N = P(Y_1>t, ..., Y_N> t) = P(S_0>t)= e^{-\\mu_N t}.$$\r\nHence, $1-F(t)= e^{-\\mu_N t/N}$ is\r\nan exponential function in $t$, meaning that\r\n$Y_i$ follows exponential distribution with parameter\r\n$\\mu_N/N$. The $\\textcolor[rgb]{1,0,0}{\\hbox{necessity}}$ is proved.", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 2.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": 0.0, "errors": "", "alternativesolutions": "", "messagefailure": "Oops, try it again. You may review Expl 6.1.4.", "messagesuccess": "Well done! You have mastered the concept of Linear pure death process.", "sensitivity": 1.0, "gussingparameter": 0.25, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [96, 102, 105], "rightproblems": [], "wrongproblems": [], "twinproblems": [183]}}, {"model": "mathematics.question", "pk": 195, "fields": {"code": "5.2.1", "category": 3, "problem": "Let $X(n,p)$ have a binomial distribution with parameters $n$ and $p$. Let $n\\to\\infty$ and $p\\to0$ in such a way that $np=\\lambda$.  Calculate\r\n\\begin{equation*}\r\n\\lim_{n\\to\\infty}\\text{Pr}\\{X(n,p)=0\\}\r\n\\end{equation*}\r\nand \r\n\\begin{equation*}\r\n\\lim_{n\\to\\infty}\\frac{\\text{Pr}\\{X(n,p)=k+1\\}}{\\text{Pr}\\{X(n,p)=k\\}}\r\n\\end{equation*}", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "\\begin{equation*}\r\n\\lim_{n\\to\\infty}\\text{Pr}\\{X(n,p)=0\\}=\\lambda\r\n\\end{equation*}\r\n\\begin{equation*}\r\n\\lim_{n\\to\\infty}\\frac{\\text{Pr}\\{X(n,p)=k+1\\}}{\\text{Pr}\\{X(n,p)=k\\}}=\\frac{\\lambda}{k}\r\n\\end{equation*}", "choicesb": "\\begin{equation*}\r\n\\lim_{n\\to\\infty}\\text{Pr}\\{X(n,p)=0\\}=e^{-\\lambda}\r\n\\end{equation*}\r\n\\begin{equation*}\r\n\\lim_{n\\to\\infty}\\frac{\\text{Pr}\\{X(n,p)=k+1\\}}{\\text{Pr}\\{X(n,p)=k\\}}=\\frac{\\lambda}{k}\r\n\\end{equation*}", "choicesc": "\\begin{equation*}\r\n\\lim_{n\\to\\infty}\\text{Pr}\\{X(n,p)=0\\}=\\lambda\r\n\\end{equation*}\r\n\\begin{equation*}\r\n\\lim_{n\\to\\infty}\\frac{\\text{Pr}\\{X(n,p)=k+1\\}}{\\text{Pr}\\{X(n,p)=k\\}}=\\frac{\\lambda}{k+1}\r\n\\end{equation*}", "choicesd": "\\begin{equation*}\r\n\\lim_{n\\to\\infty}\\text{Pr}\\{X(n,p)=0\\}=e^{-\\lambda}\r\n\\end{equation*}\r\n\\begin{equation*}\r\n\\lim_{n\\to\\infty}\\frac{\\text{Pr}\\{X(n,p)=k+1\\}}{\\text{Pr}\\{X(n,p)=k\\}}=\\frac{\\lambda}{k+1}\r\n\\end{equation*}", "choicese": "None of the above", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "D", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "\\begin{equation*}\r\n\\lim_{n\\to\\infty}\\text{Pr}\\{X(n,p)=0\\}=(1-p)^n=(1-\\frac{\\lambda}{n})^n=e^{-\\lambda}\r\n\\end{equation*}\r\n\\begin{align*}\r\n\\lim_{n\\to\\infty}\\frac{\\text{Pr}\\{X(n,p)=k+1\\}}{\\text{Pr}\\{X(n,p)=k\\}}&=\\frac{\\frac{n!}{(k+1)!(n-k-1)!}p^{k+1}(1-p)^{n-k-1}}{\\frac{n!}{(k)!(n-k)!}p^{k}(1-p)^{n-k}}\\\\&=\\frac{n-k}{k+1}\\frac{p}{1-p}\\\\\r\n&=\\frac{n-k}{k+1}\\frac{\\lambda/n}{1-\\lambda/n}\\\\\r\n&\\approx\\frac{\\lambda}{k+1}\r\n\\end{align*}", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "An alternative solution is to derive the Poisson distribution from Binomial distribution. Please check Proposition 5.1 on lecture note. Here we provide another derivation using the Stirling approximation.\r\nFor binomial distribution with probability p and total number of events n, \r\n\\begin{equation*}\r\nP(k)=\\binom{n}{k}p^k(1-p)^{n-k}=\\frac{n!}{k!(n-k)!}p^k(1-p)^{n-k}\r\n\\end{equation*}\r\nSince $n$ is large, we take Stirling approximation to $n!$ and $(n-k)!$ so that\r\n\\begin{equation*}\r\nn!\\approx\\sqrt{2\\pi n}(\\frac{n}{e})^n \\qquad (n-k)!\\approx\\sqrt{2\\pi (n-k)}(\\frac{n-k}{e})^{n-k}\\approx\\sqrt{2\\pi n}(\\frac{n-k}{e})^{n-k}\r\n\\end{equation*}\r\nBefore we go into the algebra, keep in mind that $\\left(1+\\frac{a}{n}\\right)^n=\\left(1-\\frac{a}{n}\\right)^{-n}=e^a$. <br> What we do is to: <br>\r\n1. Turn $p$ into $\\frac{\\lambda}{n}$ <br>\r\n2. Group terms into power of $n$ or power of $k$\r\n\\begin{align*}\r\nP(k)&=\\frac{n!}{k!(n-k)!}p^k(1-p)^{n-k}\\\\\r\n&=\\frac{(\\frac{n}{e})^n}{k!(\\frac{n-k}{e})^{n-k}}p^k(1-p)^{n-k}\\\\\r\n&=\\frac{1}{k!}(\\frac{n}{n-k})^n(n-k)^ke^{-k}p^k(1-p)^{n}(1-p)^{-k}\\\\\r\n&=\\frac{1}{k!}(1-\\frac{k}{n})^{-n}(n-k)^ke^{-k}(\\frac{\\lambda}{n})^k(1-\\frac{\\lambda}{n})^{n}(1-\\frac{\\lambda}{n})^{-k}\\\\\r\n&=\\frac{1}{k!}e^{k}(n-k)^ke^{-k}(\\frac{\\lambda}{n})^ke^{-\\lambda}(1-\\frac{\\lambda}{n})^{-k}\\\\\r\n&=\\frac{1}{k!}e^{-\\lambda}(n-k)^k(\\frac{\\lambda}{n})^k(\\frac{n}{n-\\lambda})^{k}\\\\\r\n&=\\frac{1}{k!}e^{-\\lambda}(\\frac{n-k}{n-\\lambda})^k\\lambda^k\\\\\r\n&\\approx \\frac{e^{-\\lambda}\\lambda^k}{k!}\r\n\\end{align*}\r\nOnce we get the Poisson distribution, we have \r\n\\begin{equation*}\r\n\\lim_{n\\to\\infty}\\text{Pr}\\{X(n,p)=0\\}=\\frac{\\lambda^0}{0!}e^{-\\lambda}=e^{-\\lambda}\r\n\\end{equation*}\r\nand\r\n\\begin{equation*}\r\n\\lim_{n\\to\\infty}\\frac{\\text{Pr}\\{X(n,p)=k+1\\}}{\\text{Pr}\\{X(n,p)=k\\}}=\\frac{\\lambda^{k+1}}{\\lambda^k}\\frac{1/(k+1)!}{1/k!}=\\frac{\\lambda}{k+1}\r\n\\end{equation*}", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [62, 63, 65, 66, 68, 69], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 196, "fields": {"code": "5.2.4", "category": 3, "problem": "Suppose that N points are uniformly distributed over the interval\r\n$[0, N)$. Determine the probability distribution for the number of points k in\r\nthe interval $[0, 1)$ as $N\\to\\infty$.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$1/N$", "choicesb": "$\\frac{e^{-k}}{k!}$", "choicesc": "$\\frac{e^{-1}}{k!}$", "choicesd": "$(\\frac{1}{N})^k$", "choicese": "None of the above", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Consider the question this way:\r\nFor each point, it has a probability p fall into the interval $[0,1)$ and probability 1-p fall outside the interval $[0,1)$. The uniformly distributed condition give you $p=1/N$. Therefore, it is just a binomial distribution. <br>\r\nAs N tends to infinity, you know $p=1/N$ go to 0, so it must converge to a Poisson distribution. $\\lambda=pN=1$. The distribution is, therefore, $\\frac{1^k}{k!}e^{-1}=\\frac{e^{-1}}{k!}$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [63, 66], "rightproblems": [], "wrongproblems": [], "twinproblems": [197]}}, {"model": "mathematics.question", "pk": 197, "fields": {"code": "5.2.7", "category": 3, "problem": "$N$ bacteria are spread independently with uniform distribution on a\r\nmicroscope slide of area A. An arbitrary region having area $a$ is selected for\r\nobservation. Determine the probability of $k$ bacteria within the region of\r\narea $a$. Show that as $N\\to\\infty$ and $a\\to0$ such that $(a/A)N\\to c$ ($0<c<\\infty$),\r\nthen $p(k)\\to e^{-c}c^k/k!$", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$P(k)=\\frac{(N)^ke^{-N}}{k!}$", "choicesb": "$P(k)=\\frac{((a/A)N)^ke^{-(a/A)N}}{k!}$", "choicesc": "$P(k)=\\frac{((a/A))^ke^{-(a/A)}}{k!}$", "choicesd": "$P(k)=aN/A$", "choicese": "None of the above", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The probability p of one bacteria being selected is $a/A$. As N go to infinity, it turns from a binomial distribution (selected vs not selected) to a Poisson distribution. $\\lambda=pN=(a/A)N$, so $P(k)=\\frac{((a/A)N)^ke^{-(a/A)N}}{k!}$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [], "wrongproblems": [], "twinproblems": [196]}}, {"model": "mathematics.question", "pk": 198, "fields": {"code": "5.1.1", "category": 1, "problem": "{Example 5.1} {Traffic accidents})<br>\r\nSuppose (1). The chance of one traffic accident on the Clear Water Bay\r\nroad on any one day is very small, so small that more than one traffic\r\naccidents  one day is ignorable);  (2). Over different days,\r\nthe occurrence of traffic accidents\r\nare independent; and (3). On average, there are 3 traffic accidents\r\non the Clear Water Bay road per year.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "year. Then,\r\n$X$, the random number of traffic accidents over one year,\r\nfollows $Bin(365, p)$ with $p=3/365$. And $X$ follows approximately\r\nthe Poisson distribution with\r\nmean $3$, i.e.,\r\n$$ X \\sim {\\cal P}(3), \\qquad \\hbox{approximately}.$$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [62, 63, 66], "rightproblems": [], "wrongproblems": [], "twinproblems": [199, 201, 202, 205]}}, {"model": "mathematics.question", "pk": 199, "fields": {"code": "5.1.2", "category": 1, "problem": "For Chelsea's match against  Tottenham Hotspur\r\n this Saturday, suppose (1). the chance    for Chelsea to score a goal\r\nwithin any given one minute is very small; (Scoring 2 or more goals\r\nwithin one minute is so rare that it's ignorable.) (2). Over different minutes, scoring of\r\ngoals are independent.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Then the number of goals of Chelsea in the match\r\nis a Poisson random variable, a random variable following  a Poisson distribution.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [63, 66, 68], "rightproblems": [], "wrongproblems": [], "twinproblems": [198]}}, {"model": "mathematics.question", "pk": 200, "fields": {"code": "5.1.1", "category": 2, "problem": "Defects occur along the length of a filament at a rate of $\\lambda$ per foot.<br>\r\n(a) Calculate the probability that there are no defects in the first foot of the\r\nfilament.\r\n<br>(b) Calculate the conditional probability that there are no defects in the second\r\nfoot of the filament, given that the first foot contained a single defect.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$e^{-\\lambda}$;$e^{-\\lambda}$", "choicesb": "$e^{-\\lambda}$;$e^{-2\\lambda}$", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Easily see,the probability of no defect is equal to $Pr(X_{(1-foot)}=0)$=$e^{-\\lambda}$;<br>\r\nAnd  by the  indenpedence of$X_{(t1)}$and$X_{(t2)}-X_{(t1)}$,we get \r\n$Pr(X_{(2-foot)}-X_{(2-foot)}=0|X_{(1-foot)}=1)$= $Pr(X_{(2-foot)}-X_{(1-foot)}=0)$=$e^{-\\lambda}$.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [28, 62, 68, 70], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 201, "fields": {"code": "5.1.2", "category": 2, "problem": "Let $p_{k}=Pr(X=k)$ be the probability mass function corresponding to a Poisson\r\ndistribution with parameter $\\lambda$ . Compute $p_0$ , and show the relationship between  $p_{k}$ and $p_{k-1}$", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$e^{-\\lambda}$;$p_{k}$=$\\frac{\\lambda p_{k-1}}{k}$", "choicesb": ".$e^{-\\lambda}$;$p_{k-1}$=$\\frac{\\lambda p_{k}}{k}$", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Use the definition of the poisson  distrubution ,easy cheak that $p_k$=$\\frac{\\lambda^{k}*e^{-\\lambda}}{k!}$<br>\r\nSo we get $p_0$=$e^{-\\lambda}$<br>\r\n$p_{k}$=$\\frac{\\lambda p_{k-1}}{k}$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [63], "rightproblems": [], "wrongproblems": [], "twinproblems": [198]}}, {"model": "mathematics.question", "pk": 202, "fields": {"code": "5.1.3", "category": 2, "problem": "Let X and Y be independent Poisson distributed random variables with parameters\r\n$\\alpha$ and $\\beta$, respectively. Determine the conditional probability of $X=k$, given that $X+Y=N,N>K.$", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\frac{N!}{K!(N-K)!}*p^{K}*(1-p)^{N-K}$,where $p$=$\\beta$/($\\alpha$+$\\beta$)", "choicesb": "$\\frac{N!}{K!(N-K)!}*p^{K}*(1-p)^{N-K}$,where $p$=$\\alpha$/($\\alpha$+$\\beta$)", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Use the property that  the sum of the independent  poisson variable is poisson.<br>\r\nWe get $Pr(X+Y=N)$=$\\frac{\\lambda^{N}*e^{-\\lambda}}{N!}$,where $\\lambda$=$\\alpha$+$\\beta$.<br>\r\n So $Pr(X=K|X+Y=N)$=$Pr(X=K,X+Y=N)/Pr(X+Y=N)$=$Pr(X=K,Y=N-K)/Pr(X+Y=N)$=$Pr(X=K)*Pr(Y=N-k)/Pr(X+Y=N)$\r\n=$\\frac{N!}{K!(N-K)!}*p^{K}*(1-p)^{N-K}$,where $p$=$\\alpha$/($\\alpha$+$\\beta$),follow a bionomial distrubution", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [62, 63, 70], "rightproblems": [206], "wrongproblems": [], "twinproblems": [198, 206, 208, 209]}}, {"model": "mathematics.question", "pk": 203, "fields": {"code": "5.1.4", "category": 2, "problem": "Customers arrive at a service facility according to a Poisson process of rate \u0015\r\ncustomer/hour. Let $X_{t}$ be the number of customers that have arrived up to\r\ntime t.Consider fixed times 0 < s < t. Determine the conditional probability\r\n$P(X_{t}=n+k|X_{s}=n) $and the expected value E[$X_{t}X_{s}$].", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\frac{(\\lambda*(t-s))^{k}*e^{-\\lambda*(t-s)}}{k!}$;$\\lambda s$", "choicesb": "$\\frac{(\\lambda*(t-s))^{k}*e^{-\\lambda*(t-s)}}{k!}$;$(\\lambda t)^2$", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "$P(X_{t}=n+k|X_{s}=n) $=$P(X_{t}-X_{s}=k|X_{s}=n) $=$P(X_{t}-X_{s}=k) $=$\\frac{(\\lambda*(t-s))^{k}*e^{-\\lambda*(t-s)}}{k!}$.<br>\r\nE[$X_{t}X_{s}$]=E[$(X_{t}-X_{s})X_{s}$]+E[$X_{s}X_{s}$]=E[$(X_{t}-X_{s})$]E[$X_{s}$]+$(\\lambda s)^2$+$\\lambda s$\r\n=$\\lambda (t-s)*\\lambda s$+$(\\lambda s)^2$+$\\lambda s$=$(\\lambda t)^2$+$\\lambda s$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [62, 63, 70], "rightproblems": [], "wrongproblems": [], "twinproblems": [204]}}, {"model": "mathematics.question", "pk": 204, "fields": {"code": "5.1.5", "category": 2, "problem": "Suppose that a random variable X is distributed according to a Poisson distribution\r\nwith parameter$\\lambda$ \u0015. The parameter $\\lambda$ \u0015 is itself a random variable, exponentially\r\ndistributed with density$f(x)=\\theta*e^{-\\theta x}$. Find the probability mass function for X", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\theta$", "choicesb": "$\\frac{e^{-\\theta}}{\\theta^{N-1}}$", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Directly compute the intergal  $\\int_{0}^{\\infty}\\theta*e^{-\\theta x}*\\frac{x^{N}*e^{-x}}{N!}*dx$=$\\frac{e^{-\\theta}}{\\theta^{N-1}}$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [26, 62, 64], "rightproblems": [], "wrongproblems": [], "twinproblems": [203]}}, {"model": "mathematics.question", "pk": 205, "fields": {"code": "5.1.6", "category": 2, "problem": "Messages arrive at a telegraph office as a Poisson process with mean rate of\r\n3 messages per hour.What is the probability that no messages arrive during the morning hours 8:00 a.m. to noon?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$e^{-3}$", "choicesb": "$e^{-12}$", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Easy to see,$\\lambda=3$<br>\r\nthe probability $p=\\frac{\\(lambda t)^{k}*e^{-\\lambda t}}{k!}$=$e^{-12}$.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [68], "rightproblems": [207], "wrongproblems": [], "twinproblems": [198, 207]}}, {"model": "mathematics.question", "pk": 206, "fields": {"code": "5.1.7", "category": 2, "problem": "Suppose that customers arrive at a facility according to a Poisson process having\r\nrate  $\\lambda$. Let X(t) be the number of customers that have arrived up to time t.\r\nDetermine the  probabilitie $Pr(X(1)=2)$  and $Pr(X(1)=2|X(3)=6)$.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\frac{\\lambda^{2}*e^{-\\lambda}}{2}$?$\\frac{40}{81}$", "choicesb": "$\\frac{\\lambda^{3}*e^{-\\lambda}}{2}$?$\\frac{43}{81}$", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "$Pr(X(1)=2)$=$\\frac{(\\lambda t)^{k}*e^{-\\lambda t}}{k!}$=$\\frac{\\lambda^{2}*e^{-\\lambda}}{2}$<br>\r\n$Pr(X(1)=2|X(3)=6)$=$Pr(X(1)=2,X(3)=6)$/$Pr(X(3)=6)$=$Pr(X(1)=2,X(3)-X(1)=4)$/$Pr(X(3)=6)$=$\\frac{6!}{2!4!}*(\\frac{1}{3})^{2}*(\\frac{2}{3})^{4}$.\r\nWe can see this is\r\nresult of exercise 5.1.3", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [62, 65], "rightproblems": [202], "wrongproblems": [], "twinproblems": [202]}}, {"model": "mathematics.question", "pk": 207, "fields": {"code": "5.1.8", "category": 2, "problem": "Let$ \\{X(t):t\\geq0\\}$ be a Poisson process having rate parameter \u0015 $\\lambda$. Determine\r\nthe numerical values to two decimal places for the following probabilities:$Pr(X(1)\\leq2)$;$Pr(X(1)\\geq2|X(1)\\geq1)$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$(1+\\lambda+\\lambda^{2}/2)*e^{-\\lambda }$;1", "choicesb": "0.5;$\\frac{1- (1+\\lambda )e^{-\\lambda }}{1- e^{-\\lambda }}$", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "On one hand,<br>$Pr(X(1)\\leq2)$=$Pr(X(1)=2)$+$Pr(X(1)=1)$+$Pr(X(1)=0)$=$(1+\\lambda+\\lambda^{2}/2)*e^{-\\lambda }$;<br>\r\nOn the other hand ,<br>\r\n$Pr(X(1)\\geq2|X(1)\\geq1)$=$Pr(X(1)\\geq2,X(1)\\geq1)/Pr(X(1)\\geq1)$=$Pr(X(1)\\geq2)/Pr(X(1)\\geq1)$=$\\frac{1- (1+\\lambda )e^{-\\lambda }}{1- e^{-\\lambda }}$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [65, 70], "rightproblems": [205], "wrongproblems": [], "twinproblems": [205]}}, {"model": "mathematics.question", "pk": 208, "fields": {"code": "5.1.9", "category": 2, "problem": "Let$ \\{X(t):t\\geq0\\}$ be a Poisson process having rate parameter \u0015 $\\lambda$. Determine\r\n the following expectation?E[$X(2)$];E[$X(2)X(1)$]", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "4?24", "choicesb": "2;12", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "E[$X(2)$] is equal to the expectation of a poisson,so we get <br>\r\nE[$X(2)$]=$\\lambda t$=4;<br>\r\nE[$X(2)X(1)$]=E[$X(1)^2$]+E[$(X(2)-X(1))X(1)$]=$(\\lambda *2)^2+2*\\lambda$+$\\lambda^2$=24", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [7, 63, 65], "rightproblems": [], "wrongproblems": [], "twinproblems": [202]}}, {"model": "mathematics.question", "pk": 209, "fields": {"code": "5.2.1", "category": 2, "problem": "Determine numerical values to three decimal places for $Pr(X=1)$ for:<br>\r\n(a)X has a binomial distribution with parameters $n=40$ and p=0.03\r\n(b)X has a Poisson distribution with parameter \u0015 $\\lambda$=1.2", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$1.2*(1-0.03)^{39}$?$1.2*e^{-1.2}$", "choicesb": "$1.2*(0.98)^{39}$;$1.2*e^{-1.2}$", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Directly compute:<br>\r\n First one,$p=40*0.03*(1-0.03)^{39}$<br>\r\n Second one,$p=1.2*e^{-1.2}$<br>\r\n use the limiting equation  $\\lim_{x->0}(1-x)^{1/x}=e^{-1}$,we can see the two value  should be quite close.(Consider the theorem on  the notes!)", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [66, 68], "rightproblems": [], "wrongproblems": [], "twinproblems": [202, 210]}}, {"model": "mathematics.question", "pk": 210, "fields": {"code": "5.2.2", "category": 2, "problem": "Explain in general terms if  the following\r\nrandom variables follow a Poisson distribution?Think about why.<br>\r\n(a)The number of customers that enter a store in a fixed time period.<br>\r\n(b)The number of atomic particles in a radioactive mass that disintegrate in a\r\nfixed time period.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "both a and b", "choicesb": "a", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Easy to see,as the indenpendece of every occurence and the markov property(which means,the waiting time should follow a \r\nexponetial distrubution!)", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [66], "rightproblems": [], "wrongproblems": [], "twinproblems": [209]}}, {"model": "mathematics.question", "pk": 211, "fields": {"code": "6.1.1", "category": 1, "problem": "In a SciFi movie ``{\\it  Invasion}\"<br>\r\n  (2007, starring Nicole Kidman and Daniel Craig, adapted from the 1956\r\n    novel {The Body Snatchers} by Jack Finney.),\r\n  an alien life form affects the earth people and transforms them\r\n  into alien-like emotionless humankind, pod people or PP. Suppose,\r\n  every PP,  independent of everything else,\r\n  at any time $(t, t+\\Delta t)$ has chance $\\alpha \\Delta t$\r\n  to make a ``lethal\" contact with one random person, normal or PP, and transform\r\n  the person into PP if he/she is normal, and nothing happens if\r\n  he/she is already a PP.<br>\r\n  (continued) Let $N$ be the total population of\r\nthe earth.\r\nLet $X(t)$ be the number of PP by time $t$ beginning with the time\r\nof the first infection upon the crash of the space shuttle Columbia,\r\nand suppose $X(0)=1$.\r\nThen, (DIY)\r\n$$P(X(t+\\Delta t) - X(t) = 1 |X(t)=k) = (N-k) k \\alpha /(N-1) \\Delta t +\r\no(\\Delta t). $$\r\nThen, $\\{X(t): t \\in [0, \\infty)\\}$ is a pure birth process\r\nwith birth rates $\\lambda_k = (N-k)k\\alpha/(N-1)$ for $k=1,..., N$;\r\nand $\\lambda_k=0$ for $k \\geq N$.\r\nIt is seen that  growth rates   increase till half of the population\r\nis PP and decrease afterwards.\r\nFrom Postulate 3, we know $S_k \\sim $ exponential distribution with\r\nmean $1/\\lambda_k$. The first time the entire population on earth is\r\nPP is $S_1+\\cdots+S_{N-1}$ which has mean\r\n$$\\sum_{k=1}^{N-1} 1/\\lambda_k =\r\n\\sum_{k=1}^{N-1} {N-1 \\over \\{N-k)k\\alpha } = {N-1\\over N \\alpha}\r\n\\sum_{k=1}^{N-1} ( {1 \\over N-k} + {1 \\over k})\r\n= { 2(N-1)\\over N \\alpha } \\sum_{k=1}^{N-1} 1/k\r\n$$", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "As of March, 2009, the world population is about  $N= 6.76$ billion.\r\nLet $\\alpha=1$ and the unit of time is day, meaning that on average one day one\r\n``lethal\" contact from every one PP. Then,\r\nit takes on average about $2 \\sum_{k=1}^{N-1} 1/k /\\alpha = 46.4$\r\ndays for the entire earth population\r\nto turn PP.\r\n  Then, the number of PP on earth, along with time, follows a pure birth process\r\n  (but not a Poisson process).", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [86], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 212, "fields": {"code": "6.1.2", "category": 1, "problem": "The electron avalanche is a natural phenomenon, in which\r\n free electrons collide with\r\n  each other\r\n  in a strong electric field releasing \"new\" electrons\r\n  to undergo the same process in successive cycles.\r\nSuppose initially there are $m$ electrons in a ``jar\" $(X_0=m \\geq 2)$. At\r\nany time $(t, t+\\Delta t)$, if there are $k$ electrons in a jar,\r\nany pair of two electrons collide and generate a new electron\r\nwith probability $\\alpha \\Delta t +o(\\Delta t)$, independent of\r\neverything else.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Then, $X(t)$ is a pure birth process satisfying\r\n$$P(X(t+ \\Delta t) - X(t) = 1 |X(t)=k)=\r\n{k \\choose 2} \\alpha \\Delta t + o(\\Delta t) = {k(k-1)\\over 2} \\alpha \\Delta t +o(\\Delta t).$$\r\nHence it is a pure birth process with rates\r\n$\\lambda_k = (1/2) k(k-1)\\alpha $.\r\n As to be seen, the number of electrons\r\nconverge to infinity in finite time, causing avalanche or explosion.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [86], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 213, "fields": {"code": "6.1.1", "category": 2, "problem": "A pure birth process starting from $X_{0}=0$ has birth parameters $\\lambda_0$=1,\r\n\u0015$\\lambda_1=3$;\u0015$\\lambda_2=2$ and $\\lambda_3=5$. Determine the $P_{3}(t)$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$e^{-t}$", "choicesb": "$e^{-15t}$", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Use the  equation on the book:<br>\r\n$P_{0}^{'}(t)=- \\lambda_{0}P_{0}(t)$,<br>\r\n$P_{n}^{'}(t)=- \\lambda_{n}P_{n}(t)+\\lambda_{n-1}P_{n-1}(t)$ for $n\\geq1$<br>\r\n\r\n$P_n(t)=\\lambda_0\\cdots \\lambda_{n-1}\\left[\r\nB_{0,n}e^{-\\lambda_0t}+\\ldots+B_{n,n}e^{-\\lambda_nt}\\right],$ $B_{k,n}=$\r\n$1\\over\r\n{(\\lambda_0-\\lambda_k)\\cdots(\\lambda_{k-1}-\\lambda_k)(\\lambda_{k+1}-\r\n\\lambda_k)\\cdots(\\lambda_n-\\lambda_k)}$<br>\r\nSo apply value we get $P_{3}(t)$=$-e^{-t}-7.5e^{-3t}+10e^{-2t}+5e^{-5t}$<br>\r\nTo be honest,this type of problem is ugly.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [96], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 214, "fields": {"code": "6.1.2", "category": 2, "problem": "A pure birth process starting from $X_{0}=0$ has birth parameters $\\lambda_0$=1\r\n$\\lambda_1=3$;\u0015$\\lambda_2=2$ and $\\lambda_3=5$. Let $W_{3}$ be the random time that it takes the process\r\nto reach state 3.What is the variance of $W_{3}$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$49/36$", "choicesb": "$21/15$", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "$S_k$ are mutually independent with variance $1/\\lambda_k^2$.\r\n $$\r\n   Var(W_3)= Var(S_0+S_1+S_2)  =\r\n     Var(S_0)+Var(S_1)+Var(S_2)\r\n  = {1/{1^2}}+{1/ {3^2}}+{1/ {2^2}}\r\n ={ 49/36}. $$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 215, "fields": {"code": "6.1.3", "category": 2, "problem": "A population of organisms evolves as follows. Each organism exists, independent\r\nof the other organisms, for an exponentially distributed length of time with\r\nparameter \u0012$\\theta$, and then splits into two new organisms, each of which exists, independent\r\nof the other organisms, for an exponentially distributed length of time\r\nwith parameter $\\theta$ \u0012, and then splits into two new organisms, and so on. Let $X_{t}$\r\ndenote the number of organisms existing at time t. Is  $X_{t}$  a Yule\r\nprocess?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "yes", "choicesb": "no", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "(This problem, although conceptually clear, as in our\r\nvampire example, is not easy to articulate.) \\\r\nLet $\\xi$ be the life time of a particle before splitting.\r\nThen $\\xi$ follows exponential distribution with parameter $\\theta$,\r\nindependent of everything else including its birth time, the time\r\nit's generated in a split, denoted as  $T$.\r\nThis particle, given it exists at $t$,\r\nsplits  in time interval $(t, t+\\Delta t)$,\r\nwith probability\r\n\\begin{eqnarray*}\r\n&&\r\n\\int_0^t P(\\xi \\in (t-s, t-s + \\Delta t )|\\xi > t-s, T = s) f_T(s) ds / P(T \\leq t)\r\n\\\\\r\n&=&\r\n\\int_0^t P(\\xi \\in (t-s, t-s+ \\Delta t|\\xi>t-s) f_T(s) ds /P(T\\leq t)\r\n\\\\\r\n&=&\\int_0^t \\theta e^{-(t-s)\\theta} \\Delta t / e^{-(t-s)\\theta} f_T (s)ds\r\n/P(T \\leq t) + o(\\Delta t)\r\n\\\\\r\n&=& \\theta \\Delta t + o(\\Delta t).\r\n\\end{eqnarray*}\r\nAnd, by the same token,\r\nit does not split in interval $(t, t+\\Delta t)$ with probability\r\n$1- \\theta \\Delta t +o(\\Delta t)$.\r\nThen,\r\n\\begin{eqnarray*}\r\n&&P(X(t+\\Delta t)- X(t)=1| X(t)=k)\r\n\\\\\r\n&=&  \\sum_{j=1}^k P(\\hbox{\r\nthe $j$-th existing particle splits in $(t, t+\\Delta t)$\r\nand others don't})\r\n\\\\ &=&\r\nk(\\theta \\Delta t + o(\\Delta t))(1-\\theta \\Delta t + o(\\Delta t))^{k-1}\r\n= k \\theta \\Delta t +o(\\Delta t).\r\n\\end{eqnarray*}\r\nSimilarly,\r\n$$ P(X(t+\\Delta t)- X(t)=1| X(t)=k) =\r\n1- k \\theta \\Delta t + o(\\Delta t).$$\r\nAs a result, $\\{X(t)\\}$ is a Yule process.<br>\r\n(Think about it, it's like the sum of poisson procees in some sense)", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [102], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 216, "fields": {"code": "6.1.4", "category": 2, "problem": "Consider an experiment in which a certain event will occur with probability \u000b$\\alpha h$\r\nand will not occur with probability $1-\\alpha \u000bh$, where \u000b is a fixed positive parameter\r\nand h is a small .$h < 1\u000b/\\alpha$.positive variable. Suppose that n independent trials\r\nof the experiment are carried out, and the total number of times that the event\r\noccurs is noted. What is the  probability that the event occurs twice or more?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "o(1)", "choicesb": "o(h)", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "We shall use that fact that $(1+x)^m= 1+mx +o(x)$ for $ x \\to 0$.\r\n\r\n$\r\n\\mbox{that the event never occurs during the n trials} =(1-\\alpha h)^n\r\n=1-n\\alpha h+ o(h).\r\n$\r\n\r\n\\begin{eqnarray*}\r\n\\mbox{that the event occurs exactly once} = {n \\choose 1}(\\alpha h)(1-\\alpha h)^{n-1}\r\n% \\hskip 1cm (i.e. \\sim Bin(n,\\alpha h)) \\\\\r\n= n\\alpha h(1-(n-1)\\alpha h + o(h) )\r\n= n\\alpha h + o(h).\r\n\\end{eqnarray*}\r\nSo,\r\n$\r\np=1-[1-n\\alpha h+ o(h)]-[n\\alpha h+ o(h)] = o(h).\r\n$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [95], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 217, "fields": {"code": "6.1.5", "category": 2, "problem": "calculate the mean and variance for the Yule process\r\nwhere $X(0)= 1$", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$e^{\\beta t}$;$(1-e^{-\\beta t})*e^{2\\beta t}$", "choicesb": "$(1-e^{-\\beta t})*e^{2\\beta t}$;$e^{\\beta t}$", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Directly use the property of geometry distribution?\r\n$E(N(t))=e^{\\beta t}$;\r\n$var(N(t))=(1-e^{-\\beta t})*e^{2\\beta t}$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [95], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 218, "fields": {"code": "7.1.1", "category": 1, "problem": "{sc Poisson Process}<br> Let\r\n$X_1,X_2...,$ be iid $\\sim$ exponential distribution\r\nwith parameter $\\lambda$. Let\r\n$N(t) = \\max\\{n: \\sum_{i=0}^n X_i \\leq t,\\}$. ($X_0=0.$)\r\nThen, $\\{N(t): t \\in [0,\\infty)$ is a Poisson process and is also a renewal process.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": ".", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 219, "fields": {"code": "7.1.2", "category": 1, "problem": "A coin has probability $p$ to turn head and $1-p$ to turn tail when\r\nit is tossed. Let $\\xi_i=1$ if the $i$-th toss is a head and $0$ if the i-th toss\r\nis a tail. Set $\\xi_0=0$.\r\nDefine\r\n$$ N(t)= \\sum_{i=0}^t \\xi_i :  t=0, 1, 2, ...$$\r\nThen $N(t)$ is a discrete time renewal process. $N(t)$ is the\r\nnumber of heads till $t$-th toss.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The waiting time or event time $W_k$ is the time of the $k$-th head.\r\n $W_0=0$. The inter-occurrence time $X_k=W_k-W_{k-1}$ is\r\n the time periods before the $k-1$-th head and the $k$-th head.\r\n It can be shown that\r\n $X_1,X_2...$ are iid following a geometric distribution:\r\n $P(X_1=j)= p(1-p)^{j-1}, j\\geq 1$. (DIY).", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 220, "fields": {"code": "7.1.3", "category": 1, "problem": "Suppose $\\{Y_n, n \\geq 0\\}$ is a discrete time {MC} with state space\r\n $\\{0, 1, ..., N\\}$. Suppose $Y_0=k$. Set\r\n Let $Z_0=0$. $Z_i$ the $i$-th time the {MC} visits state $k$, $i\\geq 1$.\r\n Set $N(0)=0$ and $N(t)=\\max\\{n: Z_n \\leq t,\\}$, $t\\geq 1$,\r\n  the number of visits of state $k$\r\n before or at time $t$ (excluding the $0$-th time).\r\n Then $\\{N(t): t=0, 1, 2, \\}$ is a renewal process.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": ".", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 221, "fields": {"code": "7.1.1", "category": 2, "problem": "Verify the following equivalences for the age and the excess life in a renewal\r\nprocess N(t):\r\n <br> $\\gamma_{t} >x $ if and only if $ N(t+x)-N(t)=0$<br>\r\nand for $t>x>0$<br>\r\n$\\delta_{t}>x$ if and only if   $ N(t)-N(t-x)=0$\r\n<br>Think abou why  the condition  $x < t$ is important in the second case but not the first?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "both are right", "choicesb": "neither one is right", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Easy to see,as the definition of $\\gamma_{t}$ and $\\delta_{t}$.<br>\r\nThe condition is important because we don't know what happens whe time less than zero.In othe words,pay attetion of the direction of time.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [75], "rightproblems": [], "wrongproblems": [], "twinproblems": [222]}}, {"model": "mathematics.question", "pk": 222, "fields": {"code": "7.1.2", "category": 2, "problem": "Consider a renewal process in which the interoccurrence times have an exponential\r\ndistribution with parameter$\\lambda$:<br>\r\n$f(x)=\\lambda e^{-\\lambda x}$ and $F(x)=1-e^{-\\lambda x}$ for $x>0$<br>\r\nCaculate $F_{2}(t)$by carrying out the appropriate convolution and then determine$Pr(N(t)=1)$.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$1-e^{-\\lambda t}-\\lambda t e^{-\\lambda t}$;$\\lambda t e^{-\\lambda t}$", "choicesb": "$1-e^{-\\lambda t}$; $\\lambda t e^{-\\lambda t}$", "choicesc": "none of above is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "\\begin{eqnarray*}\r\nF_2(t)&=&\\int_0^tF_1(t-y)dF(y)=\\int_0^tF(t-y)dF(y) \\\\\r\n&=&\\int_0^t[1-e^{-\\lambda(t-y)}]\\lambda e^{-\\lambda y}dy \\\\\r\n&=&1-e^{-\\lambda t}-\\lambda t e^{-\\lambda t}. \\\\\r\nP(N(t)=1)&=&P(N(t)\\ge 1)-P(N(t)\\ge 2) \\\\\r\n&=&F_1(t)-F_2(t) \\\\\r\n&=&\\left(1-e^{-\\lambda t}\\right)-\\left(1-e^{\\lambda t}-\\lambda t\r\ne^{-\\lambda t}\\right)=\\lambda t e^{-\\lambda t}\r\n\\end{eqnarray*}<br>\r\nEasy to see,it's actually a poisson process", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [], "wrongproblems": [], "twinproblems": [221, 223]}}, {"model": "mathematics.question", "pk": 223, "fields": {"code": "7.1.3", "category": 2, "problem": "Which of the following are true statements?<br>\r\n(a) $N(t) < k$ if and only if $W_{k} > t.$<br>\r\n(b) $N(t) \\leq k$ if and only if  $W_{k} \\geq t.$<br>\r\n(c) $N(t)> k $ if and only if $Wk < t$", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "a", "choicesb": "b", "choicesc": "c", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Easy to verify,b is false as $N(t) \\leq k$ don't imply $W_{k} \\geq t.$(can stay at N) <br>\r\nc is flase because $Wk < t$ don't imply $N(t)> k $(can be equal).", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [], "wrongproblems": [], "twinproblems": [222, 224, 225]}}, {"model": "mathematics.question", "pk": 224, "fields": {"code": "7.1.4", "category": 2, "problem": "Consider a renewal process for which the lifetimes $X1,X2...$are discrete random\r\nvariables having the Poisson distribution with mean \u0015$\\lambda$That is<br>\r\n$Pr(X_{k}=n)=\\frac {e^{-\\lambda} \\lambda_{n}}{n!}$  for       $n=0,1...$<br>\r\nWhat is the distribution of the waiting time $W_{k}$ and $Pr(N(t)< k)$", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$P_0(k\\lambda)$;$P_0(k\\lambda)$", "choicesb": "0.5,0.5.", "choicesc": "none of them is correct.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "On one hand, <br>Since $X_1, X_2, \\ldots, X_k\\sim P_0(\\lambda)$,\r\n$  W_k=X_1+\\ldots+X_k\\sim P_0(k\\lambda). $\r\n\r\n<br>\r\n<br>On the other hand,\r\n\\begin{eqnarray*}\r\nP(N(t)=k)&=&P(N(t)\\ge k)-P(N(t)\\ge k+1) \\\\\r\n&=& P(W_k\\le t)-P(W_{k+1}\\le t) \\\\\r\n&=&\\sum_{i=0}^t{{e^{-k\\lambda}(k\\lambda)^i}\\over\r\n{i!}}-\\sum_{i=0}^t{{e^{-(k+1)\\lambda}[(k+1)\\lambda]^i}\\over {i!}}.\r\n\\\\\r\n\\end{eqnarray*}", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [75], "rightproblems": [], "wrongproblems": [], "twinproblems": [223]}}, {"model": "mathematics.question", "pk": 225, "fields": {"code": "7.3.1", "category": 2, "problem": "Let $W_{1},W_{2}...$ be the event times in a Poisson process $\\{X(t);t>0\\}$ of rate \u0015$\\lambda$.\r\nEvaluate\r\n$Pr(W_{N(t)+1}>t+s)$ and $E(W_{N(t)+1})$", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\lambda e^{-\\lambda s}$;$\\lambda+t$", "choicesb": "$\\lambda e^{-\\lambda s}$;$\\lambda$", "choicesc": "none of them is correct.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Actually,$W_{n}$ is just an arrival time of poisson distribution.<br>\r\nSo by the property of poisson process,,$W_{n(t)+1}-W_{n(t)}$  follow a exponential distribution with parameter $\\lambda$.\r\nSo, as no-memory property of the exponential distribution,we get $Pr(W_{N(t)+1}>t+s)=Pr(W_{N(t)+1}-t>s)$=$\\lambda e^{-\\lambda s}$.<br>\r\n Same way ,we get <br>\r\n$E(W_{N(t)+1})=E((W_{N(t)+1}-t)+t=\\lambda+t$<br>\r\nThink about that,what happen if it doesn't follow poisson distribution?<br>\r\nThere will be a function of t!", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [75], "rightproblems": [], "wrongproblems": [], "twinproblems": [223]}}, {"model": "mathematics.question", "pk": 226, "fields": {"code": "7.3.2", "category": 2, "problem": "Particles arrive at a counter according to a Poisson process of rate $\\lambda$\u0015. An arriving\r\nparticle is recorded with probability $p$ and lost with probability $1-p$ independently\r\nof the other particles. Is that the sequence of recorded particles  a\r\nPoisson process?If so,show the rate of it.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "yes?$\\lambda s$", "choicesb": "no", "choicesc": "none of them is correct", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $N(t)$ be the process of {\\it recorded } arriving\r\nparticles, $N^*(t)$ be the process of arriving particles, then\r\n$\\{N^*(t)\\} $ is a Poisson process with rate  $\\lambda$. Write\r\n\\begin{eqnarray*}\r\n &&P(N(t+\\triangle t)-N(t)=1|N(t)=k) \\\\\r\n &=&\r\n P(N^*(t+\\triangle t)-N^*(t)=1|N(t)=k)\\cdot p\r\n\\\\\r\n &=&   \\lambda p \\triangle t+o(\\triangle\r\nt). \\\\\r\n\\hbox{and } \\quad &&P(N(t+\\triangle t)-N(t)=0|N(t)=k) \\\\\r\n&=&P(N^*(t+\\triangle t)-N^*(t)=0|N(t)=k)\\cdot p\\\\\r\n&&+P(N^*(t+\\triangle\r\nt)-N^*(t)=1|N(t)=k)\\cdot (1-p) \\\\\r\n&=&(1-\\lambda \\triangle t)+(\\lambda \\triangle t)(1-p)+o(h) \\\\\r\n&=& 1-\\lambda p \\triangle t +o(\\triangle t).\r\n\\end{eqnarray*}\r\nAs a result, $\\{N(t)\\}$ is a Poisson process with rate $\\lambda p$.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [80, 95], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 227, "fields": {"code": "4.1.1", "category": 2, "problem": "A Markov chain $X_0,X_1,X_2,...$ has the transition probability matrix\r\n\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, 0 \\quad\\quad 1\\quad \\quad\r\n2\\ \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} }\r\n\\pmatrix{\r\n0.7 & 0.2  &0.1  \\cr\r\n0& 0.6 &0.4  \\cr\r\n0.5 & 0  & 0.5\r\n}\r\n\\end{eqnarray*}\r\nDetermine the limiting distribution", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "${\\bf \\pi}=(\\frac{10}{21},\\frac{5}{21},\\frac{6}{21})$", "choicesb": "${\\bf \\pi}=(\\frac{9}{21},\\frac{5}{21},\\frac{7}{21})$", "choicesc": "${\\bf \\pi}=(\\frac{11}{21},\\frac{4}{21},\\frac{6}{21})$", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "This is a question about the limiting distribution of a regular matrix. To solve this kind of problem, we just solve the equations below:<br>\r\n$\\pi_j=\\sum_{k=0}^N \\pi_kP_{kj}$\r\n$\\sum_j{\\pi_j}=1$\r\n<br>\r\nBy putting the values in, we get the equations for this question:<br>\r\n$\\pi_0=0.7\\pi_0+0+0.5\\pi_2$<br>\r\n$\\pi_1=0.2\\pi_0+0.6\\pi_1+0$<br>\r\n$\\pi_2=0.1\\pi_0+0.4\\pi_1+0.5\\pi_2$\r\n<br>\r\n\r\nAnother equations is given by the property of probability<br>\r\n$\\pi_0+\\pi_1+\\pi_2=1$<br>\r\nSolve them and get $\\pi_0=\\frac{10}{21},\\pi_1=\\frac{5}{21},\\pi_2=\\frac{6}{21}$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Oh no, you'd better refer to the content about the regular Markov chain and think it again.", "messagesuccess": "Congratulations! Now you can challenge the next problem.", "sensitivity": null, "gussingparameter": 0.33, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [35], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 228, "fields": {"code": "4.1.2", "category": 2, "problem": "A Markov chain $X_0,X_1,X_2,...$ has the transition probability matrix\r\n\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, 0 \\quad\\quad 1\\quad \\quad\r\n2 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} }\r\n\\pmatrix{\r\n0.6 & 0.3  &0.1  \\cr\r\n0.3& 0.3 &0.4  \\cr\r\n0.4 & 0.1  & 0.5\r\n}\r\n\\end{eqnarray*}\r\nDetermine the limiting distribution", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\pi_0=\\frac{33}{66},\\pi_1=\\frac{14}{66},\\pi_2=\\frac{19}{66}$", "choicesb": "$\\pi_0=\\frac{31}{66},\\pi_1=\\frac{16}{66},\\pi_2=\\frac{19}{66}$", "choicesc": "$\\pi_0=\\frac{33}{66},\\pi_1=\\frac{16}{66},\\pi_2=\\frac{17}{66}$", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "This is a question about the limiting distribution of a regular matrix. To solve this kind of problem, we just solve the equations below:<br>\r\n$\\pi_j=\\sum_{k=0}^N \\pi_kP_{kj}$\r\n$\\sum_j{\\pi_j}=1$\r\n<br>\r\nBy putting the values in, we get the equations for this question:<br>\r\n$\\pi_0=0.6\\pi_0+0.3\\pi_1+0.4\\pi_2$<br>\r\n$\\pi_1=0.3\\pi_0+0.3\\pi_1+0.1\\pi_2$<br>\r\n$\\pi_2=0.1\\pi_0+0.4\\pi_1+0.5\\pi_2$\r\n<br>\r\n\r\nAnother equations is given by the property of probability<br>\r\n$\\pi_0+\\pi_1+\\pi_2=1$<br>\r\nSolve them and get $\\pi_0=\\frac{31}{66},\\pi_1=\\frac{16}{66},\\pi_2=\\frac{19}{66}$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Oh no, you'd better refer to the content about the regular Markov chain and think it again.", "messagesuccess": "Congratulations! Now you can challenge the next problem.", "sensitivity": null, "gussingparameter": 0.33, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [35], "rightproblems": [], "wrongproblems": [231, 237], "twinproblems": [231, 235, 237, 238, 240, 241]}}, {"model": "mathematics.question", "pk": 229, "fields": {"code": "1.1.1", "category": 1, "problem": "De M\u00e9r\u00e9's Problem. ( Probability Space )\r\n<br>\r\nThe Chevalier de M\u00e9r\u00e9 was a French nobleman and a gambler of\r\nthe 17th century. He was interested in two gambling games:\r\nA: betting on At least one ace turns up with four\r\nrolls of a die; B: betting at least one double-ace turns up in 24 rolls\r\nof two dice.\r\nIt is tempting to believe the two events have same chance to occur.\r\nHis reasoning:\r\n<br>\r\n In one roll of a die, there is $1/6$ chance of getting an ace, so\r\n in 4 rolls, there is $4*(1/6)=2/3$ chance of getting at least one\r\n die.\r\n<br>\r\n In one roll of two dice there is $1/36$ chance of getting a double-ace,\r\n so in 24 rolls, there is $24*(1/36)=2/3$ chance of getting at\r\n least one double ace.\r\n<br>\r\nHow do you think of that?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "True", "choicesb": "Not True", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Therefore the two events, by this reasoning, have same chance. However,\r\n the chance computed here, $2/3$ obviously contracted his gambling experience\r\n that the chances are somewhat close to half, making the betting games a\r\n nearly fair game.\r\n<br>\r\nDe M\u00e9r\u00e9 turned to Blaise Pascal (1623-1662) and Pierre de\r\nFermat (1601-1665) for help. And the two mathematicians/physicists\r\npointed out that the above reasoning is erroneous and gave the right answer:\r\n<br>\r\n$P($getting at least one ace in 4 rolls of a die$) = 1- (1-1/6)^4=\r\n0.518$\r\n<br>\r\n4-rolls makes favorable bet, and 3-rolls makes not.\r\n<br>\r\n$P($getting at least one double-ace in 24 rolls of two dice$) = 1-\r\n(1-1/36)^{24}= 0.491$\r\n<br>\r\n25 rolls makes a favorable bet, 24 rolls make still an unfavorable bet.", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": 1.0, "linkneuron": [23], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 230, "fields": {"code": "1.1.2", "category": 1, "problem": "The St. Petersberg Paradox. ( Expectation )\r\n<br>\r\nA gambler pays an entry fee $M$ dollar to play following game: A fair\r\ncoin is tossed repeated until the first head occurs and you win\r\n$2^{n-1}$ amount of money where $n$ is the total number of tosses.\r\n<br>\r\nQuestion: what is the \"fair\" amount of $M$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Finite number.", "choicesb": "$\\infty$", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "$n$ is a random number with $P(n=k) = 2^{-k}$ for $k=1, 2, ...$.\r\nTherefore the ``Expected Winning\" is\r\n$$E(2^{n-1} ) = \\sum_{k=1}^\\infty 2^{k-1} \\times {1 \\over 2^k} = \\infty.$$\r\n{\\it Notice that here I have used the expectation of a function of\r\nrandom variable}. It appears that a  \"fair\", but indeed naive,\r\n$M$ should be $\\infty$. However, by common sense, this game,\r\ndespite its infinite payoff, should not worth the same as\r\ninfinity.", "linkability1": 1.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [31], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 231, "fields": {"code": "4.1.3", "category": 2, "problem": "A Markov chain $X_0,X_1,X_2,...$ has the transition probability matrix\r\n\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, 0 \\quad \\quad1\\quad \\quad\r\n2 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} }\r\n\\pmatrix{\r\n0.1 & 0.1 &0.8  \\cr\r\n0.2& 0.2 &0.6 \\cr\r\n0.3 & 0.3  & 0.4\r\n}\r\n\\end{eqnarray*}\r\nWhat fraction of time , in the long run, does the process spend in state 1?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\pi_1=\\frac{3}{13}$", "choicesb": "$\\pi_1=\\frac{7}{13}$", "choicesc": "$\\pi_1=\\frac{6}{13}$", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "This is a question about the limiting distribution of a regular matrix. To solve this kind of problem, we just solve the equations below:<br>\r\n$\\pi_j=\\sum_{k=0}^N \\pi_kP_{kj}$\r\n$\\sum_j{\\pi_j}=1$\r\n<br>\r\nBy putting the values in, we get the equations for this question:<br>\r\n$\\pi_0=0.1\\pi_0+0.2\\pi_1+0.3\\pi_2$<br>\r\n$\\pi_1=0.1\\pi_0+0.2\\pi_1+0.3\\pi_2$<br>\r\n$\\pi_2=0.8\\pi_0+0.6\\pi_1+0.4\\pi_2$\r\n<br>\r\n\r\nAnother equations is given by the property of probability<br>\r\n$\\pi_0+\\pi_1+\\pi_2=1$<br>\r\nSolve them and get $\\pi_1=\\frac{3}{13}$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Oh no, you'd better refer to the content about the regular Markov chain and think it again.", "messagesuccess": "Congratulations! Now you can challenge the next problem.", "sensitivity": null, "gussingparameter": 0.33, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [35], "rightproblems": [], "wrongproblems": [228, 235], "twinproblems": [228, 235]}}, {"model": "mathematics.question", "pk": 232, "fields": {"code": "1.1.3", "category": 1, "problem": "The dice game called \"craps\". ( Conditional probability )\r\n<br>\r\nTwo dice are rolled repeatedly and let the total dots of the\r\n$n$-th roll be $Z_n$. If $Z_1$ is 2 or 3 or 12, it is an immediate\r\nloss of the game. If $Z_1$ is 7 or 11, it is an immediate win.\r\nElse, continue the rolls of the two dice until either $Z_1$\r\noccurs, meaning Win, or $7$ occurs, meaning Loss.  What is the\r\nchance of  win of this game?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Larger than 1/2.", "choicesb": "Smaller than 1/2.", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Notice that $Z_1$, $Z_2$,... are iid, with\r\nprobabilities:\r\n<br>\r\n$ P(Z_i=2) = P(Z_i= 12)= {1\\over 36} $, $ P(Z_i=3) = P(Z_i= 11)={2\\over 36} $, $ P(Z_i=4) = P(Z_i= 10)={3\\over 36} $, $ P(Z_i=5) = P(Z_i= 9)={4\\over 36}  $, $ P(Z_i=6) = P(Z_i= 8)={5\\over 36}  $, $ P(Z_i=7) = {6\\over 36}  $\r\n<br>\r\nWrite\r\n\\begin{eqnarray*}   P(\\hbox{Win  }) &=& \\sum_{k=2}^{12}\r\nP(\\hbox{Win and } Z_1= k) = \\sum_{k=2}^{12} P(\\hbox{Win  } |Z_1=\r\nk) P(Z_1=k) \\\\\r\n&=& P(Z_1=7) +P(Z_1 = 11) + \\sum_{k=4, 5, 6, 8, 9, 10} P(\\hbox{Win\r\n} |Z_1= k) P(Z_1=k) \\\\\r\n&=& 6/36 + 2/36 + \\sum_{k=4, 5, 6, 8, 9, 10} P(A_k^c |Z_1= k)\r\nP(Z_1=k)\r\n\\\\\r\n&=& 2/9 + \\sum_{k=4, 5, 6, 8, 9, 10} P(A_k^c) P(Z_1=k)\r\n\\end{eqnarray*}\r\nwhere $A_k$ is the event that starting from the second roll, $7$\r\ndots occur before $k$ dots; and $A_k^c$ is the complement of $A_k$. Now,\r\n\\begin{eqnarray*}   P(A_k) &=& \\sum_{j=2}^{12} P(A_k \\cap \\{Z_2\r\n=j\\} ) = \\sum_{j=2}^{12} P(A_k | \\{Z_2 =j\\} )P(Z_2= j) \\\\\r\n&=& P(Z_2=7) + \\sum_{j=2 \\atop j\\not=k, j\\not=7 }^{12}\r\nP(\\hbox{starting from the 3rd roll, 7 occurs before $k$}) P(Z_2 =j\r\n)\r\n\\\\\r\n&=& 6/36 + P(A_k) ( 1-P(Z_2=k) - P(Z_2=7)).\r\n\\end{eqnarray*}\r\nAs a result,\r\n$$P(A_k) = {P(Z_1=7) \\over P(Z_1=k) + P(Z_1=7)} \\quad \\hbox{and}\\quad\r\nP(A_k^c) = {P(Z_1=k) \\over P(Z_1=k) + P(Z_1=7)}.$$\r\nAnd,\r\n\\begin{eqnarray*}\r\nP(\\hbox{Win}) &=& {2 \\over 9} +\\sum_{k=4, 5, 6, 8, 9, 10} {P(Z_1=k)\\over [P(Z_1=k) + P(Z_1=7)]}\r\nP(Z_1=k)  \\\\\r\n&=& {2 \\over 9} + 2\\Bigl( {(3/36)^2\\over (3+6)/36} +\r\n{(4/36)^2\\over (4+6)/36} +\r\n{(5/36)^2\\over (5+6)/36} \\Bigr)\r\n\\\\\r\n&=& {2 \\over 9} + {2 \\over 36} [ 1 + { 16 \\over 10} + {25 \\over 11}]\\\\\r\n&=& 488/990 = 0.4929293.\r\n\\end{eqnarray*}", "linkability1": 2.0, "linkability2": 3.0, "linkability3": 1.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [25], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 233, "fields": {"code": "1.1.4", "category": 1, "problem": "Jailer's reasoning. ( Bayes formula )\r\n<br>\r\nThree men, A, B and C are in jail and one to be executed\r\nand the other two to be freed. C, being\r\nanxious, asked the jailer to tell him who of A and B would be freed.\r\nThe jailer, pondering for a while, answered \u201cfor your own interest,\r\nI will not tell you, because, if I do, your chance of being executed\r\nwould rise from 1/3 to 1/2.\" Is it true with the jailer's reasoning?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "True", "choicesb": "Not True", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let AF  (BF) be the event that jailer $says$ A  (B) to be freed.\r\nLet  AE or  BE or CE be the event that A or B or C to be executed.\r\nThen, $P(CE)=1/3$. but, by the {\\it Bayes formula },\r\n\\begin{eqnarray*}\r\nP(CE|AF) &=& {P( AF|CE)P(CE) \\over P(AF|AE)P(AE)  + P(AF|BE)P(BE) + P(AF|CE)P(CE)}\r\n\\\\\r\n&=&{ 0.5 * 1/3 \\over 0*1/3 + 1* 1/3 + 1/2* 1/3 }\\\\\r\n&=&1/3 \\\\\r\n&=&P(CE).\r\n\\end{eqnarray*}\r\nLikewise $P(CE|BF) = P(CE)$. So the \u201crise of probability\" is false.", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [27], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 234, "fields": {"code": "1.1.5", "category": 1, "problem": "Buffon's needle. ( Continuous random variables and expectation)\r\n<br>\r\nRandomly drop a needle of 1 cm onto a surface of many parallel straight lines\r\nthat are 1 cm apart. What is the chance that the needle touches one of the lines?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$1 \\over 2$", "choicesb": "$1\\over \\pi$", "choicesc": "$2\\over \\pi$", "choicesd": "$2\\over 3\\pi$", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $x$ be the distance from the center of the needle to the nearest line. Let\r\n$\\theta$ be the smaller angle of the needle with the nearest line.\r\nThen, the needle crosses a line if and only if $x/sin(\\theta) \\leq 0.5$.\r\n\r\nIt follows from the randomness of the drop that   that $\\theta$ and $x$ are\r\nindependent following the uniform distribution on $[0, \\pi/2]$ and $[0, 1/2]$.\r\nTherefore,\r\n$$P(x/sin(\\theta) \\leq 0.5) = {4 \\over \\pi} \\int_0^{\\pi/2} \\int_0^{sin(\\theta)/2 } dx d\\theta\r\n={2 \\over  \\pi} \\int_0^{\\pi/2}  sin(\\theta)   d\\theta\r\n={2 \\over \\pi}.$$\r\nThe chance is $2/\\pi$.", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [24, 31, 34], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 235, "fields": {"code": "4.1.4", "category": 2, "problem": "A Markov chain $X_0,X_1,X_2,...$ has the transition probability matrix\r\n\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, 0 \\quad\\quad 1\\quad \\quad\r\n2 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} }\r\n\\pmatrix{\r\n0.3 & 0.2  &0.5  \\cr\r\n0.5& 0.1 &0.4  \\cr\r\n0.5 & 0.2  & 0.3\r\n}\r\n\\end{eqnarray*}\r\nEvery period that the process spends in state 0 incurs a cost of \\$2. Every period\r\nthat the process spends in state 1 incurs a cost of \\$5. Every period that the\r\nprocess spends in state 2 incurs a cost of \\$3. What is the long run cost per\r\nperiod associated with this Markov chain?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "2.905", "choicesb": "2.957", "choicesc": "2.947", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "This is a question about the limiting distribution of a regular matrix. To solve this kind of problem, we just solve the equations below:<br>\r\n$\\pi_j=\\sum_{k=0}^N \\pi_kP_{kj}$\r\n$\\sum_j{\\pi_j}=1$\r\n<br>\r\nBy putting the values in, we get the equations for this question:<br>\r\n$\\pi_0=0.3\\pi_0+0.5\\pi_1+0.5\\pi_2$<br>\r\n$\\pi_1=0.2\\pi_0+0.1\\pi_1+0.2\\pi_2$<br>\r\n$\\pi_2=0.5\\pi_0+0.4\\pi_1+0.3\\pi_2$\r\n<br>\r\n\r\nAnother equations is given by the property of probability<br>\r\n$\\pi_0+\\pi_1+\\pi_2=1$<br>\r\nSolve them and get $\\pi_0=\\frac{55}{132},\\pi_1=\\frac{24}{132},\\pi_2=\\frac{53}{132}$<br>\r\nSo the answer is $\\pi_0*2+\\pi_1*5+\\pi_2*3=2.947$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Oh no, you'd better refer to the content about the regular Markov chain and think it again.", "messagesuccess": "Congratulations! Now you can challenge the next problem.", "sensitivity": null, "gussingparameter": 0.33, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [], "wrongproblems": [231], "twinproblems": [228, 231]}}, {"model": "mathematics.question", "pk": 236, "fields": {"code": "5.1.4", "category": 3, "problem": "The probability generating function of Poisson process is given by \r\n\\begin{equation*}\r\ng(s)=E[s^X]=e^{-\\mu(1-s)}\r\n\\end{equation*}\r\n<br>\r\nLet $X$ and $Y$ be independent random variables, Poisson distributed with parameters $\\alpha$ and $\\beta$, respectively. Calculate the generating function of their sum.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "\\begin{equation*}\r\ng_N(s)=e^{-(\\alpha+\\beta)(1-s)}\r\n\\end{equation*}", "choicesb": "\\begin{equation*}\r\ng_N(s)=e^{-(\\alpha\\beta)(1-s)}\r\n\\end{equation*}", "choicesc": "\\begin{equation*}\r\ng_N(s)=e^{-(\\alpha)(1-e^{-\\beta(1-s)})}\r\n\\end{equation*}", "choicesd": "None of the above", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "For the probability generating function of the sum of two independent distributions, we are calculating\r\n\\begin{equation*}\r\nE[s^{X+Y}]=E[s^X]E[s^Y]\r\n\\end{equation*}\r\nTherefore,\r\n\\begin{equation*}\r\ne^{-\\alpha(1-s)}e^{-\\beta(1-s)}=e^{-(\\alpha+\\beta)(1-s)}\r\n\\end{equation*}", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 237, "fields": {"code": "4.1.5", "category": 2, "problem": "Consider the Marcov chain whose transition probability matrix is given by\r\n\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, 0 \\quad \\quad1\\quad\\,\r\n2\\quad\\,3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr  \\hbox{3}}\r\n\\pmatrix{\r\n0.1 &0.5 & 0  &0.4  \\cr\r\n0& 0 &1 &0  \\cr\r\n0& 0 &0 &1  \\cr\r\n1& 0 &0 &0 \r\n}\r\n\\end{eqnarray*}\r\nDetermine the limiting distribution for the process", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\pi_0=\\frac{10}{29},\\pi_1=\\frac{5}{29},\\pi_2=\\frac{5}{29},\\pi_3=\\frac{9}{29}$", "choicesb": "$\\pi_0=\\frac{9}{29},\\pi_1=\\frac{5}{29},\\pi_2=\\frac{5}{29},\\pi_3=\\frac{10}{29}$", "choicesc": "$\\pi_0=\\frac{10}{29},\\pi_1=\\frac{4}{29},\\pi_2=\\frac{4}{29},\\pi_3=\\frac{11}{29}$", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "This is a question about the limiting distribution of a regular matrix. To solve this kind of problem, we just solve the equations below:<br>\r\n$\\pi_j=\\sum_{k=0}^N \\pi_kP_{kj}$\r\n$\\sum_j{\\pi_j}=1$\r\n<br>\r\nBy putting the values in, we get the equations for this question:<br>\r\n$\\pi_0=0.1\\pi_0+0*\\pi_1+0*\\pi_2+1*\\pi_3$<br>\r\n$\\pi_1=0.5\\pi_0+0*\\pi_1+0*\\pi_2+0*\\pi_3$<br>\r\n$\\pi_2=0*\\pi_0+1*\\pi_1+0*\\pi_2+0*\\pi_3$<br>\r\n$\\pi_3=0.4\\pi_0+0*\\pi_1+1*\\pi_2+0*\\pi_3$\r\n<br>\r\n\r\nAnother equations is given by the property of probability<br>\r\n$\\pi_0+\\pi_1+\\pi_2+\\pi_3=1$<br>\r\nSolve them and get $\\pi_0=\\frac{10}{29},\\pi_1=\\frac{5}{29},\\pi_2=\\frac{5}{29},\\pi_3=\\frac{9}{29}$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Oh no, you'd better refer to the content about the regular Markov chain and think it again.", "messagesuccess": "Congratulations! Now you can challenge the next problem.", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [35], "rightproblems": [], "wrongproblems": [228, 238, 239], "twinproblems": [228, 239]}}, {"model": "mathematics.question", "pk": 238, "fields": {"code": "4.1.6", "category": 2, "problem": "Compute the limiting distribution for the  transition probability matrix\r\n\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, 0 \\quad 1\\quad \r\n2 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} }\r\n\\pmatrix{\r\n\\frac{1}{2} & \\frac{1}{2}  &0  \\cr\r\n\\frac{1}{3}&\\frac{1}{3} &\\frac{1}{3}  \\cr\r\n\\frac{1}{6} & \\frac{1}{2} & \\frac{1}{3}\r\n}\r\n\\end{eqnarray*}", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\pi_0=\\frac{3}{14},\\pi_1=\\frac{6}{14},\\pi_2=\\frac{5}{14}$", "choicesb": "$\\pi_0=\\frac{5}{14},\\pi_1=\\frac{6}{14},\\pi_2=\\frac{3}{14}$", "choicesc": "$\\pi_0=\\frac{6}{14},\\pi_1=\\frac{5}{14},\\pi_2=\\frac{3}{14}$", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "This is a question about the limiting distribution of a regular matrix. To solve this kind of problem, we just solve the equations below:<br>\r\n$\\pi_j=\\sum_{k=0}^N \\pi_kP_{kj}$\r\n$\\sum_j{\\pi_j}=1$\r\n<br>\r\nBy putting the values in, we get the equations for this question:<br>\r\n$\\pi_0=\\frac{1}{2}\\pi_0+\\frac{1}{3}\\pi_1+\\frac{1}{6}\\pi_2$<br>\r\n$\\pi_1=\\frac{1}{2}\\pi_0+\\frac{1}{3}\\pi_1+\\frac{1}{2}\\pi_2$<br>\r\n$\\pi_2=0*\\pi_0+\\frac{1}{3}\\pi_1+\\frac{1}{3}\\pi_2$\r\n<br>\r\n\r\nAnother equations is given by the property of probability<br>\r\n$\\pi_0+\\pi_1+\\pi_2=1$<br>\r\nSolve them and get $\\pi_0=\\frac{5}{14},\\pi_1=\\frac{6}{14},\\pi_2=\\frac{3}{14}$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Oh no, you'd better refer to the content about the regular Markov chain and think it again.", "messagesuccess": "Congratulations! Now you can challenge the next problem.", "sensitivity": null, "gussingparameter": 0.33, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [35], "rightproblems": [], "wrongproblems": [237, 240, 241], "twinproblems": [228, 240, 241]}}, {"model": "mathematics.question", "pk": 239, "fields": {"code": "4.1.7", "category": 2, "problem": "A Marcov chain on the states 0,1,2,3 has the transition probability matrix \r\n\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, 0 \\quad \\quad1\\quad\\,\r\n2\\quad\\,3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr  \\hbox{3}}\r\n\\pmatrix{\r\n0.1 &0.2 & 0.3  &0.4  \\cr\r\n0& 0.3 &0.3 &0.4  \\cr\r\n0& 0 &0.6 &0.4  \\cr\r\n1& 0 &0 &0 \r\n}\r\n\\end{eqnarray*}\r\nDetermine the corresponding limiting distribution for the process", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\pi_0=\\frac{140}{441},\\pi_1=\\frac{40}{441},\\pi_2=\\frac{135}{441},\\pi_3=\\frac{126}{441}$", "choicesb": "$\\pi_0=\\frac{140}{441},\\pi_1=\\frac{43}{441},\\pi_2=\\frac{135}{441},\\pi_3=\\frac{123}{441}$", "choicesc": "$\\pi_0=\\frac{135}{441},\\pi_1=\\frac{40}{441},\\pi_2=\\frac{140}{441},\\pi_3=\\frac{126}{441}$", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "This is a question about the limiting distribution of a regular matrix. To solve this kind of problem, we just solve the equations below:<br>\r\n$\\pi_j=\\sum_{k=0}^N \\pi_kP_{kj}$\r\n$\\sum_j{\\pi_j}=1$\r\n<br>\r\nBy putting the values in, we get the equations for this question:<br>\r\n$\\pi_0=0.1\\pi_0+0*\\pi_1+0*\\pi_2+1*\\pi_3$<br>\r\n$\\pi_1=0.2\\pi_0+0.3\\pi_1+0*\\pi_2+0*\\pi_3$<br>\r\n$\\pi_2=0.3\\pi_0+0.3\\pi_1+0.6\\pi_2+0*\\pi_3$<br>\r\n$\\pi_3=0.4\\pi_0+0.4\\pi_1+0.4\\pi_2+0*\\pi_3$\r\n<br>\r\n\r\nAnother equations is given by the property of probability<br>\r\n$\\pi_0+\\pi_1+\\pi_2+\\pi_3=1$<br>\r\nSolve them and get $\\pi_0=\\frac{140}{441},\\pi_1=\\frac{40}{441},\\pi_2=\\frac{135}{441},\\pi_3=\\frac{126}{441}$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Oh no, you'd better refer to the content about the regular Markov chain and think it again.", "messagesuccess": "Congratulations! Now you can challenge the next problem.", "sensitivity": null, "gussingparameter": 0.33, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [35], "rightproblems": [], "wrongproblems": [237], "twinproblems": [237]}}, {"model": "mathematics.question", "pk": 240, "fields": {"code": "4.1.8", "category": 2, "problem": "Suppose that the social classes of successive generations in a family follow a\r\nMarkov chain with transition probability matrix given by\r\n\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, Lower \\,\\, Middle\\,\\,\r\nUpper \\\\\r\n{\\bf Father's class} & & \\matrix{ \\hbox{Lower} \\cr\r\n\\hbox{Middle} \\cr  \\hbox{Upper} }\r\n\\pmatrix{\r\n0.7 & 0.2 &0.1  \\cr\r\n0.2& 0.6 &0.2 \\cr\r\n0.1 & 0.4  & 0.5\r\n}\r\n\\end{eqnarray*}\r\nWhat fraction of families are upper class in the long run?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\pi_L=\\frac{7}{17}$", "choicesb": "$\\pi_L=\\frac{2}{17}$", "choicesc": "$\\pi_L=\\frac{4}{17}$", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "This is a question about the limiting distribution of a regular matrix. To solve this kind of problem, we just solve the equations below:<br>\r\n$\\pi_j=\\sum_{k=0}^N \\pi_kP_{kj}$\r\n$\\sum_j{\\pi_j}=1$\r\n<br>\r\nBy putting the values in, we get the equations for this question:<br>\r\n$\\pi_L=0.6\\pi_L+0.3\\pi_M+0.4\\pi_U$<br>\r\n$\\pi_M=0.3\\pi_L+0.3\\pi_M+0.1\\pi_U$<br>\r\n$\\pi_U=0.1\\pi_L+0.4\\pi_M+0.5\\pi_U$\r\n<br>\r\n\r\nAnother equations is given by the property of probability<br>\r\n$\\pi_L+\\pi_M+\\pi_U=1$<br>\r\nSolve them and get $\\pi_L=\\frac{4}{17}$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Oh no, you'd better refer to the content about the regular Markov chain and think it again.", "messagesuccess": "Congratulations! Now you can challenge the next problem.", "sensitivity": null, "gussingparameter": 0.33, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [35], "rightproblems": [], "wrongproblems": [238], "twinproblems": [228, 238]}}, {"model": "mathematics.question", "pk": 241, "fields": {"code": "4.1.9", "category": 2, "problem": "Determine the limiting distribution for the Markov chain whose transition probability matrix is\r\n\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, 0 \\quad \\quad1\\quad \\quad\r\n2 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} }\r\n\\pmatrix{\r\n\\frac{1}{2} & \\frac{1}{2}  &0  \\cr\r\n\\frac{1}{3}&\\frac{1}{2} &\\frac{1}{6}  \\cr\r\n0 & \\frac{1}{4} & \\frac{3}{4}\r\n}\r\n\\end{eqnarray*}", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$\\pi_0=\\frac{2}{7},\\pi_1=\\frac{3}{7},\\pi_2=\\frac{2}{7}$", "choicesb": "$\\pi_0=\\frac{3}{7},\\pi_1=\\frac{3}{7},\\pi_2=\\frac{1}{7}$", "choicesc": "$\\pi_0=\\frac{2}{7},\\pi_1=\\frac{4}{7},\\pi_2=\\frac{1}{7}$", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "This is a question about the limiting distribution of a regular matrix. To solve this kind of problem, we just solve the equations below:<br>\r\n$\\pi_j=\\sum_{k=0}^N \\pi_kP_{kj}$\r\n$\\sum_j{\\pi_j}=1$\r\n<br>\r\nBy putting the values in, we get the equations for this question:<br>\r\n$\\pi_0=\\frac{1}{2}\\pi_0+\\frac{1}{3}\\pi_1+0*\\pi_2$<br>\r\n$\\pi_1=\\frac{1}{2}\\pi_0+\\frac{1}{2}\\pi_1+\\frac{1}{4}\\pi_2$<br>\r\n$\\pi_2=0*\\pi_0+\\frac{1}{6}\\pi_1+\\frac{3}{4}\\pi_2$\r\n<br>\r\n\r\nAnother equations is given by the property of probability<br>\r\n$\\pi_0+\\pi_1+\\pi_2=1$<br>\r\nSolve them and get $\\pi_0=\\frac{2}{7},\\pi_1=\\frac{3}{7},\\pi_2=\\frac{2}{7}$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Oh no, you'd better refer to the content about the regular Markov chain and think it again.", "messagesuccess": "Congratulations! Now you can challenge the next problem.", "sensitivity": null, "gussingparameter": 0.33, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [35], "rightproblems": [], "wrongproblems": [238], "twinproblems": [228, 238]}}]	
export default data;