const data = 
[{"model": "mathematics.question", "pk": 4, "fields": {"code": "3.2.8", "category": 1, "problem": "(Mickey in Maze} continued from Example 3.2). \r\n(i) Compute the probability Mickey goes to cell 2 before it reaches cell 6, beginning\r\nfrom cell 1.\r\n<br>\r\n(ii). Compute the mean number of steps to reach cells 2 or 6, beginning\r\nfrom cell 4.\r\n<br>\r\n(iii). Compute the mean number of times Mickey visits cell 2 before reaching cell\r\n6, starting from cell 4.", "problempicture1": "theall/image/Expl3.2.8-1.png", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "(i) Set\r\n$$p_i = P(\\hbox{Mickey reaches cell 2 before reaching 6 } | \\hbox{ starting from i}).\r\n$$\r\nThen, for example,\r\n\\begin{eqnarray*}\r\np_0 &=& P_{01}p_1 + P_{03} p_3 = 1/2 p_1 + 1/2 p_3  \\\\\r\np_1 &=& 1/3p_0  + 1/3 p_4 + 1/3 p_2 = 1/3(p_0 +p_4) + 1/3 \\\\\r\np_2 &=& 1  \\\\\r\np_3 &=& 1/3 p_0 + 1/3 p_4 + 1/3p_6 = 1/3(p_0 +p_4) \\\\\r\np_4 &=& 1/4 p_1 + 1/4 p_3 + 1/4p_5 + 1/3p_7 \\\\\r\np_5 &=& 1/3 p_2 + 1/3 p_4 + 1/3p_8 = 1/3(p_0 +p_8) +1/3 \\\\\r\np_6 &=& 0 \\\\\r\np_7 &=& 1/3 p_4 + 1/3 p_6 + 1/3p_8 = 1/3(p_4 +p_8) \\\\\r\np_8 &=& 1/2 p_5 + 1/2 p_7\r\n\\end{eqnarray*}\r\nThe above linear equations can be solved for the answer\r\n$$ p_0=p_8=1/2, \\quad p_1=p_5 = 2/3, \\quad p_3=p_7=1/3 \\quad\r\np_4=1/2.$$\r\n\r\nIn fact, a short-cut to solving the eight equations is, by the\r\nsymmetry of the maze, to realize that $p_0=p_8$, $p_1=p_5$, $p_3=p_7$.\r\nTogether with the fact that $p_2=1$ and $p_6=0$, the number of\r\nequations can be immediately reduced to four about $p_0, p_1, p_3$\r\nand $p_4$.\r\n\r\n(ii). Set $w_i$ the mean number of steps to reach 2 or 6, starting from\r\ncell $i$. Then, obviously, $w_2=w_6=0$. By symmetry,\r\n$w_0=w_8$, $w_1=w_3=w_5=w_7$.\r\nMoreover,\r\n\\begin{eqnarray*}\r\nw_0 &=& 1 + 1/2(w_1 + w_3) = 1 + w_1 \\\\\r\nw_1 &=& 1+ 1/3(w_0+w_2+w_4)= 1 + 1/3(w_0 + w_4) \\\\\r\nw_4 &=& 1+ 1/4(w_1+w_3+w_5+w_7) = 1+w_1\r\n\\end{eqnarray*}\r\nSolving the three equations,\r\n$$w_0=w_4=w_8=6\\quad \\hbox{and} \\quad w_1=w_3=w_5=w_7=5.$$\r\n\r\n(iii).\r\nSet $w_i$ the mean number of visits of cell 2 before reaching cell 6, starting\r\nfrom cell $i$ (including the starting state).\r\n  Then, obviously, $w_6=0$. By symmetry,\r\n$w_0=w_8$, $w_1=w_5$ and $w_3= w_7$.\r\nMoreover,\r\n\\begin{eqnarray*}\r\nw_0 &=&  1/2(w_1 + w_3)   \\\\\r\nw_1 &=&  1/3(w_0+w_2+w_4)  \\\\\r\nw_2 &=& 1 + 1/2(w_1 + w_5) = 1+ w_1 \\\\\r\nw_3 &=& 1/3(w_0 + w_4 + w_6) = 1/3(w_0 +w_4) \\\\\r\nw_4 &=&  1/4(w_1+w_3+w_5+w_7) = 1/2(w_1+w_3)\r\n\\end{eqnarray*}\r\nSolving the three equations,\r\n$$w_0=w_4=w_8=3/2\\quad w_1=w_5= 2 \\quad w_2= 3 \\quad \\hbox{and} \\quad  w_3=w_7=1.$$\r\nStarting from cell 4,  Mickey's  mean number of visits of cell 2 before reaching cell 6\r\nis $w_4=3/2$.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Alas! Please return to the graph and review related knowledge.", "messagesuccess": "Congratulations! You have mastered the First Step Analysis method.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [5, 60], "wrongproblems": [8, 54, 130], "twinproblems": [8, 54, 130]}}, {"model": "mathematics.question", "pk": 5, "fields": {"code": "3.2.9", "category": 1, "problem": "( A Fecundity Model) \r\nThe life span of  women is divided into\r\nfive-year periods. In each period, a woman assumes\r\none of the following six states:\r\n$E_0$: prepuberty; $E_1$: single; $E_2$: married;\r\n$E_3$: divorced; $E_4$: widowed; $E_5$: out-of-population.\r\nAssume the transit process is a {MC}  with transition matrix\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\, E_0 \\quad E_1 \\quad E_2\\quad E_3\\quad E_4\\quad E_5\\\\\r\n {\\bf P}&=& \\matrix{E_0 \\cr E_1 \\cr E_2 \\cr E_3 \\cr E_4\\cr E_5 }\r\n  \\pmatrix{\r\n \\,\\,\\,\\,\\, &  0.9  &     &       &    & 0.1        \\cr\r\n &  0.5   &  0.4  &      &    &   0.1      \\cr\r\n  &       &  0.6   & 0.2 & 0.1  &   0.1    \\cr\r\n &    & 0.4   &  0.5  &     &     0.1     \\cr\r\n &     & 0.4  &      &  0.5   &  0.1     \\cr\r\n  &     &     &      &     &    1\r\n}\r\n\\end{eqnarray*}\r\nWhat is the mean marriage periods of a woman in her life time?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Set $w_i$ the mean periods in marriage in the remaining life time,\r\nstarting from state $E_i$ for the current period (including\r\nthe current period.)\r\nThen,\r\n\\begin{eqnarray*}\r\nw_0 &=& 0.9 w_1 +0.1 w_5 \\\\\r\nw_1 &=& 0.5w_1 + 0.4 w_2 + 0.1w_5 \\\\\r\nw_2 &=& 0.6w_2 + 0.2 w_3 + 0.1 w_4 + 0.1w_5 + 1 \\\\\r\nw_3 &=& 0.4 w_2  + 0.5 w_3 + 0.1 w_5\\\\\r\nw_4 &=& 0.4 w_2  + 0.5 w_4 + 0.1 w_5\\\\\r\nw_5 &=& 0\r\n\\end{eqnarray*}\r\nSolving the equations, we have\r\n$$w_0 = 4.5, \\quad w_1=5 \\quad w_2=5.25 \\quad \\hbox{and} \\quad w_3=w_4=5.$$\r\nThe mean marriage periods of a woman in her life time is $w_0 = 4.5$.\r\nIn other words, the mean marriage time of a woman in her life time\r\nis $4.5\\times 5 =22.5$ years.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Alas! Please return to the graph and review related knowledge.", "messagesuccess": "Congratulations! You have mastered the First Step Analysis method.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [4, 54], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 6, "fields": {"code": "3.3.12", "category": 1, "problem": "( The family tree of Confucius ) \r\n The following diagram shows the first few generations of the family tree\r\n of Confucius.", "problempicture1": "theall/image/Expl3.3.12-1.png", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Note that $\\xi_i^{(n)} $ denotes the number of sons fathered by\r\nthe $i$-th member of the $n$-generation of Kong Zi(Confucius),\r\nand $X_n$ is the number of (male) descendants of Kong Zi at\r\nthe $n$-th generation.\r\n\r\nAllegedly, the 80-th generation of Kong Zi  is around\r\n1,300,000. If the process is indeed a branching process. Then\r\na reasonable estimate of $\\mu$, the mean number of sons produced by\r\nany male descendent of Kong Zi, can be obtained by\r\n$$ \\hat \\mu^{80} = 1,300,000,$$\r\nbased on the formula that $E(X_n) = \\mu^n$, to be shown in\r\nthe following Proposition 3.1.\r\nIt turns out $\\hat \\mu = 1.1924$, meaning that\r\nthe average number of sons produced by any  Kong father  is about\r\n1.2. Taking one step further, if the process is indeed\r\na branching process,  the data is trustworthy and\r\nthe male Kongs can be regarded as typical of\r\nChinese males, the one might claim, on average, each Chinese\r\nmale produces about 1.2 sons.\r\nIt is quite clear that the process cannot be really or even\r\napproximately a branching process---$\\xi^{(n)}_i$ cannot be\r\niid, not over good/bad historical periods, and at least\r\nnot over periods with/without birth control policies.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Alas! Please return to the graph and review related knowledge.", "messagesuccess": "Congratulations! You have mastered the concept and fundamental characteristics of Branching Process.", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [14], "rightproblems": [59], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 31, "fields": {"code": "3.1.1", "category": 1, "problem": "Let $\\xi_0=0$ and, for $i \\geq 1$,\r\n$\\xi_i$ =\r\n $$\\cases{ 1 & if the $i$-th toss is a Head (with probability $p$) \\cr\r\n0 & if the $i$-th toss is a Tail (with probability $1-p$)}$$\r\nSet $X_n= \\sum_{i=0}^n \\xi_i $, $n \\geq 0$. $X_n$ is the random number which counts\r\nthe number of Heads up to the $n$-th toss.What's the value of\r\n$P(X_{n+1}=j | X_0=0, X_1 = i_1, ..., X_{n-1}= i_{n-1}, X_n = i)?$", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "\\[  = \\left\\{\r\n\\begin{array}{ll}\r\n P(\\xi_{n+1}=1) & if ~j=i+1 \\\\\r\nP(\\xi_{n+1}=0) & if ~j=i\\\\\r\n0 & otherwise\\\\\r\n\\end{array} \r\n\\right. \\]", "choicesb": "$ { P(\\xi_{n+1}=1)}=1$", "choicesc": "None of the above is correct.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Just straight compute the probability\r\n\\begin{eqnarray*}\r\n&& P(X_{n+1}=j | X_0=0, X_1 = i_1, ..., X_{n-1}= i_{n-1}, X_n = i) \\\\\r\n&=& \\cases{ P(\\xi_{n+1}=1|X_0=0, X_1 = i_1, ..., X_{n-1}= i_{n-1}, X_n = i) & if $j=i+1$ \\cr\r\nP(\\xi_{n+1}=0|X_0=0, X_1 = i_1, ..., X_{n-1}= i_{n-1}, X_n = i) & if $j=i$ \\cr\r\n0 & otherwise \\cr\r\n}\\\\\r\n&=& \\cases{ P(\\xi_{n+1}=1) & if $j=i+1$ \\cr\r\nP(\\xi_{n+1}=0) & if $j=i$ \\cr\r\n0 & otherwise \\cr\r\n}\\\\\r\n&=& \\cases{ p & if $j=i+1$ \\cr\r\n1-p & if $j=i$ \\cr\r\n0 & otherwise \\cr\r\n}\\\\\r\n&=& \\cases{ P(\\xi_{n+1}=1|X_n=i) & if $j=i+1$ \\cr\r\nP(\\xi_{n+1}=0|X_n=i) & if $j=i$ \\cr\r\n0 & otherwise \\cr\r\n}\\\\\r\n&=& P(X_{n+1}=j |X_n=i)\r\n\\end{eqnarray*} <br>\r\n\r\n\r\nIt implies, once the present $X_n$ is fixed, the past history $X_0, ..., X_{n-1}$, shall not\r\naffect the future distribution of $X_{n+1}$.}\r\nHere, we are using time $n$ as present, time before $n$ is past, and time beyond $n$ is\r\nfuture.\r\n\r\nThe above statement is the same as saying that\r\nthe future $\\{X_k: k\\geq n+1\\}$ and the past $\\{X_k: k\\leq n-1\\}$ are\r\n$conditionally$ $independent$ given the present $X_n$ taking any fixed value.<br>\r\n$remark$   <br>Not only $n+1$ but, all future distribution $X_{n+1}, X_{n+2}, ....$ shall depend on\r\nthe past and now only through now.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Alas! Please check the definition of markov chain and conditional independence.", "messagesuccess": "Congratulations! You have mastered the definition of markov chains\r\nand conditional independence.", "sensitivity": 1.0, "gussingparameter": 3.0, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [3, 7], "rightproblems": [], "wrongproblems": [2], "twinproblems": []}}, {"model": "mathematics.question", "pk": 32, "fields": {"code": "3.1.2", "category": 1, "problem": "({ Mickey in Maze})\r\nMickey mouse travels in a maze with nine $3\\times 3$ cells. The cells are numbered\r\nas 0, 1, ..., 8 from left to right and top down. Each step Mickey travels\r\nfrom where it is to one\r\nof the surrounding connected cells with equal chance.Let $X_n$ denote the\r\ncell number of Mickey at step $n$. $(X_0=4)$.Is The process$\\{X_n: n=0, 1, 2, ...\\}$ a Markov chain?", "problempicture1": "theall/image/Example_3-1-2.PNG", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "yes", "choicesb": "no", "choicesc": "None of the above is correct.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The definition of MC $\\{X_n\\}$, namely the past and future are\r\n independent given any fixed state of the present, <br>\r\n \\begin{eqnarray*}\r\n&& P(X_{n+1}=j | X_0=0, X_1 = i_1, ..., X_{n-1}= i_{n-1}, X_n = i) \\\\\r\n&=&\r\nP(X_{n+1}=j |X_n=i).\r\n\\end{eqnarray*}\r\nSuppose currently Mickey is in cell 5, for example,\r\nthe future movement or path of Mickey is irrelevant with the past movement\r\nor path of Mickey. In other words, how Mickey has got to cell 5 in the past\r\nhas nothing to do with how Mickey would move around in the future.\r\nThe process $\\{X_n: n=0, 1, 2, ...\\}$ is a Markov chain.<br>\r\n{ \\it Example for fun} (``First Blood.\") John Rambo only  obeys\r\nthe order from  Colonel Samuel Trautman,\r\nwho supposedly only  obeys the order from the Pentagon. Then\r\nPentagon $\\longrightarrow$ Trautman $\\longrightarrow$\r\nRambo forms a $MC$.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "well,try again?", "messagesuccess": "good job!", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [3, 7], "rightproblems": [2], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 33, "fields": {"code": "3.1.3", "category": 1, "problem": "({Mickey in Maze run more steps)\r\nMickey mouse travels in a maze with nine $3\\times 3$ cells. The cells are numbered\r\nas 0, 1, ..., 8 from left to right and top down. Each step Mickey travels\r\nfrom where it is to one\r\nof the surrounding connected cells with equal chance.Compute $P^{(3)}_{4\\, 8}$ and $P^{(3)}_{1\\, 8}.$", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$P^{(3)}_{4\\, 8}$=$0.5$,$P^{(3)}_{1\\, 8}$=$0.5$", "choicesb": "$P^{(3)}_{4\\, 8}$=$0$,$P^{(3)}_{1\\, 8}$=$0$", "choicesc": "None of the above is correct.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Remember the basic definition and method of computing the transition  probability,\r\n  write\r\n$$P^{(3)}_{4\\, 8} = \\sum_{k=0}^\\infty P_{4\\, k} P_{k\\, 8}^{(2)}\r\n=\\sum_{k=1, 3, 5, 7} P_{4\\, k} P_{k\\, 8}^{(2)} = 1/4\\sum_{k=1, 3, 5, 7}   P_{k\\, 8}^{(2)}=0;$$\r\n$$P^{(3)}_{1\\, 8} = 1/3 \\times 1/2 \\times 1/3 + 1/3\\times 1/4 \\times 1/3 +\r\n1/3 \\times 1/4 \\times 1/3 =0.$$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": 2.0, "gussingparameter": 3.0, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [3, 6, 7], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 34, "fields": {"code": "3.1.4", "category": 1, "problem": "( An Inventory Mode)\r\nLet $X_n$ be the number of TV sets at a store in the end of day $n$ with $X_0=2$. Let $\\xi_n$ be the\r\nsales of the number of TVs on day $n$. Assume $\\xi_1, \\xi_2, ...$ are iid (independent, identically distributed) such that\r\n$$P(\\xi_n=i) = \\cases{0.5 & $i=0 $\\cr 0.4 & $i =1$ \\cr  0.1 & $i=2$ }$$\r\nAt the end of any day $n$, if $X_n=0$ or $-1$, two TVs will be sent to the store overnight. Moreover,\r\nin the case of  $X_n=-1$,\r\nanother TV will be sent directly to the customer's house. If $X_n =1$ or $2$,\r\nnothing happens. With this inventory policy,\r\n$$X_{n+1}= \\cases{ X_n - \\xi_{n+1} & if $X_n=1, 2$ \\cr 2- \\xi_{n+1} & if $X_n=-1, 0$.}$$\r\nThen, $X_0, X_1, X_2, ..., $ is a $MC$ with state space $\\{-1, 0, 1, 2\\}$ .What's the one-step trasition probability from 1 to 2($p_{1\\,2}$)", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "1", "choicesb": "0", "choicesc": "None of the above is correct.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Remember the basic definition and method of computing the one-step transition  probability,<br>\r\n $X_0, X_1, X_2, ..., $ is a $MC$ with state space $\\{-1, 0, 1, 2\\}$ and with one-step\r\ntransition probability\r\n$$\\qquad \\,\\,\\,\\,\\,\\,\\, -1 \\qquad 0 \\qquad 1 \\qquad 2$$\r\n$$P=\\matrix{-1 \\cr 0 \\cr 1 \\cr 2}\\pmatrix{0 &0.1 &0.4 &0.5 \\cr 0 &0.1 &0.4 &0.5\\cr\r\n0.1 &0.4 &0.5 &  0 \\cr 0 &0.1 &0.4 &0.5 }.$$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [3, 6], "rightproblems": [], "wrongproblems": [], "twinproblems": [35, 90]}}, {"model": "mathematics.question", "pk": 35, "fields": {"code": "3.1.5", "category": 1, "problem": "{ ({ The Ehrenfest Model}) <br>\r\nThere are $2N$ particles in a jar separated by a membrane into two chambers A and B. Let\r\n$Y_n$ be the number of particles in A after $n$ crossings. Each crossing is a particle from\r\nA to B or from B to A. Assume when any once crossing happens, it happens to any one of the\r\n$2N$ particles with\r\neach equal chance $1/2N$.Then $Y_n, n\\geq 0$ is a $MC$ with state space $\\{0, 1, 2, ..., 2N\\}$.Compute the\r\n$P_{ij}$", "problempicture1": "theall/image/Example_3-1-5.PNG", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0.5", "choicesb": "$1/3$", "choicesc": "None of the above is correct.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Write down the one-step transition,easily see that\r\n $$P_{ij}= P(Y_{n+1}=j|Y_n=i) = \\cases{ i/(2N) & if $j=i-1$ \\cr  1-i/(2N) & if $j=i+1$ \\cr\r\n0 & else; } $$\r\nfor $0 \\leq i \\leq j \\leq 2N$.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "wrong,another try?", "messagesuccess": "good job!", "sensitivity": 2.0, "gussingparameter": 3.0, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [3, 6], "rightproblems": [], "wrongproblems": [], "twinproblems": [34]}}, {"model": "mathematics.question", "pk": 36, "fields": {"code": "3.1.6", "category": 1, "problem": "Repeatedly toss a fair coin a number of times.  What's the expected number of\r\ntosses till the first two consecutive heads occur?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "0", "choicesb": "$1/2$", "choicesc": "none of above is correct.", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $\\xi_0=0$ and, for $i \\geq 1$,\r\n$$\\xi_i = \\cases{ 1 & if the $i$-th toss is a Head (with probability $1/2$) \\cr\r\n0 & if the $i$-th toss is a Tail (with probability $1/2$)}$$\r\nSet, for $n\\geq 1$,\r\n$$X_n= \\cases{ 0 & if the $n$-th toss is tail  \\cr\r\n1 & if $\\xi_{n-1}=0, \\xi_n=1$ \\cr\r\n2 & if $\\xi_{n-1}=1, \\xi_n=1$.}\r\n$$\r\nThen,\r\n $X_n$ is a $MC$ with state space $\\{0, 1, 2\\}$ and\r\n transition matrix\r\n \\begin{eqnarray*}\r\n && \\qquad \\,\\,\\, 0 \\qquad 1 \\qquad 2\\\\\r\n  P  &=& \\matrix{0\\cr 1 \\cr 2} \\pmatrix{\r\n 1/2 & 1/2 & 0 \\cr\r\n 1/2 & 0 & 1/2 \\cr\r\n 1/2 & 0 & 1/2\r\n }.\r\n \\end{eqnarray*}\r\nFor example,\r\n$$P_{00}= P(X_{n+1}=0|X_n=0)=P(\\xi_{n+1}=0|\\xi_n=0)=P(\\xi_{n+1}=0)=1/2.$$\r\nFor $k \\geq 1$, let $T_k= \\min\\{ n \\geq 0: X_{n+k}=2\\} $, which is the minimum number of\r\nadditional tosses till the first two consecutive heads occur, starting from (excluding)\r\nthe $k$-th toss. Then\r\n$$T_k=\\cases{0 &  if  $X_k=2$ \\cr\r\n T_{k+1} + 1 & if $X_k= 0$ or $1$ }.\r\n $$\r\n Let\r\n$$ w_0 = E( T_k|X_k=0) \\qquad w_1 = E(T_k |X_k=1).$$\r\nThen,\r\n\\begin{eqnarray*}\r\nw_0 &=& E(T_1 |X_1=0) = E(T_1 1_{\\{X_{1+1}=0 \\, {\\rm or} \\, 1  {\\rm or} \\, 2\\}} | X_1=0) \\\\\r\n&=& E(T_1 1_{\\{X_{2}=0 \\}} | X_1=0) + E(T_1\r\n1_{\\{X_{2}=  1\\}} | X_1=0) + E(T_1\r\n1_{\\{X_{2}=  2\\}} | X_1=0)\\\\\r\n&=&E(T_1|X_2=0, X_1=0)P(X_2=0|X_1=0)  \\\\\r\n&& + E(T_1|X_2=1, X_1=0)P(X_2=1|X_1=0)\r\n  + E(1 \\time 1_{\\{X_{2}=  2\\}} | X_1=0)\\\\\r\n  &=&E(T_2+1|X_2=0, X_1=0)P(X_2=0|X_1=0)  \\\\\r\n&& + E(T_2+1|X_2=1, X_1=0)P(X_2=1|X_1=0)\r\n  + P_{02}\\\\\r\n&=&(1+w_0)P_{00} + (1+w_1)P_{01} +  P_{02} \\\\\r\n&=& 1 + w_1P_{01}+ w_0P_{00}\r\n\\end{eqnarray*}\r\nLikewise,\r\n$$w_1 = 1 + P_{10} w_0 + P_{11} w_1. $$\r\nTogether, we have\r\n\\begin{eqnarray*}\r\n&& w_0= 1 + P_{00} w_0 + P_{01} w_1 = 1 + (1/2)(w_0+w_1) \\\\\r\n&& w_1 = 1 + P_{10} w_0 + P_{11} w_1 = 1 + (1/2) w_0 .\r\n\\end{eqnarray*}\r\nSolving the equation, we have $w_0=6$ and $w_1=4$.\r\nSince $X_1=0$ or $1$ with half chance.\r\nThe mean number of tosses till the first HH occur is\r\n$$ 1/2(w_0 + w_1) + 1 = 6.$$\r\n\r\n\r\n\r\nThe above example in fact, for the purpose of illustrating\r\n the method of first-step analysis, demonstrates a hard way of solving the problem.\r\n For this particular problem,\r\nthere is actually an easier method without invoking\r\nthe {\\it MC}  $\\{X_n, n\\geq 1 \\}$, but, rather, directly based on $\\{\\xi_i, i \\geq 1\\}$\r\n (Please DIY).\r\nThe idea is contained   in\r\nthe example called a dice game called craps presented in review and\r\nin the following example as well.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": 2.0, "gussingparameter": 3.0, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [3, 6, 9], "rightproblems": [], "wrongproblems": [7], "twinproblems": []}}, {"model": "mathematics.question", "pk": 59, "fields": {"code": "3.3.11", "category": 1, "problem": "(The Confucius descendants) \r\nHypothetically, every male in the Kong family, since Confucius, produces his sons,\r\nindependent of everything else,\r\naccording to the same distribution\r\n<br>\r\n\\[\r\n\\begin{array}{ l cccccc c }\r\nNumber  \\,  of  \\,\\, sons:  & 0   &    1   &   2    &  3     &  4   &   5   &  6  \\,\\,  or\\,\\,  more  \\\\\r\nProbability :      &  0.17 & 0.50&  0.25 & 0.05 & 0.02&  0 .01    &  0 \\\\\r\n\\end{array}\r\n\\]\r\n<br>\r\n Then the mean number of sons of any Kong male is about 1.2.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $X_n$ be the number of the $n$-th generation of descendants of\r\n Confucius. (According to the old Chinese tradition, only males carrying\r\n the last name are counted as family descendants.)\r\n Then $\\{X_n: n \\geq 1\\}$ is a so-called branching process.\r\n Here $X_0=1$ meaning that the $0$-th generation is Confucius himself, alone.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Congratulations! You have mastered the concept and fundamental characteristics of Branching Process.", "messagesuccess": "Alas! Please return to the graph and review related knowledge.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [14], "rightproblems": [6], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 60, "fields": {"code": "3.2.7", "category": 1, "problem": "({\\sc Penney-ante}) Repeatedly tossing a fair coin.\r\nThere are 8 patterns of length 3:\r\n<br>\r\n$$HHH,  HHT, HTH, HTT, THH, THT, TTH, TTT.$$\r\n<br>\r\nTwo men, called  Yugong (Y) and Zhishou (Z),  are betting on whose choice\r\nof pattern would occur first. Mr Z  ``generously\"  allows\r\nMr Y to pick his favorite pattern first and, afterwards, he picks his own.\r\nWhatever Mr Y picks,  Mr Z, being ``Zhishou\", somehow\r\nalways beats him by a  chance at least 2/3! (This time, even infinite offspring\r\nto continue the game can't help Yugong.)", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $X_n, n \\geq 2$ denote the pattern of length 2 of tosses\r\n$n-1$ and $n$.\r\nThen $\\{X_n: n \\geq 2\\}$ is a $MC$  with state space $\\{$HH , HT, TH, TT$\\}$,\r\nwhich is accordingly denoted as $\\{0, 1, 2, 3\\}$, and transition matrix\r\n$$ \\qquad  \\,\\, 0 \\qquad 1 \\qquad 2\\qquad 3  $$\r\n$${\\bf P} =\\matrix{0 \\cr 1 \\cr 2 \\cr 3    }\r\n  \\pmatrix{\r\n  1/2 &  1/2  &     &              \\cr\r\n       &     &  1/2  &  1/2            \\cr\r\n  1/2 &  1/2   &     &             \\cr\r\n       &      &  1/2   &   1/2\r\n}\r\n$$\r\nSuppose Mr Y's pick is HTH. Mr Z can pick HHT  to beat Mr Y\r\nby a chance $2/3$. The following is the proof using first-step analysis.\r\n\r\nLet $p_i $ be the probability that pattern HTH occurs before HHT given\r\n$X_2=i$, $i=0, 1, 2, 3$.\r\nThen\r\n\\begin{eqnarray*}\r\np_0 &=& 1/2p_0 + 1/2 \\times 0 \\\\\r\np_1 &=& 1/2 \\times 1 + 1/2 p_3 \\\\\r\np_2 &=& 1/2 p_0 + 1/2p_1 \\\\\r\np_3 &=& 1/2 p_2 + 1/2 p_3\r\n\\end{eqnarray*}\r\nSolving the equations, we have\r\n$$p_0 = 0 \\quad p_1 = 2/3 \\quad p_2=p_3=1/3$$\r\nThen, the chance that Mr Y wins is\r\n$$ (p_0 + p_1 +p_2+p_3)/4 = 1/3,$$\r\nand Mr Z wins with chance $2/3$.\r\n\r\nGeneral results are:\r\n\r\nMr Y's preemptive Choice: $$ HHH , HHT , HTH , HTT , THH , THT , TTH , TTT $$\r\nMr Z's responsive Choice: $$ THH , THH , HHT , HHT ,  TTH , TTH , HTT , HTT $$\r\n $P$(Mr Z  wins):$$ 7/8\\quad  3/4\\quad  2/3\\quad 2/3\\quad 2/3 \\quad 2/3\\quad 3/4\\quad 7/8  $$\r\n%Mr Y's choice:\r\n % H & HHH &\r\n%Mr Z's choice:\r\n  % & THH &\r\n%Chance of Mr Z winning:\r\n  % & 7/8\r\n\\end{tabular}\r\n\\end{center}\r\n\\hfill $\\square$\r\n\r\nThe above problem may sound counter-intuitive in that Mr Y, with the right of\r\nchoosing first, is always in disadvantage. In other words, preemptive strike always\r\nloses to the counter-strike,   because there does NOT exist\r\nan optimal pattern superior to the rest seven patterns.\r\n It's essentially\r\nabout four random variables, numbers of tosses till the patterns of length 3, forming  a loop\r\nof one dominating another as follows:\r\n\r\nSuch a seemingly strange/bizarre  phenomenon is not uncommon. For a simpler occasion,\r\ntry to construct\r\nthree r.v.s, say, $X$, $Y$ and $Z$, such that\r\n$P(X>Y ) = P(Y> Z)= P(Z> X)=2/3$; which may appear equally bizarre at first\r\nglance. (Please DIY.\r\nHint:  ``TIAN GI SHAI MA\"---the horse racing strategy of the legendary Sun Bin suggested  to\r\nGeneral Tian in the kingdom\r\nof Qi.)\r\n\r\nSome more hints about an insightful explanation of Mr Z's strategy: HHH defeats THH only\r\nwhen the first three tosses turn  out HHH (why?). HHT defeats THH only when the first two tosses\r\nturn out HH (why?). For HTH vs HHT, the competition begins sometime with an H.\r\nIf the following one is H, Mr\r\nZ wins ($1/2$ chance already). If the following two is TH ($1/4$ chance) Mr Y wins,\r\nIf the following two is TT $(1/4$ chance), back to origin  waiting for the next H to\r\noccur. DIY: based on this reasoning, come up with one equation about the chance of Mr Z's HHT\r\ndefeating Mr Y's HTH. Work out the similar for the case of HTT vs HHT.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "Alas! Please return to the graph and review related knowledge.", "messagesuccess": "Congratulations! You have mastered the First Step Analysis method.", "sensitivity": 1.0, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [9], "rightproblems": [4], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 108, "fields": {"code": "4.1.5", "category": 1, "problem": "English premiere league football team Chelsea  recently fired the coach\r\n  Scolari  and hired Mr. N. (not Hiddink), who, known for his firmness with rules,\r\n  declares to strictly employ a naive\r\n  tactic on using for center forward\r\n  one of the two equally good strikers: Didier Drogba or Nicholas Anelka.\r\n  On every match, the player (D or A) has chance $1-p$ to  be performing (up to\r\n  Mr. N.'s standard), in which case\r\n  he continues to play for the next match, and chance $p$ to be non-performing, in\r\nwhich case, he is suspended   for the next two matches (as a kind of penalty or distrust)\r\n  and afterwards waits for his turn to play when the other  is suspended.\r\n  Mr. N. lets Drogba play the first match.\r\n  Let $X_n$ be the number of ``ready\" players at match $n$, playing\r\n  or in bench\r\n  $(X_1=2)$.\r\n  Let $Y_n$ be the number of players that is suspended in\r\n   match $n$\r\n  but would be ``ready\" for the $n+1$-th match $(Y_1=0)$.\r\n  To illustrate the situation, consider the following\r\n  possible path:\r\n\r\nRight now Mr. N. worries about the chance of\r\nhis nightmare match: both Drogba and Anelka are suspended.", "problempicture1": "theall/image/example4-1-5.PNG", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "no", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Note first that $\\{X_n\\}$ is NOT a {MC}, since, for example,\r\n$$P(X_{n+1}=1|X_n=1, X_{n-1}=2) = 1-p \\quad \\hbox{but}\r\n\\quad P(X_{n+1}=1|X_n=1, X_{n-1}=1, X_{n-2}=2)=p.$$\r\n  But   $\\{(X_n, Y_n): n \\geq 0\\}$ is a {\\it MC} with\r\n  state space of $(X_n, Y_n)$\r\n  is $\\{(2, 0), (1,1), (1,0), (0, 1)\\}$ and\r\n  transition probability matrix:\r\n  \\begin{eqnarray*}\r\n && \\qquad \\qquad \\,\\, ({\\rm 2, 0})\\,\\,\\,\\,\\,\\, \\,\\,({\\rm 1, 0})\r\n \\,\\,\\,\\,\\,\\,\\,\r\n({\\rm 1, 1})\\,\\,\\,\\,\\,\\, ({\\rm 0, 1}) \\\\\r\n  P &=&  \\matrix{\r\n  ({\\rm 2, 0}) \\cr ({\\rm 1, 0}) \\cr\r\n({\\rm 1, 1}) \\cr ({\\rm 0, 1}) }\r\n\\pmatrix{\r\n\\,\\,\\,\\, 1-p \\,\\,\\, & \\,\\,\\,\\, p \\,\\,\\,\\,\\,\r\n& \\,\\,\\,\\,\\,\\,\\,     &\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,  \\cr\r\n & & 1-p & \\,\\,\\, p \\,\\,\\cr\r\n1-p & p & & \\cr\r\n&  &   1 & \\,\\,\\, \\,\\, }\r\n\\end{eqnarray*}\r\nLet 0, 1, 2 and 3 denote ({2, 0}), ({1, 0}),\r\n({1, 1}) and ({0, 1}), respectively.\r\nEquations (4.2)-(4.3) becomes\r\n$$\\cases{(\\pi_0 \\,\\, \\pi_1 \\,\\, \\pi_2 \\,\\, \\pi_3 )\r\n= (\\pi_0 \\,\\, \\pi_1 \\,\\, \\pi_2 \\,\\, \\pi_3 )P & \\cr\r\n\\pi_0 + \\pi_1 +\\pi_2 + \\pi_3 = 1 }\r\n$$\r\nSolving the equations gives\r\n$$\\pi_0 = {1-p  \\over 1+p+p^2}, \\quad \\pi_1=\\pi_2= {p \\over 1+p+p^2},\r\n  \\quad\\hbox{and}\\quad\r\n \\pi_3={p^2 \\over 1+p+p^2}.$$\r\nThe long run fraction of the Chelsea nightmare matches\r\nis\r\n$$ \\lim_{n \\to \\infty} P(X_n  =0 )\r\n= \\lim_{n\\to \\infty} P((X_n, Y_n) =(0, 1))\r\n =\\pi_3 = p^2/(1+p+p^2)\r\n$$\r\nIf Mr. N. has the mind of\r\ntrusting whoever employed and un-employing whoever distrusted,\r\n(Yong Ren Bu Yi, Yi Ren Bu Yong--a old Chinese saying)  he sets $p$ low.\r\nThe fraction of nightmare match is at the order\r\nof $p^2$ rather than $p$.\r\nFor example, if $p=1/10$, meaning that a player is nonperforming\r\nin one match is 1 out of 10.\r\nHaving two players, the nightmare matches would be only\r\n1 out of 111, which is relatively small.\r\nIf, instead, Mr. N.   is too harsh and demanding,\r\n  he could set $p$ as 0.4 for example.\r\nThen, his chance of nightmare match is a little over\r\n1 out of 10.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 5, "calculateddifficulty": null, "linkneuron": [37, 40], "rightproblems": [137], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 109, "fields": {"code": "4.1.6", "category": 1, "problem": "({ Stock Price Movement})<br>\r\nSuppose, on each trading day, the HSBC stock is either\r\nup (U) or down (D). And today's up or down depends on the\r\nhistorical movement only through   that of the\r\nprevious two days in the following\r\nway:(figure)<br>\r\n  What's the long run fraction of days the stock price is up?", "problempicture1": "theall/image/example4-1-6.PNG", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "At first glance, it appears that both down and up trends tend to persist, with\r\n  the downs only slightly more so. One might expect that a little over\r\n  50% of the days would be down. It turns out the down days are nearly\r\n  twice as often as up days! Surprising, isn't it?\r\n\r\n  Let $Y_n={\\rm U}$ of D if the stock price on day $n$ is\r\n  up or down, respectively. Then, $\\{Y_n\\}$ is NOT a {$MC$}, because\r\n  $$P(Y_{n+2}={U} | Y_{n+1} = {U}, Y_{n}={U}) = 0.8 \\not=\r\n  0.6= P(Y_{n+2}={U} | Y_{n+1} = {\\rm U}, Y_{n}={\\rm D}). $$\r\n  As a result, the limit theorem of regular {$MC$} does not\r\n  apply to $\\{Y_n\\}$. Looks like a dead end.\r\n\r\n% shan chong shui fu yi wu lu,\r\n% Liu An hua ming yiu yi chun. ---Lu Yiu.\r\n\r\nRecall the tricks of constructing {MC} using\r\npattern of length 2 or 3 in Examples  and exercises\r\nabout coin tossing. The idea applies here.\r\nSet\r\n$ X_n= (Y_{n-1 }, Y_{n})$. Then $\\{X_n\\}$ is a {MC}\r\nwith state space $\\{({\\rm U, U}), ({\\rm D, U}),\r\n({\\rm U, D}),   ({\\rm D, D})\\}$ and transition probability\r\nmatrix\r\n\\begin{eqnarray*}\r\n && \\qquad \\qquad ({\\rm U, U})\\,\\,\\,\\, ({\\rm D, U})\\,\\,\\,\r\n({\\rm U, D})\\,\\, ({\\rm D, D}) \\\\\r\n  P &=&  \\matrix{\r\n  ({\\rm U, U}) \\cr ({\\rm D, U}) \\cr\r\n({\\rm U, D}) \\cr ({\\rm D, D}) }\r\n\\pmatrix{\r\n\\,\\,\\,\\, 0.8 \\,\\,\\, & \\,\\,\\,\\,   \\,\\,\\,\\,\\,\r\n& \\,\\,\\,\\,0.2 \\,\\,\\,     &\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,  \\cr\r\n0.6 &  & 0.4& \\cr\r\n & 0.4&  & \\,\\,\\, 0.6 \\,\\,\\cr\r\n& 0.1 &   & \\,\\,\\,0.9\\,\\, }\r\n\\end{eqnarray*}\r\nLet 0, 1, 2 and 3 denote ({\\rm U, U}), ({\\rm D, U}),\r\n({\\rm U, D}) and ({\\rm D, D}), respectively.\r\nEquations (4.2)-(4.3) becomes\r\n$$\\cases{(\\pi_0 \\,\\, \\pi_1 \\,\\, \\pi_2 \\,\\, \\pi_3 )\r\n= (\\pi_0 \\,\\, \\pi_1 \\,\\, \\pi_2 \\,\\, \\pi_3 )P & \\cr\r\n\\pi_0 + \\pi_1 +\\pi_2 + \\pi_3 = 1 }\r\n$$\r\nSolving the equations gives\r\n$$\\pi_0 = 3/11, \\qquad \\pi_1=\\pi_2=1/11 \\qquad \\pi_3=6/11.$$\r\nThe long run fraction of   the up days\r\nis\r\n\\begin{eqnarray*}\r\n&& \\lim_{n \\to \\infty} P(Y_n={\\rm U})\r\n= \\lim_{n\\to \\infty} [P(Y_n={\\rm U}, Y_{n+1}={\\rm U}) +\r\nP(Y_n={\\rm U}, Y_{n+1}={\\rm D})]\\\\\r\n&=& \\lim_{n\\to \\infty} [P(X_{n+1}=({\\rm U, U}) )+ P(X_{n+1}=({\\rm U, D}))]\r\n= \\lim_{n\\to \\infty} [P(X_{n+1}=0) + P(X_{n+1}=2)]\\\\\r\n&=& \\pi_0 + \\pi_2\r\n= 1/11 + 3/11 = 4/11.\r\n\\end{eqnarray*}\r\nOn average and over a long time, four out of eleven trading days\r\nthe HSBC stock is up and seven out of eleven trading days it's down.\r\n $\\square$\r\n\r\n\r\nThe so-called age replacement policies in the following example\r\n are very common in industry.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 5, "calculateddifficulty": null, "linkneuron": [3, 37], "rightproblems": [], "wrongproblems": [137], "twinproblems": [137]}}, {"model": "mathematics.question", "pk": 110, "fields": {"code": "4.1.7", "category": 1, "problem": "{\\bf Example 4.7} \\ {\\textcolor[rgb]{1, .5,0} {$\\star\\star\\star\\star$}}\r\n \\ ({\\sc Age replacement policies}) \\\r\n To ensure flight safety,\r\nOasis Hong Kong Airline  has a policy to retire aging Boeing 777 with new replacement: as long as\r\nthe airplane fails a regular year-end   examination or, even\r\nif it passes the examination, it has served for $N$ years.\r\nShould there be a replacement, it always takes place in New Year's eve.\r\nSuppose the life time (till failure of examination)\r\nof a Boeing 777 is $Y$ . Let\r\n$a_j = P(Y=j), j=1,2,...$, and\r\n$$p_k=P(Y=k+1|Y>k)={a_{k+1}\\over a_{k+1}+a_{k+2} + \\cdots},\r\n\\quad k=0, 1, 2, ...$$\r\nThen, $p_k$ is the chance the airplane fails next year's test\r\nafter it has serviced $k$ years. What's the long run\r\nfraction of replacements?", "problempicture1": "theall/image/example4-1-7.PNG", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "no", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "To ensure flight safety,\r\nOasis Hong Kong Airline  has a policy to retire aging Boeing 777 with new replacement: as long as\r\nthe airplane fails a regular year-end   examination or, even\r\nif it passes the examination, it has served for $N$ years.\r\nShould there be a replacement, it always takes place in New Year's eve.\r\nSuppose the life time (till failure of examination)\r\nof a Boeing 777 is $Y$ . Let\r\n$a_j = P(Y=j), j=1,2,...$, and\r\n$$p_k=P(Y=k+1|Y>k)={a_{k+1}\\over a_{k+1}+a_{k+2} + \\cdots},\r\n\\quad k=0, 1, 2, ...$$\r\nThen, $p_k$ is the chance the airplane fails next year's test\r\nafter it has serviced $k$ years. What's the long run\r\nfraction of replacements?\r\n\r\nDefine $X_n$ as  the number of years of service of\r\n the airplane servicing on Jan 1 of year $n$.  ($X_0=0$.)\r\n $\\{X_n\\}$ is a {\\it MC} with state space $\\{0, 1, ..., N-1\\}$\r\nand transition probability matrix\r\n\\begin{eqnarray*}\r\n&&  \\qquad \\quad \\qquad 0 \\qquad \\quad 1 \\qquad \\quad 2 \\qquad \\quad \\,\\, 3\r\n\\qquad \\, \\cdots \\quad  N-1 \\\\\r\nP &=& \\matrix{0 \\cr 1 \\cr 2 \\cr 3 \\cr \\vdots \\cr N-2 \\cr N-1}\r\n\\pmatrix{ p_0 & 1-p_0 & 0 &  0 & \\cdots & 0 \\cr\r\n          p_1 & 0     &1-p_1& 0& \\cdots & 0 \\cr\r\n          p_2 & 0     &0    &1-p_2& \\cdots & 0 \\cr\r\n          p_3 & 0     &0    & 0 & \\cdots & 0 \\cr\r\n          \\vdots&\\vdots & \\vdots &\\vdots & \\vdots & \\vdots \\cr\r\n          p_{N-2} & 0& 0& 0& \\cdots & 1-p_{N-2} \\cr\r\n          1 & 0 & 0 & 0 & \\cdots & 0 }.\r\n          \\end{eqnarray*}\r\nFor illustration, consider a typical path with $N=3$ as follows.\r\n(figure)\r\nFor example, for $k\\leq  N-2$,\r\n\\begin{eqnarray*}\r\nP_{k,0}&=& P(X_{n+1}=0|X_n=k) \\\\\r\n&=&  P(\\hbox{the plane fails\r\nthe  year-end examination at year $n$} \\, \\\\\r\n&& \\quad\r\n| \\,\\, \\hbox{the plane in service on Jan 1st of year $n$ has served $k$ years}) \\\\\r\n&=& p_k, \\\\\r\nP_{k, k+1} &=& P(X_{n+1}=k+1|X_n=k) = 1-p_k;\r\n\\end{eqnarray*}\r\n and $P_{N-1, 0} = P(X_{n+1}=0|X_n=N-1)=1.$\r\nThe equation (4.2)-(4.3) can be rewritten as\r\n$$\\cases{ \\pi_0=p_0\\pi_0 +p_1 \\pi_1 + \\cdots +p_{N-2}\r\n\\pi_{N-2} +\\pi_{N-1} & \\cr\r\n\\pi_1=(1-p_0)\\pi_0 & \\cr\r\n\\pi_2=(1-p_1)\\pi_1 & \\cr\r\n\\vdots & \\cr\r\n\\pi_{N-1}=(1-p_{N-2})\\pi_{N-2} & \\cr\r\n\\pi_0 + \\cdots + \\pi_{N-1} = 1 & \\cr\r\n}\r\n$$\r\nIt follows that $\\pi_k = (1-p_{k-1})\\cdots (1-p_0)\\pi_0$\r\nfor $1 \\leq k \\leq N-1$.\r\nHence,\r\n$$\\pi_0 \\{ 1 + (1-p_0) + (1-p_0)(1-p_1) + \\cdots\r\n+ \\prod_{i=0}^{N-2} (1-p_i) \\} = 1.$$\r\nAnd, finally, we have\r\n\\begin{eqnarray*}\r\n\\pi_0 &=& { 1\\over 1 + (1-p_0) + (1-p_0)(1-p_1) + \\cdots\r\n+ \\prod_{i=0}^{N-2} (1-p_i)  } \\\\\r\n&=&{1 \\over P(Y> 0) +P(Y>1)+ \\cdots + P(Y > N-1)} \\\\\r\n&=&{1 \\over E[\\min(Y, N) ]}.  \\qquad \\hbox{(See {\\it Exercise 4.2})}\r\n\\end{eqnarray*}\r\n$\\pi_0$ is the long run fraction of\r\nreplacements. In other words, by our\r\n{\\it time average} interpretation,\r\n$\\pi_0 \\approx   R_n/n$ for a large $n$,\r\nwhere $R_n$ is number of replacements over\r\n$n$ years. As a result,\r\n$$ R_n \\times E[\\min(Y, N) ] \\approx n$$\r\nSince $E[\\min(Y, N)]$ is the mean length of\r\nservice of an airplane, the above approximation\r\nindeed makes sense.\r\n $\\square$\r\n\r\nRemark. Suppose $\\xi_i$ is the time of service of the $i$-th Boeing 777.\r\nThen $\\xi_1, \\xi_2, ...$ are iid positive r.v.s following the common distribution\r\nas that of $\\min(Y, N)$.  This is the key element in the definition\r\nof a renewal process   $R(t)$,   the number of\r\nreplacements by time $t$, to be specified  in Chapter 7.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 5, "calculateddifficulty": null, "linkneuron": [3, 37], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 111, "fields": {"code": "4.1.8", "category": 1, "problem": "Consider transition probability matrix:\r\n\\begin{eqnarray*}\r\n&&\\qquad \\,\\,\\, 0 \\qquad 1 \\qquad 2 \\qquad 3 \\\\\r\nP &=& \\matrix{0 \\cr 1 \\cr 2 \\cr 3} \\pmatrix{\r\n1/2 & 1/2 & 0 & 0 \\cr\r\n1/2 & 0 & 1/2 & 0 \\cr\r\n0 & 0 & 1/4 & 3/4 \\cr\r\n0 & 0 & 1/3 & 2/3\r\n}\r\n\\end{eqnarray*}\r\n\r\nState $j$ is called {accessible} from state\r\n$i$, denoted as $i \\to j$,\r\n if $P_{ij}^{(k)} > 0$ for some $k > 0$.\r\nIn other words, from $i$ there is a positive chance\r\nthe $MC$ can reach state $j$. In the above\r\nexample, $0 \\to 1$, $0 \\to 2$, but $2 \\nrightarrow 0$.\r\n\r\nState $i$ {\\it communicates } with state $j$, denoted\r\nas $i \\leftrightarrow j $, if\r\n$i \\to j$ and $j \\to i$. In the above example,\r\n$0 \\leftrightarrow 1$, $2 \\leftrightarrow 3$, but\r\n$0 \\nleftrightarrow 2$.\r\n\r\nIt is easily seen that communicativeness has properties:\r\n(a). reflexivity: $i \\leftrightarrow i$; (b). symmetry:\r\n$i \\leftrightarrow j$ is the same as $j \\leftrightarrow i$;\r\n(c). transitivity: if $i \\leftrightarrow j$ and\r\n$j \\leftrightarrow k$, then $i \\leftrightarrow k$.\r\nA relation satisfies the above three properties\r\nusually defines classes of certain equivalency.\r\nAll states of a {\\MC} can be partitioned into\r\na number of classes such that states within a class\r\nall communicate with each other and states belong to\r\ndifferent classes do not.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "A {MC}  is called { irreducible}\r\n if all states communicate with each\r\nother. The {\\it MC} with transition probability matrix\r\nin Example 4.8 is  not irreducible. A regular {MC}\r\nis irreducible, but an irreducible {MC} is not necessarily\r\nregular.\r\n\r\nState $i$ is {\\periodic} if, for some $k>1$, $P_{ii}^{(n)}=0$\r\nfor {\\all} $n$ that are not multiples $k$ and $P_{ii}^{(n)}>0$\r\nfor {some } $n$ as multiples of $k$.\r\n  In other words,\r\n$P_{ii}^{(km+1)}=P_{ii}^{(km+2)}=\\cdots=P_{ii}^{(km+k-1)}=0$\r\nfor all $m\\geq 0$ and $P_{ii}^{(kj)}>0$ for some $j \\geq 1$.\r\n\r\nThe {period} of state $i$, denoted as $d(i)$, is\r\nthe greatest common divisor (gcd) of all $n\\geq 1$\r\nsuch that $P_{ii}^{(n)}>0$, i.e.,\r\n$$d(i) = \\hbox{gcd}\\{n \\geq 1: P_{ii}^{(n)} > 0 \\}.$$\r\nIf $d(i)\\geq 2$, we say $i$ is periodic with period $d(i)$;\r\nif $d(i)=1$, we say state $i$ is {\\it aperiodic}. We\r\nset $d(i)=0$ if $P_{ii}^{(n)}=0$ for all $n\\geq 1$.\r\nStarting from state $i$, only when the $MC$ takes\r\nsome multiples of $d(i)$ steps, would $MC$ have a\r\npositive chance to return to state $i$.\r\n\r\nConsider a $MC$ with states 0, 1, 2, 3 and\r\none step transition probabilities $P_{01}=P_{12}=P_{23}=P_{30}=1$.\r\nThen, all states are periodic with period 4.\r\nSuppose the one-step\r\ntransition probabilities are such\r\nthat $P_{01}=P_{12}=P_{23}=1$, $P_{30}=1/3$ and $P_{32}=2/3$.\r\nThen, all states are periodic with period 2.\r\nIn Example 4.8, all states are aperiodic.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [3, 40], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 177, "fields": {"code": "4.1.1", "category": 1, "problem": "Let $\\{X_n\\}$ be a $MC$ with two states 0 and 1, and transition\r\nmatrix:\r\n$$\\bf P = \\pmatrix{0.33 & 0.67 \\cr 0.75 & 0.25}.$$\r\nThen,\r\n\\begin{eqnarray*}\r\n&&  \\bf P^2 = \\pmatrix{0.611 & 0.389 \\cr 0.438 & 0.562}, \\quad\r\n\\bf P^5 = \\pmatrix{0.524 & 0.476 \\cr 0.536 & 0.464}, \\\\\r\n&& \\bf P^7 = \\pmatrix{0.528 & 0.472 \\cr 0.530 & 0.469}, \\quad .... \\quad\r\n\\bf P^{16} = \\pmatrix{0.5294 & 0.4706 \\cr 0.5294 & 0.4706} \\quad ....\r\n\\end{eqnarray*}\r\nRecall that $\\bf P^{(n)}=\\bf P^n$.\r\nWe get the impression from this example that\r\nthe $n$-step transition matrix converge, actually quite fast in this example,\r\nand that limits in the same columns are the same.\r\nThe question is: is this a general phenomenon or only pertained to\r\nthis $MC$? Note that all the above transition matrices, one-step or many\r\nstep, have all positive entries.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No solution for an example", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [35], "rightproblems": [], "wrongproblems": [178], "twinproblems": []}}, {"model": "mathematics.question", "pk": 178, "fields": {"code": "4.1.2", "category": 1, "problem": "Let $\\{X_n\\}$ be a $MC$ with two states 0 and 1, and transition\r\nmatrix:\r\n$$\\bf P = \\pmatrix{0 & 1 \\cr 1 & 0}.$$\r\nIn other words, from state 1, the $MC$ sure goes to 0 the next step; and\r\nfrom state 0, it sure goes to 1 the next step. Then\r\n$$\\bf P^k =\\bf P \\quad \\hbox {for all odd $k$}, \\qquad\r\n\\bf P^k= \\pmatrix{1 & 0 \\cr 0  & 1 } \\qquad \\hbox{for all even $k$}.$$\r\nClearly, $\\bf P^n$ does not converge as $n \\to \\infty$.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No solution for an example", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [35], "rightproblems": [], "wrongproblems": [177, 179], "twinproblems": []}}, {"model": "mathematics.question", "pk": 179, "fields": {"code": "4.1.3", "category": 1, "problem": "According to an old Chinese saying: Fu Bu Guo San Dai\r\n(wealthiness does not last beyond third generation---a word-by-word\r\ntranslation). <br>\r\nIndeed, suppose every family in a population assumes one of $N+1$ financial classes/states:\r\nState 0 is the poorest, 1 the second poorest, ...,  $N$ the richest.\r\nLet $X_n$  the state of the $n$-th generation of a family. It is reasonable to assume a transition matrix with all $P_{ij} > 0$, $i, j=0, 1,..., N$. Such transition probability matrix and the $MC$ are so called $regular$, to be defined below.\r\n<br>\r\nIt will be shown that, in the long run,the family, no matter it started rich or poor, will turn out ordinary after many generations. The same is true for, other than wealth, height or IQ or nearly all of the biological traits. The hundredth generation of Yao, Ming, the star basketball player, and that of Pan, Changjiang, a  Chinese comedy star known for being short and funny, will have about same distribution of height.  The former can very well be shorter than the latter, and, even if taller, it's largely by randomness.\r\n<br>\r\nIndeed, we shall show a large class of $MC$s, called regular $MC$s,\r\nthe transition matrices do converge. Example 4.1 is one of those and\r\nExample 4.2 is not.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No solution for an example", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [35], "rightproblems": [], "wrongproblems": [178, 180], "twinproblems": []}}, {"model": "mathematics.question", "pk": 180, "fields": {"code": "4.1.4", "category": 1, "problem": "Mr. C  spends his life time   only in three places Home, Office and\r\nGym. Assume every time he reaches H, O and G, he spends 12, 10 and 2 hours, respectively,\r\n and\r\nthen transits to another location according to the following transition\r\nprobability matrix\r\n\\begin{eqnarray*}\r\n&&\\qquad \\,\\, \\, {\\rm H} \\quad \\,\\, {\\rm O} \\quad \\,\\, {\\rm G} \\\\\r\n\\bf P\r\n&=& \\matrix{{\\rm H} \\cr {\\rm O} \\cr {\\rm G}}\r\n\\pmatrix{ 0   & 0.9 & .1 \\cr .1 & 0& .9 \\cr 0.8 & 0.2 & 0}\r\n\\end{eqnarray*}\r\nIt is easily verified that $\\bf P^2$ has all entries positive and therefor\r\n$\\bf P$ is regular, which is perhaps easier to see from the this figure.\r\n\r\nSolving equation (4.2)-(4.3), we have\r\n$$\\pi_H= 0.309 \\qquad \\pi_O=0.347\\qquad \\pi_G=0.343.$$\r\nLet $X_n$ be Mr C's whereabout after $n$ transitions.\r\n$\\{X_n: n\\geq 0\\}$ is a $MC$ with state space\r\n$\\{ {\\rm H, O, G} \\}$ and  transition matrix $\\bf P$.\r\nLet $n_H$ be the number of times he stays in state H\r\n in a long period with (a large) $n$ transitions. Likewise\r\n define\r\n $n_O$ and $n_G$.\r\nThe long  run fraction of his life time spent in office is\r\n \\begin{eqnarray*}\r\n && {n_O \\times 10 \\over n_H \\times 12 + n_O \\times 10 + n_G \\times 2}\r\n ={10 n_O/n \\over 12 n_H/n + 10 n_O/n + 2 n_G/n} \\\\\r\n& {   \\approx } &\r\n {10 \\pi_O \\over 12 \\pi_H + 10 \\pi_O + 2 \\pi_G }\r\n = 0.441.\r\n  \\end{eqnarray*}\r\nTranslating into daily hours, it is $0.441*24 =10.58$ hours in office per day.\r\nLikewise, the long run fractions of his life time spent in Home and\r\nGym are, respectively, $0.472$ and $0.087$, which are\r\n$11.32$ and $2.09$ in daily hours.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No solution for an example", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [24], "rightproblems": [], "wrongproblems": [179], "twinproblems": []}}, {"model": "mathematics.question", "pk": 181, "fields": {"code": "5.1.4", "category": 1, "problem": "Suppose coin $A$ and coin $B$ each respectively has chances\r\n$q$ and $p$ to turn up head, with $q$ being very small.\r\n  Suppose coins A and B are tossed\r\n$together$     a large number of $n$ times, and\r\n  $nq$ is close to $\\mu>0$. \r\n<br>\r\nLet $X$ be the number of times $A$ turning up\r\nhead and $Y$ be the number of times both $A$ and $B$  turning\r\nup heads.\r\nIt is clear that\r\n$X \\sim Bin(n, q)$, which is approximately ${\\cal P}(\\mu)$.\r\n$Y|\\{X=k\\}$ is the number of heads of coin B on those $k$ tosses with\r\n coin A being heads.\r\nThen, $Y|\\{X=k\\} \\sim Bin(k, p)$, as the  two coins are tossed\r\nindependently.\r\nOn the other hand,\r\n$Y\\sim Bin(n, qp)$, which is approximately ${\\cal P}(\\mu p)$.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No solution for an example", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [68], "rightproblems": [], "wrongproblems": [182], "twinproblems": []}}, {"model": "mathematics.question", "pk": 182, "fields": {"code": "5.1.5", "category": 1, "problem": "Suppose, in addition to the assumptions (1) and (2) in Example 5.2,\r\nthe chance, denoted by $p$, for  Chelsea to score a goal\r\nwithin any given one minute is equal. Let $X(t)$\r\nbe the number of the Chelsea goals from the beginning of the\r\nmatch till time $t$, with the unit of the time being minute.\r\nThen, $\\{X(t): t\\in [0, 90]\\}$ is $approximately$ a Poisson\r\nprocess with rate $p$, the mean number of goals per minute, the time unit.\r\n(A match has 90 minutes.)\r\n\r\nIt is important to understand Poisson processes through binomial approximation as,\r\nfor example, follows.\r\nHeuristically, let $\\Delta t$ be a very very small time unit. Cut up the half line\r\n$(0, \\infty)$ into $(0, \\Delta t], (\\Delta t, 2\\Delta t], (2 \\Delta t, 3 \\Delta t], ...$\r\nSuppose events occur along time satisfying the following assumptions.\r\nFirst, corresponding to (b) or (5.1'),\r\n$$\\xi_i =\\cases{ 1 &  with probabability $\\lambda \\Delta t$, if an event occur in interval\r\n$((i-1)\\Delta t, i \\Delta t]$ \\cr\r\n0 & otherwise.}\r\n$$\r\nSecond, corresponding to (a), assume $\\xi_i, i \\geq 1$ are independent.\r\nLet $X(t)$ be the number of events happening up to time $t$. Then,\r\n$X(t), t\\geq 0$ is approximately a Poisson process with intensity $\\lambda$.\r\nIn fact, for any fixed $t>0$,\r\n$X(t) \\approx \\sum_{i=1}^n \\xi_i$ where $n $ is the integer part of $t/\\Delta t$.\r\nSince, by binomial approximation,\r\n$$\\sum_{i=1}^n \\xi_i \\sim Bin(n,  \\lambda \\Delta t) \\approx {\\cal P}( n \\lambda \\Delta t)\r\n\\approx {\\cal P}(\\lambda t),$$\r\nWe conclude $X(t)$ follows approximately ${\\cal P}(\\lambda t)$. Likewise argument\r\nimplies $X(t+s)-X(s)$ follows approximately ${\\cal P}(\\lambda t )$. The independent\r\nincrement of $\\{X(t)\\}$ is ensured approximately by the independence of $\\xi_i, i \\geq 1$.\r\n\r\n\r\nThe Poisson process can be viewed as tallying\r\nmicro-rare events over time.\r\n Therefore it is widely used to model number of events which happen\r\nrandomly along time. Here $\\lambda$ measures the intensity of the\r\nevents happening. It measures\r\nthe average number of events per time unit.\r\n The larger (smaller) the $\\lambda$, the more (less) likely the\r\nevents.\r\nThe most essential ingredient of the Poisson process is\r\nthat {\\it over different non-overlapping time intervals, event occurrences\r\n are entirely independent}. More general version of Poisson processes\r\n allows the intensity vary over time, which is referred to as\r\n non-homogeneous Poisson process.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No solution for an example", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [68], "rightproblems": [], "wrongproblems": [181], "twinproblems": []}}, {"model": "mathematics.question", "pk": 183, "fields": {"code": "6.1.4", "category": 1, "problem": "In a partly (or mostly) fictitious portray of\r\n the battle of Thermopylae in 480 BC,  300 Greek Spartans, led by King Leonidas,\r\nwere   allegedly  able  to block for three days the advance of a Persian army\r\nof King Xerxes numbered in the hundreds of thousands, and were later all killed.\r\nSuppose the ever-brave King Leonidas  only counted the number of surviving soldiers\r\nincluding himself, at any time $t$; whilst the ever-optimistic King Xerxes\r\n only counted the number of kills\r\nthey had inflicted upon the Spartans at any time $t$. Let\r\n$X(t)$ and $Y(t)$ be the counting process in the minds of Leonidas and Xerxes, respectively.\r\nThen $X(0)=300$ and $Y(0)=0$. Clearly $X(t)=300-Y(t)$.\r\nThen,  if $\\{ Y(t): t \\in [0, \\infty) \\}$ is a pure birth process, then\r\n$\\{ X(t): t\\in [ \\infty)\\}$ is a pure death process.\r\nIf the later is a pure death process, then the former is a pure birth process.}\r\n<br>\r\n In fact, it can be seen from the definitions of pure birth and pure death processes,\r\n especially through postulate 3 via sojourn times, that there is a mirror\r\n relation between pure birth and pure death process as illustrated above\r\n and reiterated as follows.\r\n For any pure death process $X(t)$, beginning with $X(0)=N$, with state space $\\{0, ..., N\\}$\r\n and death rates $\\mu_N, ..., \\mu_1$,\r\nthe process $Y(t) = N-X(t)$, which counts the cumulative number of\r\ndeaths, is a pure birth process with birth rates\r\n$$\\lambda_0 = \\mu_N,\r\n\\lambda_1 = \\mu_{N-1}, ..., \\lambda_{N-1} = \\mu_1\r\n\\quad  {\\rm and }  \\quad \\lambda_N = 0.$$\r\nNote also that the sojourn time $S_k$ for the pure death process\r\n$X(\\cdot)$ is then the sojourn time $S_{N-k} $ for\r\nthe pure birth process $Y(\\cdot) = N-X(\\cdot)$.\r\n<br>\r\n  (continued)  Assume\r\nthe   life times of the $N=300$  Spartan soldiers are\r\niid following a common\r\n exponential distribution with parameter\r\n$\\alpha$,\r\nthen the process of the number of\r\nsurviving solders by time $t$,   $\\{X(t)\\}$, is a pure death\r\nprocess with rates $\\mu_k = K\\alpha$.\r\nThe following is a proof.\r\nLet $\\xi_i, i=1,..., N$ be the life times of the $N$ members of\r\nthe population. Then, $\\xi_i$ are iid following exponential distribution\r\nwith parameter $\\alpha$.\r\n\\begin{eqnarray*}\r\n&& P(X(t+h) - X(t) =-1 | X(t)= k)\r\n\\cr &=& P(\r\n\\hbox{one of the $k$ surviving soldiers by $t$ dies within $(t, t+h)$}\r\n \\, | \\,\r\n\\hbox{\r\n$k$ soldiers survive up to time $t$})\r\n\\cr &=& k\r\nP(\\xi_1 \\in (t, t+h), \\xi_2 \\geq t+h, ..., \\xi_k \\geq t+h\r\n\\, | \\, \\xi_1 \\geq t, ..., \\xi_k \\geq t)\r\n\\cr &=&\r\nk  P(\\xi_1 \\in (t, t+h)|\\xi_1 \\geq t) \\prod_{j=2}^k P(\\xi_j \\geq t+h | \\xi_j\\geq t)\r\n\\cr &=&\r\nk  [ e^{-\\alpha t} - e^{-\\alpha (t+h) } ]/\r\n e^{-\\alpha t} \\prod_{j=2}^k e^{-\\alpha (t+h) }/ e^{-\\alpha t}\r\n\\cr &=&\r\nk [1- e^{-\\alpha h}] e^{-(k-1) \\alpha h}\r\n\\cr &=&\r\nk \\alpha h +o(h).\r\n\\end{eqnarray*}\r\n Similarly, one can show (please DIY)\r\n$$P(X(t+h) - X(t) = 0 |X(t) = k) = 1-k\\alpha h + o(h).$$\r\nThen, $X(\\cdot)$ is a pure death process with linear rates\r\n$\\mu_k = k\\alpha$.\r\n\r\nTo compute the marginal distribution of $X(t)$, write\r\n\\begin{eqnarray*}  P(X(t)=k)\r\n  &=&\r\nP(\\hbox{$k$ of the $n$ iid r.v.s $\\xi_1,..., \\xi_n$ are $\\geq t$\r\nand $n-k$ of them are $< t$})\r\n\\cr &=&\r\n{N \\choose k} P(\\xi_i > t)^k\r\nP(\\xi_i \\leq t)^{N-k} = {N \\choose k} e^{-\\alpha t k}\r\n(1- e^{-\\alpha t})^{N-k}\r\n\\end{eqnarray*}\r\nfor $k=N, N-1, ..., 0$.\r\n\r\nRecall that $W_k = S_N + S_{N-1} + ... +S_{N-k+1}$,\r\n $ k=1,...N$,\r\ndenote  the waiting times. Set $W_0 = 0$.\r\nThen, $W_N = S_N + ...+S_1$ is the time of the death\r\nof the last member of the population. In other words,\r\n$W_N$ is {\\it the time to extinction}.\r\n$$P(W_N < t) = P(X(t) =0 ) = {N \\choose 0} e^{-\\alpha t\r\n\\times 0} (1- e^{-\\alpha t})^{N-0}\r\n= (1-e^{-\\alpha t})^N.$$\r\nAlternatively,\r\n$$P(W_N < t) = P(\\xi_1 < t, ..., \\xi_N < t) =\r\n(1- e^{-\\alpha t})^N.$$", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No solution for an example", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [96], "rightproblems": [], "wrongproblems": [], "twinproblems": [194]}}, {"model": "mathematics.question", "pk": 184, "fields": {"code": "6.1.5", "category": 1, "problem": "Australian rabbits, since they were first introduced by the Europeans in 18th century, experienced\r\nexplosive growth in the ensuing decades and caused devastating\r\necological and environmental disaster, accounting for about $70\\%$\r\nof the loss of plant species and land erosion.\r\nBecause of lack of significant natural enemies (perhaps excluding humans), the deciding factor, aside from randomness, of population growth/decrease is precisely\r\npopulation size itself. Namely, small (large) population implies abundant (scarce)\r\nenvironmental food supply per rabbit. Such a process of\r\npopulation along time can be properly modeled by birth and death process.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No solution for an example", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 185, "fields": {"code": "7.1.4", "category": 1, "problem": "Suppose the inter-occurrence times $X_1,X_2,...$ are iid $\\sim$ $F$, with density\r\n$f(x) = \\theta - x \\theta^2/2$ for $0 \\leq x \\leq 2/\\theta$; where $\\theta >0$.\r\nThen,\r\n$$F(x) = \\int_0^x f(s)ds = \\theta x - x^2\\theta^2/4; \\qquad 0 \\leq x \\leq 2/\\theta.$$\r\nAnd $\\mu = E(X_1) = 2/(3\\theta) $ and $\\sigma^2= 2/(9\\theta^2)$.\r\n\r\n$N(t)$ is a renewal process based on $X_1,X_2,...$.\r\nThen, as $ t \\to \\infty$,\r\n\r\n(1). (Elementary renewal theorem)\r\n$$M(t)/t \\to 1/\\mu = 3\\theta/2;$$\r\n\r\n(2). (Refined renewal theorem),  moreover,\r\n$$M(t) = t/\\mu + { \\sigma^2  - \\mu^2 \\over 2 \\mu^2} + o(1)\r\n= 3 \\theta t / 2 + {2/9 - 4/9 \\over 2\\times 4/9} + o(1)\r\n= 3 \\theta t/2 - 1/4 +o(1);$$\r\n\r\n(3). (Central limit theorem for the renewal process)\r\n$${N(t) - t/\\mu \\over  \\sqrt{ t\\sigma^2/\\mu^3}  }= {N(t) - 3\\theta t/ 2 \\over\r\n \\sqrt{ 3t\\theta/4}  }\\to N(0, 1)$$\r\n\r\n\r\n(4). (Asymptotic distribution for $\\gamma_t$ and $\\delta_t$)\r\n$$ P(\\gamma_t < x) \\to\r\n{1 \\over \\mu} \\int_0^x (1-F(s))ds\r\n= {3\\theta \\over 2} \\int_0^x [1-\\theta s + {s^2\\theta^2 \\over 4}] ds\r\n= {3\\theta \\over 2}\\{ x- { \\theta x^2 \\over 2} - { \\theta^2 x^3 \\over 12} \\}$$\r\nfor $ 0 \\leq x \\leq 2/\\theta$.\r\nAs we know, $\\delta_t$ has the same limiting distribution as that of $ \\gamma_t$.\r\nFurthermore,\r\n$$\r\nE(\\beta_t) / \\mu \\to 1+ {\\sigma^2 \\over \\mu^2}\r\n\\to 1+ { 2/(9 \\theta^2) \\over 4/(9\\theta^2) } = 3/2.$$\r\nThis limit does not have to do with $\\theta$.\r\nThe same is true when $F(\\cdot)$ is the uniform distribution\r\non $[0, \\theta]$. However, if $F(\\cdot)$ is the uniform\r\ndistribution on $[\\theta, \\theta+1]$ for\r\nThen, larger the $\\theta$, the smaller the upward bias\r\nof the total life time relative to the ordinary life time.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No solution for an example", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [80], "rightproblems": [], "wrongproblems": [186], "twinproblems": []}}, {"model": "mathematics.question", "pk": 186, "fields": {"code": "7.1.5", "category": 1, "problem": "Suppose $Y_t; t=0, 1, 2,...$ is a  Markov chain with state space being\r\n$\\{0, 1\\}$ and transition matrix:\r\n$${\\bf P}= \\pmatrix{P_{00} & P_{01} \\cr\r\nP_{10} & P_{11} } = \\pmatrix{0.4 & 0.6 \\cr 0.7 & 0.3}. $$\r\n$Y_0=0$.\r\nThe limiting probabilities $\\pi_0, \\pi_1$ satisfy\r\n$$\\cases{ ( \\pi_0  \\,\\,\\, \\pi_1) = (\\pi_0 \\,\\,\\, \\pi_1) {\\bf P} \\cr\r\n\\pi_0 + \\pi_1 =1}$$\r\nAnd the solutions are $\\pi_0 = 7/13 $ and $\\pi_1=6/13$.\r\nThen,\r\n$$\\lim_{t \\to\\infty} P(Y_t = 0) = 7/13 \\qquad\\hbox{and} \\qquad\r\n\\lim_{t \\to\\infty} P(Y_t = 1) = 6/13.$$\r\nMoreover, our interpretation of the long run behavior of Markov chains\r\nalso implies that, as $n \\to \\infty$,\r\n$$ { \\# \\# \\{i \\leq n: Y_i = 0 \\} \\over n } \\to\r\n\\lim_{t \\to\\infty} P(Y_t = 0) = 7/13. .......(7.11)$$\r\nIn addition,\r\n$$ { \\#\\#\r\n\\{i \\leq n, Y_i=0, Y_{i-1}=1 \\} \\over n }\r\n\\to \\lim_{t \\to \\infty} P(Y_{t-1}=1, Y_t = 0)\r\n=P_{1 0} \\lim_{t \\to \\infty} P(Y_{t-1}=1)\r\n= 0.7   \\pi_1 = 21/65\r\n......(7.12)\r\n$$\r\n\r\n\r\n\r\n(1).\r\nSet $N(t) = \\#\\#\\{ 0 < i \\leq t , Y_i =0\\} $ for $ t \\geq 1$ and $N(0)=0$.\r\nThen, $N(t)$ is the total number of times the Markov Chain $\\{Y_1, ..., Y_t\\}$ stays\r\nat state $0$.\r\nThe above interpretation implies\r\n$$N(t)/t \\to 7/13.$$\r\nWe now try to approach a similar result from the perspective of a renewal process.\r\nFirst, we verify that $N(\\cdot)$ is indeed a renewal process.\r\nSet $W_0=0$, and\r\n$W_1= \\min\\{t > 0: Y_t=0\\}$,\r\n$W_2= \\min\\{t > W_1: Y_t=0\\}$, ...\r\n$W_{k+1}= \\min\\{t > W_k: Y_t=0\\}$,...\r\nAnd let\r\n$X_k= W_k-W_{k-1}$ for $k=1,2,...$.\r\nThen,\r\n$W_k$ is the $k$-th time that the Markov chain $Y_t$ visits\r\nstate $0$, counting from time 1 (not time 0).\r\nAnd $X_k$ is, certainly, the time period between the $k-1$-th\r\nand $k$-th visits of state 0.\r\nWith some technicalities it can be shown $X_k$ are iid positive\r\nrandom variables with distribution:\r\n$$ P(X_k=i) = \\cases{  0.4 & $i=1$ \\cr\r\n0.6 \\times 0.7 & $i=2$ \\cr\r\n\\vdots & $\\vdots$ \\cr\r\n0.6 \\times 0.3^{n-2} \\times 0.7 & $i=n$ , for  $n \\geq 3$\r\n }$$\r\n And $\\mu= E(X_k) = 13/7$.\r\n By renewal theorem,\r\n we have\r\n $$ E(N(t))/t \\to {1/\\mu}= 7/13.$$\r\n $$ {N(t)/t - 7/13 \\over \\sqrt{ \\sigma^2/(t \\mu^3)} } \\to N(0,1)$$\r\n which implies (7.11)\r\n\r\n\r\n(2). Set\r\n$$N(t)= \\sum_{i=1}^t I(Y_i=0, Y_{i-1}=1).$$\r\nThen $N(t)$ is   the number of times the Markov chian $\\{Y_t\\}$ returns to state $0$\r\nfrom state 1 over times $\\{1, 2, ..., t\\}$.\r\nTo understand why $N(\\cdot)$ is a renewal process. We set\r\n$W_0=0$.\r\n$W_1= \\min\\{ k \\geq 2: Y_k=0, Y_{k-1}=1\\},$\r\n$W_2= \\min\\{ k >W_1 : Y_k=0, Y_{k-1}=1\\},$...\r\n$W_{n+1}= \\min\\{ k > W_n : Y_k=0, Y_{k-1}=1\\},$...\r\nAnd let $X_n = W_n-W_{n-1}$ for $n \\geq 1$.\r\nThen $X_n$ are iid with\r\n$$P(X_n=k) = \\sum_{i=0}^{k-2}  0.4^i\\times 0.6\\times 0.3^{k-2-i} \\times 0.7,\r\n\\qquad k\\geq 2.$$\r\nAnd\r\n$$\\mu  = E(X_n) = 65/21.$$\r\nBy the renewal theorem,\r\nwe have, as $t \\to \\infty$,\r\n$$E(N(t))/t \\to 1/\\mu = 21/65.$$\r\n$$ { N(t)/t - 21/65 \\over \\sqrt{ \\sigma^2/(t \\mu^3)} } \\to N(0,1),$$\r\nwhich implies (7.12).", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "No solution for an example", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 4, "calculateddifficulty": null, "linkneuron": [80], "rightproblems": [], "wrongproblems": [185], "twinproblems": []}}, {"model": "mathematics.question", "pk": 198, "fields": {"code": "5.1.1", "category": 1, "problem": "{Example 5.1} {Traffic accidents})<br>\r\nSuppose (1). The chance of one traffic accident on the Clear Water Bay\r\nroad on any one day is very small, so small that more than one traffic\r\naccidents  one day is ignorable);  (2). Over different days,\r\nthe occurrence of traffic accidents\r\nare independent; and (3). On average, there are 3 traffic accidents\r\non the Clear Water Bay road per year.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "year. Then,\r\n$X$, the random number of traffic accidents over one year,\r\nfollows $Bin(365, p)$ with $p=3/365$. And $X$ follows approximately\r\nthe Poisson distribution with\r\nmean $3$, i.e.,\r\n$$ X \\sim {\\cal P}(3), \\qquad \\hbox{approximately}.$$", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [62, 63, 66], "rightproblems": [], "wrongproblems": [], "twinproblems": [199, 201, 202, 205]}}, {"model": "mathematics.question", "pk": 199, "fields": {"code": "5.1.2", "category": 1, "problem": "For Chelsea's match against  Tottenham Hotspur\r\n this Saturday, suppose (1). the chance    for Chelsea to score a goal\r\nwithin any given one minute is very small; (Scoring 2 or more goals\r\nwithin one minute is so rare that it's ignorable.) (2). Over different minutes, scoring of\r\ngoals are independent.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Then the number of goals of Chelsea in the match\r\nis a Poisson random variable, a random variable following  a Poisson distribution.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [63, 66, 68], "rightproblems": [], "wrongproblems": [], "twinproblems": [198]}}, {"model": "mathematics.question", "pk": 211, "fields": {"code": "6.1.1", "category": 1, "problem": "In a SciFi movie ``{\\it  Invasion}\"<br>\r\n  (2007, starring Nicole Kidman and Daniel Craig, adapted from the 1956\r\n    novel {The Body Snatchers} by Jack Finney.),\r\n  an alien life form affects the earth people and transforms them\r\n  into alien-like emotionless humankind, pod people or PP. Suppose,\r\n  every PP,  independent of everything else,\r\n  at any time $(t, t+\\Delta t)$ has chance $\\alpha \\Delta t$\r\n  to make a ``lethal\" contact with one random person, normal or PP, and transform\r\n  the person into PP if he/she is normal, and nothing happens if\r\n  he/she is already a PP.<br>\r\n  (continued) Let $N$ be the total population of\r\nthe earth.\r\nLet $X(t)$ be the number of PP by time $t$ beginning with the time\r\nof the first infection upon the crash of the space shuttle Columbia,\r\nand suppose $X(0)=1$.\r\nThen, (DIY)\r\n$$P(X(t+\\Delta t) - X(t) = 1 |X(t)=k) = (N-k) k \\alpha /(N-1) \\Delta t +\r\no(\\Delta t). $$\r\nThen, $\\{X(t): t \\in [0, \\infty)\\}$ is a pure birth process\r\nwith birth rates $\\lambda_k = (N-k)k\\alpha/(N-1)$ for $k=1,..., N$;\r\nand $\\lambda_k=0$ for $k \\geq N$.\r\nIt is seen that  growth rates   increase till half of the population\r\nis PP and decrease afterwards.\r\nFrom Postulate 3, we know $S_k \\sim $ exponential distribution with\r\nmean $1/\\lambda_k$. The first time the entire population on earth is\r\nPP is $S_1+\\cdots+S_{N-1}$ which has mean\r\n$$\\sum_{k=1}^{N-1} 1/\\lambda_k =\r\n\\sum_{k=1}^{N-1} {N-1 \\over \\{N-k)k\\alpha } = {N-1\\over N \\alpha}\r\n\\sum_{k=1}^{N-1} ( {1 \\over N-k} + {1 \\over k})\r\n= { 2(N-1)\\over N \\alpha } \\sum_{k=1}^{N-1} 1/k\r\n$$", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "As of March, 2009, the world population is about  $N= 6.76$ billion.\r\nLet $\\alpha=1$ and the unit of time is day, meaning that on average one day one\r\n``lethal\" contact from every one PP. Then,\r\nit takes on average about $2 \\sum_{k=1}^{N-1} 1/k /\\alpha = 46.4$\r\ndays for the entire earth population\r\nto turn PP.\r\n  Then, the number of PP on earth, along with time, follows a pure birth process\r\n  (but not a Poisson process).", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [86], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 212, "fields": {"code": "6.1.2", "category": 1, "problem": "The electron avalanche is a natural phenomenon, in which\r\n free electrons collide with\r\n  each other\r\n  in a strong electric field releasing \"new\" electrons\r\n  to undergo the same process in successive cycles.\r\nSuppose initially there are $m$ electrons in a ``jar\" $(X_0=m \\geq 2)$. At\r\nany time $(t, t+\\Delta t)$, if there are $k$ electrons in a jar,\r\nany pair of two electrons collide and generate a new electron\r\nwith probability $\\alpha \\Delta t +o(\\Delta t)$, independent of\r\neverything else.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Then, $X(t)$ is a pure birth process satisfying\r\n$$P(X(t+ \\Delta t) - X(t) = 1 |X(t)=k)=\r\n{k \\choose 2} \\alpha \\Delta t + o(\\Delta t) = {k(k-1)\\over 2} \\alpha \\Delta t +o(\\Delta t).$$\r\nHence it is a pure birth process with rates\r\n$\\lambda_k = (1/2) k(k-1)\\alpha $.\r\n As to be seen, the number of electrons\r\nconverge to infinity in finite time, causing avalanche or explosion.", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [86], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 218, "fields": {"code": "7.1.1", "category": 1, "problem": "{sc Poisson Process}<br> Let\r\n$X_1,X_2...,$ be iid $\\sim$ exponential distribution\r\nwith parameter $\\lambda$. Let\r\n$N(t) = \\max\\{n: \\sum_{i=0}^n X_i \\leq t,\\}$. ($X_0=0.$)\r\nThen, $\\{N(t): t \\in [0,\\infty)$ is a Poisson process and is also a renewal process.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": ".", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 219, "fields": {"code": "7.1.2", "category": 1, "problem": "A coin has probability $p$ to turn head and $1-p$ to turn tail when\r\nit is tossed. Let $\\xi_i=1$ if the $i$-th toss is a head and $0$ if the i-th toss\r\nis a tail. Set $\\xi_0=0$.\r\nDefine\r\n$$ N(t)= \\sum_{i=0}^t \\xi_i :  t=0, 1, 2, ...$$\r\nThen $N(t)$ is a discrete time renewal process. $N(t)$ is the\r\nnumber of heads till $t$-th toss.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "The waiting time or event time $W_k$ is the time of the $k$-th head.\r\n $W_0=0$. The inter-occurrence time $X_k=W_k-W_{k-1}$ is\r\n the time periods before the $k-1$-th head and the $k$-th head.\r\n It can be shown that\r\n $X_1,X_2...$ are iid following a geometric distribution:\r\n $P(X_1=j)= p(1-p)^{j-1}, j\\geq 1$. (DIY).", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 220, "fields": {"code": "7.1.3", "category": 1, "problem": "Suppose $\\{Y_n, n \\geq 0\\}$ is a discrete time {MC} with state space\r\n $\\{0, 1, ..., N\\}$. Suppose $Y_0=k$. Set\r\n Let $Z_0=0$. $Z_i$ the $i$-th time the {MC} visits state $k$, $i\\geq 1$.\r\n Set $N(0)=0$ and $N(t)=\\max\\{n: Z_n \\leq t,\\}$, $t\\geq 1$,\r\n  the number of visits of state $k$\r\n before or at time $t$ (excluding the $0$-th time).\r\n Then $\\{N(t): t=0, 1, 2, \\}$ is a renewal process.", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "", "choicesb": "", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "A", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": ".", "linkability1": 0.0, "linkability2": 0.0, "linkability3": 0.0, "linkability4": 0.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 229, "fields": {"code": "1.1.1", "category": 1, "problem": "De M\u00e9r\u00e9's Problem. ( Probability Space )\r\n<br>\r\nThe Chevalier de M\u00e9r\u00e9 was a French nobleman and a gambler of\r\nthe 17th century. He was interested in two gambling games:\r\nA: betting on At least one ace turns up with four\r\nrolls of a die; B: betting at least one double-ace turns up in 24 rolls\r\nof two dice.\r\nIt is tempting to believe the two events have same chance to occur.\r\nHis reasoning:\r\n<br>\r\n In one roll of a die, there is $1/6$ chance of getting an ace, so\r\n in 4 rolls, there is $4*(1/6)=2/3$ chance of getting at least one\r\n die.\r\n<br>\r\n In one roll of two dice there is $1/36$ chance of getting a double-ace,\r\n so in 24 rolls, there is $24*(1/36)=2/3$ chance of getting at\r\n least one double ace.\r\n<br>\r\nHow do you think of that?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "True", "choicesb": "Not True", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Therefore the two events, by this reasoning, have same chance. However,\r\n the chance computed here, $2/3$ obviously contracted his gambling experience\r\n that the chances are somewhat close to half, making the betting games a\r\n nearly fair game.\r\n<br>\r\nDe M\u00e9r\u00e9 turned to Blaise Pascal (1623-1662) and Pierre de\r\nFermat (1601-1665) for help. And the two mathematicians/physicists\r\npointed out that the above reasoning is erroneous and gave the right answer:\r\n<br>\r\n$P($getting at least one ace in 4 rolls of a die$) = 1- (1-1/6)^4=\r\n0.518$\r\n<br>\r\n4-rolls makes favorable bet, and 3-rolls makes not.\r\n<br>\r\n$P($getting at least one double-ace in 24 rolls of two dice$) = 1-\r\n(1-1/36)^{24}= 0.491$\r\n<br>\r\n25 rolls makes a favorable bet, 24 rolls make still an unfavorable bet.", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": 1.0, "linkneuron": [23], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 230, "fields": {"code": "1.1.2", "category": 1, "problem": "The St. Petersberg Paradox. ( Expectation )\r\n<br>\r\nA gambler pays an entry fee $M$ dollar to play following game: A fair\r\ncoin is tossed repeated until the first head occurs and you win\r\n$2^{n-1}$ amount of money where $n$ is the total number of tosses.\r\n<br>\r\nQuestion: what is the \"fair\" amount of $M$?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Finite number.", "choicesb": "$\\infty$", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "$n$ is a random number with $P(n=k) = 2^{-k}$ for $k=1, 2, ...$.\r\nTherefore the ``Expected Winning\" is\r\n$$E(2^{n-1} ) = \\sum_{k=1}^\\infty 2^{k-1} \\times {1 \\over 2^k} = \\infty.$$\r\n{\\it Notice that here I have used the expectation of a function of\r\nrandom variable}. It appears that a  \"fair\", but indeed naive,\r\n$M$ should be $\\infty$. However, by common sense, this game,\r\ndespite its infinite payoff, should not worth the same as\r\ninfinity.", "linkability1": 1.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 1, "calculateddifficulty": null, "linkneuron": [31], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 232, "fields": {"code": "1.1.3", "category": 1, "problem": "The dice game called \"craps\". ( Conditional probability )\r\n<br>\r\nTwo dice are rolled repeatedly and let the total dots of the\r\n$n$-th roll be $Z_n$. If $Z_1$ is 2 or 3 or 12, it is an immediate\r\nloss of the game. If $Z_1$ is 7 or 11, it is an immediate win.\r\nElse, continue the rolls of the two dice until either $Z_1$\r\noccurs, meaning Win, or $7$ occurs, meaning Loss.  What is the\r\nchance of  win of this game?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "Larger than 1/2.", "choicesb": "Smaller than 1/2.", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Notice that $Z_1$, $Z_2$,... are iid, with\r\nprobabilities:\r\n<br>\r\n$ P(Z_i=2) = P(Z_i= 12)= {1\\over 36} $, $ P(Z_i=3) = P(Z_i= 11)={2\\over 36} $, $ P(Z_i=4) = P(Z_i= 10)={3\\over 36} $, $ P(Z_i=5) = P(Z_i= 9)={4\\over 36}  $, $ P(Z_i=6) = P(Z_i= 8)={5\\over 36}  $, $ P(Z_i=7) = {6\\over 36}  $\r\n<br>\r\nWrite\r\n\\begin{eqnarray*}   P(\\hbox{Win  }) &=& \\sum_{k=2}^{12}\r\nP(\\hbox{Win and } Z_1= k) = \\sum_{k=2}^{12} P(\\hbox{Win  } |Z_1=\r\nk) P(Z_1=k) \\\\\r\n&=& P(Z_1=7) +P(Z_1 = 11) + \\sum_{k=4, 5, 6, 8, 9, 10} P(\\hbox{Win\r\n} |Z_1= k) P(Z_1=k) \\\\\r\n&=& 6/36 + 2/36 + \\sum_{k=4, 5, 6, 8, 9, 10} P(A_k^c |Z_1= k)\r\nP(Z_1=k)\r\n\\\\\r\n&=& 2/9 + \\sum_{k=4, 5, 6, 8, 9, 10} P(A_k^c) P(Z_1=k)\r\n\\end{eqnarray*}\r\nwhere $A_k$ is the event that starting from the second roll, $7$\r\ndots occur before $k$ dots; and $A_k^c$ is the complement of $A_k$. Now,\r\n\\begin{eqnarray*}   P(A_k) &=& \\sum_{j=2}^{12} P(A_k \\cap \\{Z_2\r\n=j\\} ) = \\sum_{j=2}^{12} P(A_k | \\{Z_2 =j\\} )P(Z_2= j) \\\\\r\n&=& P(Z_2=7) + \\sum_{j=2 \\atop j\\not=k, j\\not=7 }^{12}\r\nP(\\hbox{starting from the 3rd roll, 7 occurs before $k$}) P(Z_2 =j\r\n)\r\n\\\\\r\n&=& 6/36 + P(A_k) ( 1-P(Z_2=k) - P(Z_2=7)).\r\n\\end{eqnarray*}\r\nAs a result,\r\n$$P(A_k) = {P(Z_1=7) \\over P(Z_1=k) + P(Z_1=7)} \\quad \\hbox{and}\\quad\r\nP(A_k^c) = {P(Z_1=k) \\over P(Z_1=k) + P(Z_1=7)}.$$\r\nAnd,\r\n\\begin{eqnarray*}\r\nP(\\hbox{Win}) &=& {2 \\over 9} +\\sum_{k=4, 5, 6, 8, 9, 10} {P(Z_1=k)\\over [P(Z_1=k) + P(Z_1=7)]}\r\nP(Z_1=k)  \\\\\r\n&=& {2 \\over 9} + 2\\Bigl( {(3/36)^2\\over (3+6)/36} +\r\n{(4/36)^2\\over (4+6)/36} +\r\n{(5/36)^2\\over (5+6)/36} \\Bigr)\r\n\\\\\r\n&=& {2 \\over 9} + {2 \\over 36} [ 1 + { 16 \\over 10} + {25 \\over 11}]\\\\\r\n&=& 488/990 = 0.4929293.\r\n\\end{eqnarray*}", "linkability1": 2.0, "linkability2": 3.0, "linkability3": 1.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 3, "calculateddifficulty": null, "linkneuron": [25], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 233, "fields": {"code": "1.1.4", "category": 1, "problem": "Jailer's reasoning. ( Bayes formula )\r\n<br>\r\nThree men, A, B and C are in jail and one to be executed\r\nand the other two to be freed. C, being\r\nanxious, asked the jailer to tell him who of A and B would be freed.\r\nThe jailer, pondering for a while, answered \u201cfor your own interest,\r\nI will not tell you, because, if I do, your chance of being executed\r\nwould rise from 1/3 to 1/2.\" Is it true with the jailer's reasoning?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "True", "choicesb": "Not True", "choicesc": "", "choicesd": "", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "B", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let AF  (BF) be the event that jailer $says$ A  (B) to be freed.\r\nLet  AE or  BE or CE be the event that A or B or C to be executed.\r\nThen, $P(CE)=1/3$. but, by the {\\it Bayes formula },\r\n\\begin{eqnarray*}\r\nP(CE|AF) &=& {P( AF|CE)P(CE) \\over P(AF|AE)P(AE)  + P(AF|BE)P(BE) + P(AF|CE)P(CE)}\r\n\\\\\r\n&=&{ 0.5 * 1/3 \\over 0*1/3 + 1* 1/3 + 1/2* 1/3 }\\\\\r\n&=&1/3 \\\\\r\n&=&P(CE).\r\n\\end{eqnarray*}\r\nLikewise $P(CE|BF) = P(CE)$. So the \u201crise of probability\" is false.", "linkability1": 2.0, "linkability2": 1.0, "linkability3": 1.0, "linkability4": 2.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [27], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}, {"model": "mathematics.question", "pk": 234, "fields": {"code": "1.1.5", "category": 1, "problem": "Buffon's needle. ( Continuous random variables and expectation)\r\n<br>\r\nRandomly drop a needle of 1 cm onto a surface of many parallel straight lines\r\nthat are 1 cm apart. What is the chance that the needle touches one of the lines?", "problempicture1": "", "problempicture2": "", "problempicture3": "", "problempicture4": "", "problempicture5": "", "problempicture6": "", "choicesa": "$1 \\over 2$", "choicesb": "$1\\over \\pi$", "choicesc": "$2\\over \\pi$", "choicesd": "$2\\over 3\\pi$", "choicese": "", "choicesf": "", "choicepicturea": "", "choicepictureb": "", "choicepicturec": "", "choicepictured": "", "choicepicturee": "", "choicepicturef": "", "answer": "C", "solutionspicture1": "", "solutionspicture2": "", "solutionspicture3": "", "solutions": "Let $x$ be the distance from the center of the needle to the nearest line. Let\r\n$\\theta$ be the smaller angle of the needle with the nearest line.\r\nThen, the needle crosses a line if and only if $x/sin(\\theta) \\leq 0.5$.\r\n\r\nIt follows from the randomness of the drop that   that $\\theta$ and $x$ are\r\nindependent following the uniform distribution on $[0, \\pi/2]$ and $[0, 1/2]$.\r\nTherefore,\r\n$$P(x/sin(\\theta) \\leq 0.5) = {4 \\over \\pi} \\int_0^{\\pi/2} \\int_0^{sin(\\theta)/2 } dx d\\theta\r\n={2 \\over  \\pi} \\int_0^{\\pi/2}  sin(\\theta)   d\\theta\r\n={2 \\over \\pi}.$$\r\nThe chance is $2/\\pi$.", "linkability1": 2.0, "linkability2": 2.0, "linkability3": 1.0, "linkability4": 1.0, "linkability5": 0.0, "linkability6": 0.0, "linkpersonaility": null, "errors": "", "alternativesolutions": "", "messagefailure": "", "messagesuccess": "", "sensitivity": null, "gussingparameter": null, "difficulty": 2, "calculateddifficulty": null, "linkneuron": [24, 31, 34], "rightproblems": [], "wrongproblems": [], "twinproblems": []}}]
export default data;